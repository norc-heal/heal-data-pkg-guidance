{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 A data package is an archive containining research data together with the metadata, supporting files and documentation needed to permit someone unconnected to the original study to discover, understand, replicate, and/or (re)use the data or other study materials (e.g. code, protocol, survey instrument) for a specific purpose. This guide aims to provide clear, step-by-step instructions for how to: Create a data package Prepare a data package for submission to a public data repository Creating a data package inherently makes the originating study group's job of preparing the data package for submission to a public data repository easier! Benefit for Secondary Users \u00b6 The focus here is on creating data packages suitable for submission to a public data repository to permit replication of a previous analysis and/or secondary analyses. Such packages include the information necessary to make the data findable , accessible , interoperable and reusable ( FAIR ). In particular, they (1) include metadata that may be indexed and searched to permit researchers to find the data; (2) organize and store data and supporting files using common, open standards and formats that make them easy to use; and (3) include sufficient documentation, including information on provenance and data use requirements and restrictions, to permit researchers to understand and reuse the data. Benefit for Originating study group & Data Packages without data \u00b6 Importantly, while data packages are the best way to share primary research data with potential secondary users, they are also useful for the originating study group AND they are also useful even when the originating study group is generating data that cannot be shared or is working with secondary data that cannot be redistributed . A data package provides an ideal way to organize your work locally, increasing the efficiency and reproducibility of your own work and facilitating collaboration among your team. In addition, a data package may be easily modified to exclude all or some of the data themselves\u2014possibly replacing them with links to their sources in the case of secondary data or with synthetic data\u2014leaving you with a product that can then be shared in order to document your work and permit other researchers to replicate your analysis. Navigating this guide \u00b6 The material in this guide is organized into four main sections. The first section provides a general overview of data packages, including what they are, why they're useful, and when and how to make one. The second section is a step-by-step tutorial on creating your first data package, with instructions tailored to your own situation based on your answers to three simple questions. The third section is a step-by-step tutorial on preparing your first data package for submission to a public data repository, with instructions tailored to your own situation based on your answers to three simple questions. The fourth section provides additional guidance on specific topics as well as references to additional resources that may be helpful. Links to specific parts of this last section are provided throughout the first three sections to clarify concepts and recommendations as they come up.","title":"Introduction"},{"location":"#introduction","text":"A data package is an archive containining research data together with the metadata, supporting files and documentation needed to permit someone unconnected to the original study to discover, understand, replicate, and/or (re)use the data or other study materials (e.g. code, protocol, survey instrument) for a specific purpose. This guide aims to provide clear, step-by-step instructions for how to: Create a data package Prepare a data package for submission to a public data repository Creating a data package inherently makes the originating study group's job of preparing the data package for submission to a public data repository easier!","title":"Introduction"},{"location":"#benefit-for-secondary-users","text":"The focus here is on creating data packages suitable for submission to a public data repository to permit replication of a previous analysis and/or secondary analyses. Such packages include the information necessary to make the data findable , accessible , interoperable and reusable ( FAIR ). In particular, they (1) include metadata that may be indexed and searched to permit researchers to find the data; (2) organize and store data and supporting files using common, open standards and formats that make them easy to use; and (3) include sufficient documentation, including information on provenance and data use requirements and restrictions, to permit researchers to understand and reuse the data.","title":"Benefit for Secondary Users"},{"location":"#benefit-for-originating-study-group-data-packages-without-data","text":"Importantly, while data packages are the best way to share primary research data with potential secondary users, they are also useful for the originating study group AND they are also useful even when the originating study group is generating data that cannot be shared or is working with secondary data that cannot be redistributed . A data package provides an ideal way to organize your work locally, increasing the efficiency and reproducibility of your own work and facilitating collaboration among your team. In addition, a data package may be easily modified to exclude all or some of the data themselves\u2014possibly replacing them with links to their sources in the case of secondary data or with synthetic data\u2014leaving you with a product that can then be shared in order to document your work and permit other researchers to replicate your analysis.","title":"Benefit for Originating study group &amp; Data Packages without data"},{"location":"#navigating-this-guide","text":"The material in this guide is organized into four main sections. The first section provides a general overview of data packages, including what they are, why they're useful, and when and how to make one. The second section is a step-by-step tutorial on creating your first data package, with instructions tailored to your own situation based on your answers to three simple questions. The third section is a step-by-step tutorial on preparing your first data package for submission to a public data repository, with instructions tailored to your own situation based on your answers to three simple questions. The fourth section provides additional guidance on specific topics as well as references to additional resources that may be helpful. Links to specific parts of this last section are provided throughout the first three sections to clarify concepts and recommendations as they come up.","title":"Navigating this guide"},{"location":"section1/","text":"Section 1 \u00b6 This is the first section","title":"Section 1"},{"location":"section1/#section-1","text":"This is the first section","title":"Section 1"},{"location":"annot/","text":"Different annotation approaches \u00b6 After you determine your data sharing orientation, you must determine how you will annotate your data. Specifically, you must determine whether you want to annotate all files included in your study, regardless of whether they will shared (wholistic annotation), or whether you will annotate only those files that are necessary to reproduce your results or dataset of interest (minimal annotation). Whether you annotate 'as you go' or retrospectively ('top-down') will largely be dictated by where you are in your study when you start to think about data sharing and annotation. There are three main ways to annotate: Wholistic, As you go annotation: Annotating all study files as they are created, regardless of whether they will be shared Wholistic, Top-down annotation: Starting with your result or dataset of interest and working backwards, annotating all study files necessary to reproduce the result, regardless of whether they will shared Minimal, Top down annotation : Starting with your result or dataset of interest and working backwards, annotating only study files necessary to reproduce the result that will be shared","title":"Index"},{"location":"annot/#different-annotation-approaches","text":"After you determine your data sharing orientation, you must determine how you will annotate your data. Specifically, you must determine whether you want to annotate all files included in your study, regardless of whether they will shared (wholistic annotation), or whether you will annotate only those files that are necessary to reproduce your results or dataset of interest (minimal annotation). Whether you annotate 'as you go' or retrospectively ('top-down') will largely be dictated by where you are in your study when you start to think about data sharing and annotation. There are three main ways to annotate: Wholistic, As you go annotation: Annotating all study files as they are created, regardless of whether they will be shared Wholistic, Top-down annotation: Starting with your result or dataset of interest and working backwards, annotating all study files necessary to reproduce the result, regardless of whether they will shared Minimal, Top down annotation : Starting with your result or dataset of interest and working backwards, annotating only study files necessary to reproduce the result that will be shared","title":"Different annotation approaches"},{"location":"annot/introaddtop/","text":"As-you-go annotation of study artefacts \u00b6 This form of annotation involves tracking and annotating study data and non-data/supporting files as you move through the study. This type of annotation works best when paired with wholistic annotation . As-you-go annotation is built into the study process and so does not require dedicating a significant time at study end to retrospectively gather files and annotate. If you are fairly early in your study and have collected minimal or no data, \u2018as-you-go\u2019 annotation is recommended. Top-down annotation of study artefacts \u00b6 This form of annotation involves tracking and annotating study data and non-data/supporting files at the end of your study, retrospectively. This type of annotation works best when paired with minimal annotation , however top-down annotation can also be wholistic . The decision of wholistic vs. minimal annotation may be determined by the amount of time and resources your study has set aside for data sharing, as wholistic annotation would be more time consuming in a top-down orientation. If you are later in your study and have collected almost all or all of your data, \u2018top-down\u2019 annotation is recommended.","title":"Introaddtop"},{"location":"annot/introaddtop/#as-you-go-annotation-of-study-artefacts","text":"This form of annotation involves tracking and annotating study data and non-data/supporting files as you move through the study. This type of annotation works best when paired with wholistic annotation . As-you-go annotation is built into the study process and so does not require dedicating a significant time at study end to retrospectively gather files and annotate. If you are fairly early in your study and have collected minimal or no data, \u2018as-you-go\u2019 annotation is recommended.","title":"As-you-go annotation of study artefacts"},{"location":"annot/introaddtop/#top-down-annotation-of-study-artefacts","text":"This form of annotation involves tracking and annotating study data and non-data/supporting files at the end of your study, retrospectively. This type of annotation works best when paired with minimal annotation , however top-down annotation can also be wholistic . The decision of wholistic vs. minimal annotation may be determined by the amount of time and resources your study has set aside for data sharing, as wholistic annotation would be more time consuming in a top-down orientation. If you are later in your study and have collected almost all or all of your data, \u2018top-down\u2019 annotation is recommended.","title":"Top-down annotation of study artefacts"},{"location":"annot/introholmin/","text":"Wholistic annotation of study artefacts \u00b6 Wholistic annotation involves creating and maintaining documentation that catalogues and annotates all files and non-data/supporting documents generated by and associated with your study regardless of whether you are planning to share them. Files that will not be shared are documented as permanent-private. This type of annotation works best paired with 'as you go' annotation , as it easier an less time consuming to annotate wholistically, retrospectively, as part of 'top-down' annotation . Although this mode of annotation is more time consuming, it maximizes transparency and allows investigators interested in the data to understand the full scope of the project when accessing study documentation on the Platform. This structure also allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by an investigator. Who may choose to share this way Study groups that have not started collecting data or are very early in the data collection process. Study groups that are earlier in the process and are interested in understanding and implementing a file and folder structure that facilitates data sharing in the future. Study groups that want to maximise the amount of information that they share about their study. Pros to sharing this way You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Although more time consuming at the beginning, integration of this process into your workflows allows for documentation and annotation in parts as you move through the study, so that you do not need to compile all that information at the end of the study, retrospectively. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Cons to sharing this way More time consuming, because it requires you to set up the structures to fully catalogue all data and non-data/supporting files that are relevant to your study. Notes about wholistic annotation If you are early in your study and choose to pursue wholistic annotation, we recommend you consider reviewing and implementing [HEAL recommendations for organizing and naming of study artefacts]. This will ease the process of annotation as well as future data sharing. If you are annotating wholistically, you will also document dependencies 'one-layer-deep'. For more information, see Resource Tracker: Associated Files/Dependencies and 'One Layer Deep' Dependencies . Minimal annotation of study artefacts \u00b6 Minimal annotation starts with the main product(s) you are focused on sharing, whether that is a publication, presentation, or dataset, and works backwards. Rather than documenting all files, you document only the data and non-data file dependencies required to reproduce the final results or dataset. This is likely more ideal for studies that may be close to data collection completion and are not focused on meeting data sharing requirements but do not have adequate funding or time to devote the complete documentation of the data. This type of annotation pairs well with \u2018top-down\u2019 annotation , which retrospectively annotates study artefacts. Who may choose to share this way Study groups that may be finished or close to finished collecting data and have already produced results files (e.g., figures, draft publications/NIH reports, etc.) Study groups that want to meet minimal data sharing requirements of sharing data underlying published results Pros to sharing this way You only catalog the data and non-data/supporting files that you will share/submit to a repository. This is less work than fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), especially if the study is well underway or complete/nearing completion and/or does not have resources or time set aside for a complete file inventory. This approach allows you to fulfill the minimal data sharing requirements of sharing data underlying published results. Cons to sharing this way You don\u2019t get the full local annotation benefit that would come with fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), and how they relate to each other and to published results \u2013 these benefits include facilitating continuity and passed-down knowledge within study groups , and discovery, sharing, and re-use of the data and knowledge produced by the study outside of the original study group. You don\u2019t get the full benefit of the added discoverability that data-package level metadata can provide. Notes about minimal annotation Even if you are not early in your study and are pursuing minimal annotation, we do not recommend reorganizing all your files or folder structures. However, we do recommend you consider reviewing and implementing naming conventions for like files, detailed in [HEAL recommendations for organizing and naming study artefacts]. This will ease the process of annotation. If you are annotating minimally, you will document dependences 'liberally'. For more information, see [Resource Tracker: Associated Files/Dependencies] adn ['Liberal' Dependencies].","title":"Introholmin"},{"location":"annot/introholmin/#wholistic-annotation-of-study-artefacts","text":"Wholistic annotation involves creating and maintaining documentation that catalogues and annotates all files and non-data/supporting documents generated by and associated with your study regardless of whether you are planning to share them. Files that will not be shared are documented as permanent-private. This type of annotation works best paired with 'as you go' annotation , as it easier an less time consuming to annotate wholistically, retrospectively, as part of 'top-down' annotation . Although this mode of annotation is more time consuming, it maximizes transparency and allows investigators interested in the data to understand the full scope of the project when accessing study documentation on the Platform. This structure also allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by an investigator. Who may choose to share this way Study groups that have not started collecting data or are very early in the data collection process. Study groups that are earlier in the process and are interested in understanding and implementing a file and folder structure that facilitates data sharing in the future. Study groups that want to maximise the amount of information that they share about their study. Pros to sharing this way You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Although more time consuming at the beginning, integration of this process into your workflows allows for documentation and annotation in parts as you move through the study, so that you do not need to compile all that information at the end of the study, retrospectively. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Cons to sharing this way More time consuming, because it requires you to set up the structures to fully catalogue all data and non-data/supporting files that are relevant to your study. Notes about wholistic annotation If you are early in your study and choose to pursue wholistic annotation, we recommend you consider reviewing and implementing [HEAL recommendations for organizing and naming of study artefacts]. This will ease the process of annotation as well as future data sharing. If you are annotating wholistically, you will also document dependencies 'one-layer-deep'. For more information, see Resource Tracker: Associated Files/Dependencies and 'One Layer Deep' Dependencies .","title":"Wholistic annotation of study artefacts"},{"location":"annot/introholmin/#minimal-annotation-of-study-artefacts","text":"Minimal annotation starts with the main product(s) you are focused on sharing, whether that is a publication, presentation, or dataset, and works backwards. Rather than documenting all files, you document only the data and non-data file dependencies required to reproduce the final results or dataset. This is likely more ideal for studies that may be close to data collection completion and are not focused on meeting data sharing requirements but do not have adequate funding or time to devote the complete documentation of the data. This type of annotation pairs well with \u2018top-down\u2019 annotation , which retrospectively annotates study artefacts. Who may choose to share this way Study groups that may be finished or close to finished collecting data and have already produced results files (e.g., figures, draft publications/NIH reports, etc.) Study groups that want to meet minimal data sharing requirements of sharing data underlying published results Pros to sharing this way You only catalog the data and non-data/supporting files that you will share/submit to a repository. This is less work than fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), especially if the study is well underway or complete/nearing completion and/or does not have resources or time set aside for a complete file inventory. This approach allows you to fulfill the minimal data sharing requirements of sharing data underlying published results. Cons to sharing this way You don\u2019t get the full local annotation benefit that would come with fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), and how they relate to each other and to published results \u2013 these benefits include facilitating continuity and passed-down knowledge within study groups , and discovery, sharing, and re-use of the data and knowledge produced by the study outside of the original study group. You don\u2019t get the full benefit of the added discoverability that data-package level metadata can provide. Notes about minimal annotation Even if you are not early in your study and are pursuing minimal annotation, we do not recommend reorganizing all your files or folder structures. However, we do recommend you consider reviewing and implementing naming conventions for like files, detailed in [HEAL recommendations for organizing and naming study artefacts]. This will ease the process of annotation. If you are annotating minimally, you will document dependences 'liberally'. For more information, see [Resource Tracker: Associated Files/Dependencies] adn ['Liberal' Dependencies].","title":"Minimal annotation of study artefacts"},{"location":"file-o-and-n/","text":"Guidance: File organization and naming \u00b6 File organization File naming","title":"Guidance: File organization and naming"},{"location":"file-o-and-n/#guidance-file-organization-and-naming","text":"File organization File naming","title":"Guidance: File organization and naming"},{"location":"fit/","text":"Your best fit \u00b6 While the who , what , where , and why of creating a data package are approximately the same for everyone, our recommendations for exactly when you start to create your data package, and how you go about it will differ based on a few key facts about your specific study and situation. Next steps When deciding the best fit for you with respect to how and when you will start creating your data package, you should Answer three key \"best fit\" questions , then Find your \"best fit\" annotation recommendations - Based on your answers to these three \"best fit\" questions, detailed guidance on how and when to create and complete your data package is provided","title":"Your best fit"},{"location":"fit/#your-best-fit","text":"While the who , what , where , and why of creating a data package are approximately the same for everyone, our recommendations for exactly when you start to create your data package, and how you go about it will differ based on a few key facts about your specific study and situation. Next steps When deciding the best fit for you with respect to how and when you will start creating your data package, you should Answer three key \"best fit\" questions , then Find your \"best fit\" annotation recommendations - Based on your answers to these three \"best fit\" questions, detailed guidance on how and when to create and complete your data package is provided","title":"Your best fit"},{"location":"fit/best-fit-annotation-recs/","text":"Best fit annotation recommendations \u00b6 Early \u00b6 Early, Results-support, Standard \u00b6 Study stage == Early Data-sharing goal == Results-support Data-sharing resources == Standard Use an \"As-you-go\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"As-you-go\" as soon as possible Audit all study files/resources already produced by or for your study Consider applying HEAL recommendations for file organization and naming to these files now, and to future files as they are created Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disc location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package Create a Data Dictionary for each existing tabular data file Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add all files already produced by/for your study to your Resource Tracker Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities which have already been designed to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field What to do continuously, as-you-go As you create/collect new study files: Consider applying HEAL recommendations for file organization and naming to study files/resources as they are created If you came up with file organization and naming conventions based on HEAL recommendations at the start of or earlier in your study, continue to consistently apply those conventions as you create/collect new study files Add new study files/resources to your Resource Tracker as they are created/collected (as soon as possible) As you create/collect new tabular data files: Create a Data Dictionary for tabular data files as they are created As you design new study experiments/activities: Add new study experiments/activities to your Experiment Tracker as they are designed (as soon as possible) What to do when you start to create final result products For each final result product (e.g. figure, figure panel, table, text statement), as you create it, audit the full subset of associated files/dependencies for that final result product (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies Start your Results Tracker(s) - one per manuscript or other product containing results (e.g., presentation or poster) Start to create a Results Tracker to document the final result products (e.g. figures, tables) that will be included in a published paper or report Start your Results Tracker by initializing an empty Results Tracker file based on the Results Tracker csv template Save your Results Tracker in your \"dsc-pkg\" folder as \"heal-csv-results-tracker-my-manuscript-file.csv\" (i.e. the file name starts with the prefix \"heal-csv-results-tracker-\", you append the name of the manuscript or report to which the Results Tracker applies, and save as a csv file) Add all final result products already produced by/for your study to a Results Tracker Use the Results Tracker schema to understand what each \"question\"/field in the Results Tracker means and how to \"answer\"/complete each \"question\"/field The Results Tracker will ask you to list associated files/dependencies for each final result product (i.e. files that are required to interpret, replicate, or use the result) As you produce more final result products add them to a Results Tracker as soon as possible What to do when your manuscript is finalized Finalize the Results Tracker for the completed manuscript Audit the final result products (e.g. figure, figure panel, table, text statement) produced by your study that are shared in the manuscript - create a list of final result products Check that all final result products shared in the manuscript are listed in the associated Results Tracker - Add any that are missing Check that figure/table numbers for all final result products shared in the manuscript are accurately reflected in the final result product entry in the manuscript's associated Results Tracker - Correct any that need to be updated Add your manuscript and its associated Results Tracker to your Resource Tracker Check that all final result product associated files/dependencies (listed in the associated files/dependencies fields for each final result product listed in the Results Tracker) are also listed as study resource/files in your Resource Tracker - Add any that are missing Early, Results-support, Low \u00b6 Study stage == Early Data-sharing goal == Results-support Data-sharing resources == Low What to do right away Audit all study files/resources already produced by or for your study Consider applying HEAL recommendations for file organization and naming to these files now, and to future files as they are created Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) Then follow best fit annotation recommendations for : Late, Results-support, Low Early, Dataset-sharing, Standard \u00b6 Study stage == Early Data-sharing goal == Dataset-sharing Data-sharing resources == Standard Use an \"As-you-go\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"As-you-go\" as soon as possible Audit all study files/resources already produced by or for your study Consider applying HEAL recommendations for file organization and naming to these files now, and to future files as they are created Where possible, organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disc location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package Create a Data Dictionary for each existing tabular data file Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add all files already produced by/for your study to your Resource Tracker Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities which have already been designed to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field What to do continuously, as-you-go As you create/collect new study files: Consider applying HEAL recommendations for file organization and naming to study files/resources as they are created If you came up with file organization and naming conventions based on HEAL recommendations at the start of or earlier in your study, continue to consistently apply those conventions as you create/collect new study files Add new study files/resources to your Resource Tracker as they are created/collected (as soon as possible) As you create/collect new tabular data files: Create a Data Dictionary for tabular data files as they are created As you design new study experiments/activities: Add new study experiments/activities to your Experiment Tracker as they are designed (as soon as possible) What to do when your dataset-of-interest is finalized Add your finalized dataset-of-interest to your Resource Tracker Check that all associated files/dependencies for the finalized dataset-of-interest (i.e. files required to interpret, replicate, or use the finalized dataset-of-interest) are also listed as study resource/files in your Resource Tracker - Add any that are missing Early, Dataset-sharing, Low \u00b6 Study stage == Early Data-sharing goal == Dataset-sharing Data-sharing resources == Low What to do right away Audit all study files/resources already produced by or for your study Consider applying HEAL recommendations for file organization and naming to these files now, and to future files as they are created Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) Then follow best fit annotation recommendations for : Late, Dataset-sharing, Low Late \u00b6 Late, Results-support, Standard \u00b6 Study stage == Late Data-sharing goal == Results-support Data-sharing resources == Standard Use a \"Top-down\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"Top-down\" as soon as possible Audit all study files/resources already produced by or for your study If you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error Therefore, we generally recommend that you leave file names and organization as is - However, we do request that you consider the following exceptions: Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) If you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so may make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disk location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package What to do when your manuscript is finalized Audit the final result products (e.g. figure, figure panel, table, text statement) produced by your study that are shared in the manuscript - create a list of final result products For each final result product shared in your manuscript, Audit the full set of associated files/dependencies for that final result product (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies Audit the full set of study experiments/activities that produced supporting data or other support for that final result product If any associated file/dependency for any of the final result products is a tabular data file , create a Data Dictionary for each associated file/dependency that is a tabular data file, AND add this Data Dictionary to the list of associated files/dependencies for that final result product Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Results Tracker(s) - one per manuscript or report Start to create a Results Tracker to document the final result products (e.g. figures, tables) that will be published as part of your manuscript or report Start your Results Tracker by initializing an empty Results Tracker file based on the Results Tracker csv template Save your Results Tracker in your \"dsc-pkg\" folder as \"heal-csv-results-tracker-my-manuscript-file.csv\" (i.e. the file name starts with the prefix \"heal-csv-results-tracker-\", you append the name of the manuscript or report to which the Results Tracker applies, and save as a csv file) Add all final result products shared in the manuscript or report to a Results Tracker Use the Results Tracker schema to understand what each \"question\"/field in the Results Tracker means and how to \"answer\"/complete each \"question\"/field The Results Tracker will ask you to list associated files/dependencies for each final result product (i.e. files that are required to interpret, replicate, or use the result) Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities that produced supporting data or other support for any of the final result products shared in your manuscript to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field Finalize the Results Tracker for the completed manuscript Check that all final result products shared in the manuscript or report are listed in the associated Results Tracker - Add any that are missing Check that figure/table numbers for all final result products shared in the manuscript are accurately reflected in the final result product entry in the manuscript's associated Results Tracker - Correct any that need to be updated Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add files produced by/for your study to your Resource Tracker (see next bullet for guidance on which files to add and where to start) Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Add your manuscript and its associated Results Tracker to your Resource Tracker Do not add the Results Tracker for your manuscript to your Resource Tracker until it is finalized For each final result product shared in your manuscript, add each of the associated files/dependencies for that final result product to your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies Check that all final result product associated files/dependencies (listed in the associated files/dependencies fields for each final result product listed in the Results Tracker) are also listed as study resource/files in your Resource Tracker - Add any that are missing Late, Results-support, Low \u00b6 Study stage == Late Data-sharing goal == Results-support Data-sharing resources == Low Use a \"Top-down\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"Top-down\" as soon as possible Audit all study files/resources already produced by or for your study If you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error Therefore, we generally recommend that you leave file names and organization as is - However, we do request that you consider the following exceptions: Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) If you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so may make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disk location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package What to do when your manuscript is finalized Audit the final result products (e.g. figure, figure panel, table, text statement) produced by your study that are shared in the manuscript - create a list of final result products For each final result product shared in your manuscript, Audit the full set of associated files/dependencies for that final result product (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies Audit the full set of study experiments/activities that produced supporting data or other support for that final result product If any associated file/dependency for any of the final result products is a tabular data file , create a Data Dictionary for each associated file/dependency that is a tabular data file, AND add this Data Dictionary to the list of associated files/dependencies for that final result product Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Results Tracker(s) - one per manuscript Start to create a Results Tracker to document the final result products (e.g. figures, tables) that will be published as part of your manuscript or report Start your Results Tracker by initializing an empty Results Tracker file based on the Results Tracker csv template Save your Results Tracker in your \"dsc-pkg\" folder as \"heal-csv-results-tracker-my-manuscript-file.csv\" (i.e. the file name starts with the prefix \"heal-csv-results-tracker-\", you append the name of the manuscript or report to which the Results Tracker applies, and save as a csv file) Add all final result products shared in the manuscript or report to a Results Tracker Use the Results Tracker schema to understand what each \"question\"/field in the Results Tracker means and how to \"answer\"/complete each \"question\"/field The Results Tracker will ask you to list associated files/dependencies for each final result product (i.e. files that are required to interpret, replicate, or use the result) Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities that produced supporting data or other support for any of the final result products shared in your manuscript to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field Finalize the Results Tracker for the completed manuscript or report Check that all final result products shared in the manuscript or report are listed in the associated Results Tracker - Add any that are missing Check that figure/table numbers for all final result products shared in the manuscript are accurately reflected in the final result product entry in the manuscript's associated Results Tracker - Correct any that need to be updated Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add files produced by/for your study to your Resource Tracker (see next bullet for guidance on which files to add and where to start) Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Add your manuscript and its associated Results Tracker to your Resource Tracker Do not add the Results Tracker for your manuscript to your Resource Tracker until it is finalized For each final result product shared in your manuscript, review your audit list of associated files/dependencies for that result product (including associated files/dependencies of immediate dependencies), and determine which of these files will be shared in a public repository For each final result product shared in your manuscript, add each of the associated files/dependencies for that final result product to your Resource Tracker (including associated files/dependencies of immediate associated files/dependencies) ONLY if the file will be shared in a public repository Check that all final result product associated files/dependencies (listed in the associated files/dependencies fields for each final result product listed in the Results Tracker) are also listed as study resource/files in your Resource Tracker ONLY if the file will be shared in a public repository - Add any that are missing Late, Dataset-sharing, Standard \u00b6 Study stage == Late Data-sharing goal == Dataset-sharing Data-sharing resources == Standard Use a \"Top-down\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"Top-down\" as soon as possible Audit all study files/resources already produced by or for your study If you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error Therefore, we generally recommend that you leave file names and organization as is - However, we do request that you consider the following exceptions: Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) If you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so may make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disk location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package What to do when your dataset-of-interest is finalized Audit the full set of study experiments/activities that produced supporting data or other support for the final dataset-of-interest Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities that produced supporting data or other support for the final dataset-of-interest to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field Audit the full set of associated files/dependencies for the final dataset-of-interest (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies If the dataset-of-interest, or any associated file/dependency of the dataset-of-interest is a tabular data file , create a Data Dictionary for the tabular dataset-of-interest and for each associated file/dependency that is a tabular data file, AND add these Data Dictionaries to the list of associated files/dependencies for the dataset-of-interest Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add files produced by/for your study to your Resource Tracker (see next bullet for guidance on which files to add and where to start) Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Add your finalized dataset-of-interest to your Resource Tracker Add each of the associated files/dependencies for the final dataset-of-interest to your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies Check that all associated files/dependencies for the finalized dataset-of-interest (i.e. files required to interpret, replicate, or use the finalized dataset-of-interest) are also listed as study resource/files in your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies - Add any that are missing Late, Dataset-sharing, Low \u00b6 Study stage == Late Data-sharing goal == Dataset-sharing Data-sharing resources == Low Use a \"Top-down\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"Top-down\" as soon as possible Audit all study files/resources already produced by or for your study If you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error Therefore, we generally recommend that you leave file names and organization as is - However, we do request that you consider the following exceptions: Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) If you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so may make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disk location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package What to do when your dataset-of-interest is finalized Audit the full set of study experiments/activities that produced supporting data or other support for the final dataset-of-interest Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities that produced supporting data or other support for the final dataset-of-interest to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field Audit the full set of associated files/dependencies for the final dataset-of-interest (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies If the dataset-of-interest, or any associated file/dependency of the dataset-of-interest is a tabular data file , create a Data Dictionary for the tabular dataset-of-interest and for each associated file/dependency that is a tabular data file, AND add these Data Dictionaries to the list of associated files/dependencies for the dataset-of-interest Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add files produced by/for your study to your Resource Tracker (see next bullet for guidance on which files to add and where to start) Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Add your finalized dataset-of-interest to your Resource Tracker Review your audit list of associated files/dependencies for the final dataset-of-interest (including associated files/dependencies of immediate dependencies), and determine which of these files will be shared in a public repository Add each of the associated files/dependencies for the final dataset-of-interest to your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies ONLY if the file will be shared in a public repository Check that all associated files/dependencies for the finalized dataset-of-interest (i.e. files required to interpret, replicate, or use the finalized dataset-of-interest) are also listed as study resource/files in your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies ONLY if the file will be shared in a public repository - Add any that are missing","title":"Best fit annotation recs"},{"location":"fit/best-fit-annotation-recs/#best-fit-annotation-recommendations","text":"","title":"Best fit annotation recommendations"},{"location":"fit/best-fit-annotation-recs/#early","text":"","title":"Early"},{"location":"fit/best-fit-annotation-recs/#early-results-support-standard","text":"Study stage == Early Data-sharing goal == Results-support Data-sharing resources == Standard Use an \"As-you-go\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"As-you-go\" as soon as possible Audit all study files/resources already produced by or for your study Consider applying HEAL recommendations for file organization and naming to these files now, and to future files as they are created Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disc location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package Create a Data Dictionary for each existing tabular data file Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add all files already produced by/for your study to your Resource Tracker Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities which have already been designed to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field What to do continuously, as-you-go As you create/collect new study files: Consider applying HEAL recommendations for file organization and naming to study files/resources as they are created If you came up with file organization and naming conventions based on HEAL recommendations at the start of or earlier in your study, continue to consistently apply those conventions as you create/collect new study files Add new study files/resources to your Resource Tracker as they are created/collected (as soon as possible) As you create/collect new tabular data files: Create a Data Dictionary for tabular data files as they are created As you design new study experiments/activities: Add new study experiments/activities to your Experiment Tracker as they are designed (as soon as possible) What to do when you start to create final result products For each final result product (e.g. figure, figure panel, table, text statement), as you create it, audit the full subset of associated files/dependencies for that final result product (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies Start your Results Tracker(s) - one per manuscript or other product containing results (e.g., presentation or poster) Start to create a Results Tracker to document the final result products (e.g. figures, tables) that will be included in a published paper or report Start your Results Tracker by initializing an empty Results Tracker file based on the Results Tracker csv template Save your Results Tracker in your \"dsc-pkg\" folder as \"heal-csv-results-tracker-my-manuscript-file.csv\" (i.e. the file name starts with the prefix \"heal-csv-results-tracker-\", you append the name of the manuscript or report to which the Results Tracker applies, and save as a csv file) Add all final result products already produced by/for your study to a Results Tracker Use the Results Tracker schema to understand what each \"question\"/field in the Results Tracker means and how to \"answer\"/complete each \"question\"/field The Results Tracker will ask you to list associated files/dependencies for each final result product (i.e. files that are required to interpret, replicate, or use the result) As you produce more final result products add them to a Results Tracker as soon as possible What to do when your manuscript is finalized Finalize the Results Tracker for the completed manuscript Audit the final result products (e.g. figure, figure panel, table, text statement) produced by your study that are shared in the manuscript - create a list of final result products Check that all final result products shared in the manuscript are listed in the associated Results Tracker - Add any that are missing Check that figure/table numbers for all final result products shared in the manuscript are accurately reflected in the final result product entry in the manuscript's associated Results Tracker - Correct any that need to be updated Add your manuscript and its associated Results Tracker to your Resource Tracker Check that all final result product associated files/dependencies (listed in the associated files/dependencies fields for each final result product listed in the Results Tracker) are also listed as study resource/files in your Resource Tracker - Add any that are missing","title":"Early, Results-support, Standard"},{"location":"fit/best-fit-annotation-recs/#early-results-support-low","text":"Study stage == Early Data-sharing goal == Results-support Data-sharing resources == Low What to do right away Audit all study files/resources already produced by or for your study Consider applying HEAL recommendations for file organization and naming to these files now, and to future files as they are created Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) Then follow best fit annotation recommendations for : Late, Results-support, Low","title":"Early, Results-support, Low"},{"location":"fit/best-fit-annotation-recs/#early-dataset-sharing-standard","text":"Study stage == Early Data-sharing goal == Dataset-sharing Data-sharing resources == Standard Use an \"As-you-go\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"As-you-go\" as soon as possible Audit all study files/resources already produced by or for your study Consider applying HEAL recommendations for file organization and naming to these files now, and to future files as they are created Where possible, organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disc location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package Create a Data Dictionary for each existing tabular data file Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add all files already produced by/for your study to your Resource Tracker Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities which have already been designed to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field What to do continuously, as-you-go As you create/collect new study files: Consider applying HEAL recommendations for file organization and naming to study files/resources as they are created If you came up with file organization and naming conventions based on HEAL recommendations at the start of or earlier in your study, continue to consistently apply those conventions as you create/collect new study files Add new study files/resources to your Resource Tracker as they are created/collected (as soon as possible) As you create/collect new tabular data files: Create a Data Dictionary for tabular data files as they are created As you design new study experiments/activities: Add new study experiments/activities to your Experiment Tracker as they are designed (as soon as possible) What to do when your dataset-of-interest is finalized Add your finalized dataset-of-interest to your Resource Tracker Check that all associated files/dependencies for the finalized dataset-of-interest (i.e. files required to interpret, replicate, or use the finalized dataset-of-interest) are also listed as study resource/files in your Resource Tracker - Add any that are missing","title":"Early, Dataset-sharing, Standard"},{"location":"fit/best-fit-annotation-recs/#early-dataset-sharing-low","text":"Study stage == Early Data-sharing goal == Dataset-sharing Data-sharing resources == Low What to do right away Audit all study files/resources already produced by or for your study Consider applying HEAL recommendations for file organization and naming to these files now, and to future files as they are created Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) Then follow best fit annotation recommendations for : Late, Dataset-sharing, Low","title":"Early, Dataset-sharing, Low"},{"location":"fit/best-fit-annotation-recs/#late","text":"","title":"Late"},{"location":"fit/best-fit-annotation-recs/#late-results-support-standard","text":"Study stage == Late Data-sharing goal == Results-support Data-sharing resources == Standard Use a \"Top-down\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"Top-down\" as soon as possible Audit all study files/resources already produced by or for your study If you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error Therefore, we generally recommend that you leave file names and organization as is - However, we do request that you consider the following exceptions: Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) If you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so may make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disk location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package What to do when your manuscript is finalized Audit the final result products (e.g. figure, figure panel, table, text statement) produced by your study that are shared in the manuscript - create a list of final result products For each final result product shared in your manuscript, Audit the full set of associated files/dependencies for that final result product (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies Audit the full set of study experiments/activities that produced supporting data or other support for that final result product If any associated file/dependency for any of the final result products is a tabular data file , create a Data Dictionary for each associated file/dependency that is a tabular data file, AND add this Data Dictionary to the list of associated files/dependencies for that final result product Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Results Tracker(s) - one per manuscript or report Start to create a Results Tracker to document the final result products (e.g. figures, tables) that will be published as part of your manuscript or report Start your Results Tracker by initializing an empty Results Tracker file based on the Results Tracker csv template Save your Results Tracker in your \"dsc-pkg\" folder as \"heal-csv-results-tracker-my-manuscript-file.csv\" (i.e. the file name starts with the prefix \"heal-csv-results-tracker-\", you append the name of the manuscript or report to which the Results Tracker applies, and save as a csv file) Add all final result products shared in the manuscript or report to a Results Tracker Use the Results Tracker schema to understand what each \"question\"/field in the Results Tracker means and how to \"answer\"/complete each \"question\"/field The Results Tracker will ask you to list associated files/dependencies for each final result product (i.e. files that are required to interpret, replicate, or use the result) Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities that produced supporting data or other support for any of the final result products shared in your manuscript to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field Finalize the Results Tracker for the completed manuscript Check that all final result products shared in the manuscript or report are listed in the associated Results Tracker - Add any that are missing Check that figure/table numbers for all final result products shared in the manuscript are accurately reflected in the final result product entry in the manuscript's associated Results Tracker - Correct any that need to be updated Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add files produced by/for your study to your Resource Tracker (see next bullet for guidance on which files to add and where to start) Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Add your manuscript and its associated Results Tracker to your Resource Tracker Do not add the Results Tracker for your manuscript to your Resource Tracker until it is finalized For each final result product shared in your manuscript, add each of the associated files/dependencies for that final result product to your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies Check that all final result product associated files/dependencies (listed in the associated files/dependencies fields for each final result product listed in the Results Tracker) are also listed as study resource/files in your Resource Tracker - Add any that are missing","title":"Late, Results-support, Standard"},{"location":"fit/best-fit-annotation-recs/#late-results-support-low","text":"Study stage == Late Data-sharing goal == Results-support Data-sharing resources == Low Use a \"Top-down\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"Top-down\" as soon as possible Audit all study files/resources already produced by or for your study If you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error Therefore, we generally recommend that you leave file names and organization as is - However, we do request that you consider the following exceptions: Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) If you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so may make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disk location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package What to do when your manuscript is finalized Audit the final result products (e.g. figure, figure panel, table, text statement) produced by your study that are shared in the manuscript - create a list of final result products For each final result product shared in your manuscript, Audit the full set of associated files/dependencies for that final result product (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies Audit the full set of study experiments/activities that produced supporting data or other support for that final result product If any associated file/dependency for any of the final result products is a tabular data file , create a Data Dictionary for each associated file/dependency that is a tabular data file, AND add this Data Dictionary to the list of associated files/dependencies for that final result product Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Results Tracker(s) - one per manuscript Start to create a Results Tracker to document the final result products (e.g. figures, tables) that will be published as part of your manuscript or report Start your Results Tracker by initializing an empty Results Tracker file based on the Results Tracker csv template Save your Results Tracker in your \"dsc-pkg\" folder as \"heal-csv-results-tracker-my-manuscript-file.csv\" (i.e. the file name starts with the prefix \"heal-csv-results-tracker-\", you append the name of the manuscript or report to which the Results Tracker applies, and save as a csv file) Add all final result products shared in the manuscript or report to a Results Tracker Use the Results Tracker schema to understand what each \"question\"/field in the Results Tracker means and how to \"answer\"/complete each \"question\"/field The Results Tracker will ask you to list associated files/dependencies for each final result product (i.e. files that are required to interpret, replicate, or use the result) Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities that produced supporting data or other support for any of the final result products shared in your manuscript to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field Finalize the Results Tracker for the completed manuscript or report Check that all final result products shared in the manuscript or report are listed in the associated Results Tracker - Add any that are missing Check that figure/table numbers for all final result products shared in the manuscript are accurately reflected in the final result product entry in the manuscript's associated Results Tracker - Correct any that need to be updated Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add files produced by/for your study to your Resource Tracker (see next bullet for guidance on which files to add and where to start) Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Add your manuscript and its associated Results Tracker to your Resource Tracker Do not add the Results Tracker for your manuscript to your Resource Tracker until it is finalized For each final result product shared in your manuscript, review your audit list of associated files/dependencies for that result product (including associated files/dependencies of immediate dependencies), and determine which of these files will be shared in a public repository For each final result product shared in your manuscript, add each of the associated files/dependencies for that final result product to your Resource Tracker (including associated files/dependencies of immediate associated files/dependencies) ONLY if the file will be shared in a public repository Check that all final result product associated files/dependencies (listed in the associated files/dependencies fields for each final result product listed in the Results Tracker) are also listed as study resource/files in your Resource Tracker ONLY if the file will be shared in a public repository - Add any that are missing","title":"Late, Results-support, Low"},{"location":"fit/best-fit-annotation-recs/#late-dataset-sharing-standard","text":"Study stage == Late Data-sharing goal == Dataset-sharing Data-sharing resources == Standard Use a \"Top-down\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"Top-down\" as soon as possible Audit all study files/resources already produced by or for your study If you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error Therefore, we generally recommend that you leave file names and organization as is - However, we do request that you consider the following exceptions: Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) If you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so may make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disk location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package What to do when your dataset-of-interest is finalized Audit the full set of study experiments/activities that produced supporting data or other support for the final dataset-of-interest Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities that produced supporting data or other support for the final dataset-of-interest to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field Audit the full set of associated files/dependencies for the final dataset-of-interest (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies If the dataset-of-interest, or any associated file/dependency of the dataset-of-interest is a tabular data file , create a Data Dictionary for the tabular dataset-of-interest and for each associated file/dependency that is a tabular data file, AND add these Data Dictionaries to the list of associated files/dependencies for the dataset-of-interest Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add files produced by/for your study to your Resource Tracker (see next bullet for guidance on which files to add and where to start) Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Add your finalized dataset-of-interest to your Resource Tracker Add each of the associated files/dependencies for the final dataset-of-interest to your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies Check that all associated files/dependencies for the finalized dataset-of-interest (i.e. files required to interpret, replicate, or use the finalized dataset-of-interest) are also listed as study resource/files in your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies - Add any that are missing","title":"Late, Dataset-sharing, Standard"},{"location":"fit/best-fit-annotation-recs/#late-dataset-sharing-low","text":"Study stage == Late Data-sharing goal == Dataset-sharing Data-sharing resources == Low Use a \"Top-down\" annotation approach; a detailed overview of what this involves can be found here ; a brief overview of what this involves follows below: What to do right away Start annotating \"Top-down\" as soon as possible Audit all study files/resources already produced by or for your study If you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error Therefore, we generally recommend that you leave file names and organization as is - However, we do request that you consider the following exceptions: Where practicable to implement (without duplicating original files), organize all study files/resources into a single study folder/directory (study folder/directory may of course have sub-directories; see here for guidance on and examples of recommended study folder/directory structure) If you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so may make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Initialize your Data Package Create a \"dsc-pkg\" folder/directory that will hold all Standard Data Package Metadata Files for your data package If all study files/resources are organized into a single study folder/directory, create this folder/directory as a direct sub-directory of your study folder/directory, and name it \"dsc-pkg\"; consistency in naming and location of this folder/directory relative to your overall study folder/directory will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package If not, create this folder/directory in a disk location that makes sense for you; name it \"dsc-pkg\", optionally appending a suffix to the name that will make it easy to recognize as \"belonging\" to a specific study (e.g. \"dsc-pkg-study-1\" or \"dsc-pkg-mindfulness-for-oud\"); consistency in naming (i.e. including the \"dsc-pkg\" prefix) and appending a suffix to the name that is a human-recongnizable identifier for the relevant study will make it easy to recognize as the folder that contains the Standard Data Package Metadata files for your study's data package What to do when your dataset-of-interest is finalized Audit the full set of study experiments/activities that produced supporting data or other support for the final dataset-of-interest Start your Experiment Tracker Start your Experiment Tracker by initializing an empty Experiment Tracker file based on the Experiment Tracker csv template Save your Experiment Tracker in your \"dsc-pkg\" folder as \"heal-csv-experiment-tracker.csv\" Add all experiments or other study activities that produced supporting data or other support for the final dataset-of-interest to your Experiment Tracker Use the Experiment Tracker schema to understand what each \"question\"/field in the Experiment Tracker means and how to \"answer\"/complete each \"question\"/field Audit the full set of associated files/dependencies for the final dataset-of-interest (i.e. all study files/resources required to interpret, replicate, or use the final result product), including associated files/dependencies of immediate associated files/dependencies If the dataset-of-interest, or any associated file/dependency of the dataset-of-interest is a tabular data file , create a Data Dictionary for the tabular dataset-of-interest and for each associated file/dependency that is a tabular data file, AND add these Data Dictionaries to the list of associated files/dependencies for the dataset-of-interest Start a Data Dictionary for a tabular data file by initializing an empty Data Dictionary file based on the Data Dictionary csv template Save your Data Dictionary in your \"dsc-pkg\" folder as \"heal-csv-dd-my-datafile.csv\" (i.e. the file name starts with the prefix \"heal-csv-dd-\", you append the name of the data file to which the Data Dictionary applies, and save as a csv file) Add all variables in the tabular data file to your Data Dictionary Use the Data Dictionary schema to understand what each \"question\"/field in the Data Dictionary means and how to \"answer\"/complete each \"question\"/field Start your Resource Tracker Start your Resource Tracker by initializing an empty Resource Tracker file based on the Resource Tracker csv template Save your Resource Tracker in your \"dsc-pkg\" folder as \"heal-csv-resource-tracker.csv\" Add files produced by/for your study to your Resource Tracker (see next bullet for guidance on which files to add and where to start) Use the Resource Tracker schema to understand what each \"question\"/field in the Resource Tracker means and how to \"answer\"/complete each \"question\"/field The Resource Tracker will ask you to list associated files/dependencies for each study file/resource (i.e. files that are required to interpret, replicate, or use the study file/resource) Add your finalized dataset-of-interest to your Resource Tracker Review your audit list of associated files/dependencies for the final dataset-of-interest (including associated files/dependencies of immediate dependencies), and determine which of these files will be shared in a public repository Add each of the associated files/dependencies for the final dataset-of-interest to your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies ONLY if the file will be shared in a public repository Check that all associated files/dependencies for the finalized dataset-of-interest (i.e. files required to interpret, replicate, or use the finalized dataset-of-interest) are also listed as study resource/files in your Resource Tracker, including associated files/dependencies of immediate associated files/dependencies ONLY if the file will be shared in a public repository - Add any that are missing","title":"Late, Dataset-sharing, Low"},{"location":"fit/best-fit-questions/","text":"Best fit questions \u00b6 When deciding the best fit for you with respect to how and when you will start creating your data package, you should consider three key questions : How far along are you in your study? What is your data-sharing goal? Do you have low or no resources to devote to data-sharing efforts? Next steps \u00b6 Move through the \"best fit\" questions below . Select and record the answer option to each question that best fits you and your study. Based upon your study's answer selections, detailed guidance on how and when to create and complete your data package will be provided. The questions \u00b6 Study Stage \u00b6 How far along are you in your study? How far along you are in your study when you start to think about data sharing will affect how you document your data. For example, if you are near the beginning of your study, you may be able to create your data package and set up documentation processes such that you document as you move through your study. This will lead to more complete documentation and ease the process of data sharing in the future, as opposed to completing all documentation at the end of your study. If you are nearer the end of your study, we recommend you take a narrower, more goal-oriented approach to documentation to minimize data sharing burden on your study group, while maximizing value to potential secondary data users and collaboraters. There are two options available for study stage. These study stage options are greatly simplified. For now, select the study stage that most closely fits with the stage you are at in your study . Value options Early Late Data-sharing Orientation \u00b6 What is your data-sharing goal? Determining what your goals are for data sharing will help guide you in determining what you will include in your future data package and how you will annotate the data and non-data supporting files/resources in your data package. People will have different goals in sharing data and non-data supporting files/resources from their study. These goals, or 'orientations' may be dictated by a number of factors such as the nature of the data, study staff resources and time constraints, and investigator preference. There are two main data sharing \"orientations.\" These data sharing goals or \"orientations\" are not mutually exclusive. You may choose both Results-support and Dataset-sharing as goals for your study's data sharing goal. See more about each data sharing orientation below, and select the data-sharing orientation(s) that most closely fit(s) with your study group's data sharing goals. Value options Results-support Dataset-sharing Data-sharing Resources \u00b6 Do you have low or no resources to devote to data sharing efforts? While it is sometimes the case that study groups don't explicitly budget and plan resources for data sharing efforts, many study groups have enough resources overall that they find they can still set some staff time and effort aside for data sharing efforts that will benefit potential secondary data users, the entire research community, and the originating study group. However in some cases, a study group may find themselves with a particularly low level of resources available to devote to data sharing efforts at the time they begin to consider fulfilling data sharing requirements attached to their study. They may for example have started out with a pretty tight study budget, or have already spent down most or all of the study budget, or have staff turnover that makes it difficult to spare person-time to the effort. Determining whether your study group may be in a more common position of having a standard level of resources available for data-sharing efforts, or whether your study group may be in the less common position of having a very low level of resources available will help guide you in determining how you will annotate the data and non-data supporting files/resources in your data package. There are two options available for data-sharing resources. These data-sharing resources options are greatly simplified. For now, select the data-sharing resources option that most closely fits with your study group's level of available data-sharing resources . Value options Standard Low","title":"Best fit questions"},{"location":"fit/best-fit-questions/#best-fit-questions","text":"When deciding the best fit for you with respect to how and when you will start creating your data package, you should consider three key questions : How far along are you in your study? What is your data-sharing goal? Do you have low or no resources to devote to data-sharing efforts?","title":"Best fit questions"},{"location":"fit/best-fit-questions/#next-steps","text":"Move through the \"best fit\" questions below . Select and record the answer option to each question that best fits you and your study. Based upon your study's answer selections, detailed guidance on how and when to create and complete your data package will be provided.","title":"Next steps"},{"location":"fit/best-fit-questions/#the-questions","text":"","title":"The questions"},{"location":"fit/best-fit-questions/#study-stage","text":"How far along are you in your study? How far along you are in your study when you start to think about data sharing will affect how you document your data. For example, if you are near the beginning of your study, you may be able to create your data package and set up documentation processes such that you document as you move through your study. This will lead to more complete documentation and ease the process of data sharing in the future, as opposed to completing all documentation at the end of your study. If you are nearer the end of your study, we recommend you take a narrower, more goal-oriented approach to documentation to minimize data sharing burden on your study group, while maximizing value to potential secondary data users and collaboraters. There are two options available for study stage. These study stage options are greatly simplified. For now, select the study stage that most closely fits with the stage you are at in your study . Value options Early Late","title":"Study Stage"},{"location":"fit/best-fit-questions/#data-sharing-orientation","text":"What is your data-sharing goal? Determining what your goals are for data sharing will help guide you in determining what you will include in your future data package and how you will annotate the data and non-data supporting files/resources in your data package. People will have different goals in sharing data and non-data supporting files/resources from their study. These goals, or 'orientations' may be dictated by a number of factors such as the nature of the data, study staff resources and time constraints, and investigator preference. There are two main data sharing \"orientations.\" These data sharing goals or \"orientations\" are not mutually exclusive. You may choose both Results-support and Dataset-sharing as goals for your study's data sharing goal. See more about each data sharing orientation below, and select the data-sharing orientation(s) that most closely fit(s) with your study group's data sharing goals. Value options Results-support Dataset-sharing","title":"Data-sharing Orientation"},{"location":"fit/best-fit-questions/#data-sharing-resources","text":"Do you have low or no resources to devote to data sharing efforts? While it is sometimes the case that study groups don't explicitly budget and plan resources for data sharing efforts, many study groups have enough resources overall that they find they can still set some staff time and effort aside for data sharing efforts that will benefit potential secondary data users, the entire research community, and the originating study group. However in some cases, a study group may find themselves with a particularly low level of resources available to devote to data sharing efforts at the time they begin to consider fulfilling data sharing requirements attached to their study. They may for example have started out with a pretty tight study budget, or have already spent down most or all of the study budget, or have staff turnover that makes it difficult to spare person-time to the effort. Determining whether your study group may be in a more common position of having a standard level of resources available for data-sharing efforts, or whether your study group may be in the less common position of having a very low level of resources available will help guide you in determining how you will annotate the data and non-data supporting files/resources in your data package. There are two options available for data-sharing resources. These data-sharing resources options are greatly simplified. For now, select the data-sharing resources option that most closely fits with your study group's level of available data-sharing resources . Value options Standard Low","title":"Data-sharing Resources"},{"location":"fit/data-sharing-orientation/","text":"What is your data sharing orientation? \u00b6 Determining what your goals are for data sharing will help guide you in determining what you will include in your future data package and how you will annotate the data and non-data supporting files/resources in your data package. People will have different goals in sharing data and non-data supporting files/resources from their study. These goals, or 'orientations' may be dictated by a number of factors such as the nature of the data, study staff resources and time constraints, and investigator preference. There are two main data sharing \"orientations.\" Data sharing \"orientations\" \u00b6 Results-support Dataset-sharing These data sharing goals or \"orientations\" are not mutually exclusive . You may choose both Results-support and Dataset-sharing as goals for your study's data sharing approach. See more about each data sharing orientation below, and decide if one, or both, apply to your study. Results-support Select this data sharing orientation if you are primarily interested in sharing the data that supports results you have or will publish in a manuscript(s). Example You are early on in your study, just starting to collect data and perform some initial analysis. Your data processing steps are complex and reasonable people might take a different approach at any particular data processing stage (e.g. different filtering threshold, different normalization procedure, different software implementation of an algorithm or modeling approach). You know that you would like to make sure that readers of the manuscript you will eventually publish can replicate your pre-proccessing steps and potentially access the raw data and pre-processing code/instructions/protocol so that they may modify one or more of the pre-processing steps and examine how that changes results published in your manuscript, and/or so that they may more easily compare the results from your study to results from their own studies (in which they use a slightly different pre-processing approach). Example Your study is complete, and you have very limited time or resources budgeted for data sharing. You have published a manuscript on your results or are in the process of submitting one. You want to fulfill the minimum data sharing requirements, so you will only share the data required to reproduce the manuscript results. Dataset-oriented Select this data sharing orientation if you are primarily interested in sharing a specific set of data or as much data as possible so that other researchers can get additional value from them, such as conducting additional research and analyes beyond the original study's focus. Example Your study is collecting RNA expression data. You are only focusing on a small subset of the collected data for your study, but you would like to be able to share as much of this data as possible so that other investigators can comb through the rich data for other analyses and insight. Next steps \u00b6 Learn more about and determine your study stage . Detailed guidance on how and when to create and complete your data package will be provided based upon your study's data sharing orientation and study stage.","title":"What is your data sharing orientation?"},{"location":"fit/data-sharing-orientation/#what-is-your-data-sharing-orientation","text":"Determining what your goals are for data sharing will help guide you in determining what you will include in your future data package and how you will annotate the data and non-data supporting files/resources in your data package. People will have different goals in sharing data and non-data supporting files/resources from their study. These goals, or 'orientations' may be dictated by a number of factors such as the nature of the data, study staff resources and time constraints, and investigator preference. There are two main data sharing \"orientations.\"","title":"What is your data sharing orientation?"},{"location":"fit/data-sharing-orientation/#data-sharing-orientations","text":"Results-support Dataset-sharing These data sharing goals or \"orientations\" are not mutually exclusive . You may choose both Results-support and Dataset-sharing as goals for your study's data sharing approach. See more about each data sharing orientation below, and decide if one, or both, apply to your study. Results-support Select this data sharing orientation if you are primarily interested in sharing the data that supports results you have or will publish in a manuscript(s). Example You are early on in your study, just starting to collect data and perform some initial analysis. Your data processing steps are complex and reasonable people might take a different approach at any particular data processing stage (e.g. different filtering threshold, different normalization procedure, different software implementation of an algorithm or modeling approach). You know that you would like to make sure that readers of the manuscript you will eventually publish can replicate your pre-proccessing steps and potentially access the raw data and pre-processing code/instructions/protocol so that they may modify one or more of the pre-processing steps and examine how that changes results published in your manuscript, and/or so that they may more easily compare the results from your study to results from their own studies (in which they use a slightly different pre-processing approach). Example Your study is complete, and you have very limited time or resources budgeted for data sharing. You have published a manuscript on your results or are in the process of submitting one. You want to fulfill the minimum data sharing requirements, so you will only share the data required to reproduce the manuscript results. Dataset-oriented Select this data sharing orientation if you are primarily interested in sharing a specific set of data or as much data as possible so that other researchers can get additional value from them, such as conducting additional research and analyes beyond the original study's focus. Example Your study is collecting RNA expression data. You are only focusing on a small subset of the collected data for your study, but you would like to be able to share as much of this data as possible so that other investigators can comb through the rich data for other analyses and insight.","title":"Data sharing \"orientations\""},{"location":"fit/data-sharing-orientation/#next-steps","text":"Learn more about and determine your study stage . Detailed guidance on how and when to create and complete your data package will be provided based upon your study's data sharing orientation and study stage.","title":"Next steps"},{"location":"fit/early/","text":"EARLY in study \u00b6 Use Wholistic As-You-Go Annotation Consider implementing HEAL recommendations for organization and naming of study artefacts (Note: Do not copy/duplicate files to implement) Consider annotating all study files as they are created, starting right away ( 'as you go' annotation ), regardless of whether they will be shared ( 'wholistic' annotation ) of study artefacts to reduce annotation burden at the end of the study Recommendations differ depending on your data-sharing orientation: Dataset-oriented Results-support Annotate up to and including your shareable dataset Annotate up to and including your published manuscript or shared report Include and annotate a results-tracker for each manuscript or report","title":"EARLY in study"},{"location":"fit/early/#early-in-study","text":"Use Wholistic As-You-Go Annotation Consider implementing HEAL recommendations for organization and naming of study artefacts (Note: Do not copy/duplicate files to implement) Consider annotating all study files as they are created, starting right away ( 'as you go' annotation ), regardless of whether they will be shared ( 'wholistic' annotation ) of study artefacts to reduce annotation burden at the end of the study Recommendations differ depending on your data-sharing orientation: Dataset-oriented Results-support Annotate up to and including your shareable dataset Annotate up to and including your published manuscript or shared report Include and annotate a results-tracker for each manuscript or report","title":"EARLY in study"},{"location":"fit/guidance-1/","text":"Guidance for study stage: early \u00b6 In general, an overview of the data packaging process looks like: Audit relevant files Organize and consistently name your files and folders - using HEAL recommendations for organizing and naming study files/resources Add Standard data package metadata - File-level Add Standard data package metadata - Study-level Please see below for detailed guidance on how and when to create and complete your data package if your study stage is early Audit relevant files \u00b6 Start now - complete right away - Audit all files that have so far been created/produced by or for the study. Organize and consistently name your files and folders \u00b6 Start now - complete right away - Once you've audited all of your study files/resources, consider following HEAL recommendations for file naming and organization . In order to do this, you may need to change the location of and/or rename existing study files. Establish file organization and naming conventions for all existing files, and follow these established conventions as you continue to move forward in your study and create/save new study files using these file organization and naming conventions. This may make it easier for other people (and your future self!) to intuitively understand and interpret the study organization and meaning and utility of various study files. It may also make data packaging (especially completion of your data package's Resource Tracker) easier for study staff. NOTE : Do not copy or duplicate files to implement. Add Standard data package metadata (File-level) \u00b6 Currently, file-level standard data package metadata files are the data dictionary (heal-formatted-data-dictionary; one per tabular data file) and the results tracker (heal-formatted-results-tracker; one per manuscript, report or poster. Data Dictionary : Start now - complete right away - Determine if your study has produced any tabular data files. If yes, go ahead and create a data dictionary for each of your existing tabular datasets. Please see here [add link to cli tools and/or desktop tool approach] for instructions on how to create (or at least get a head start on creating) a HEAL formatted data dictionary using just your tabular data file as input. Start now - complete over the course of your study - Create a data dictionary for each of your tabular datasets as the dataset is created. Please see here [add link to cli tools and/or desktop tool approach] for instructions on how to create (or at least get a head start on creating) a HEAL formatted data dictionary using just your tabular data file as input. NOTE : If your study has not and will not produce any tabular data files, you will not create a data dictionary Results Tracker : Wait until your study is starting to formulate final results text, tables, or figures - complete over the remainder of your study and manuscript production process Create a result annotation file for each of your final results (i.e. text, tables, or figures) as the final result is formulated and fleshed out into a text, table, or figure. Please see here [add link to cli tools and/or desktop tool approach] for instructions on how to create a result annotation file and how to use these files to create a distinct results tracker for each final manuscript or report generated by your study. NOTE : If your study's data sharing orientation or goal is focused on dataset-sharing (i.e. sharing a dataset of general interest to the scientific/research community), and less focused on results-support (i.e. providing clear support for results you share in a manuscript), you may not need to create a results tracker. Add Standard data package metadata (Study-level) - when, and how \u00b6 Currently, study-level standard data package metadata files are the experiment tracker (heal-formatted-experiment-tracker; one per study) and the resource tracker (heal-formatted-resource-tracker; one per study). Experiment Tracker : Each study should create and complete an experiment tracker; it is meant to be an inventory of each component experiment or activity that is part of your study. It is expected that some studies (e.g. clinical trials) may have just a single component study experiment/activity, whereas others may have many (e.g. many basic science studies). If you are early on in your study and all component study experiments/activities are already known/designed, go ahead and complete the experiment tracker right away. If you are early on in your study and all component study experiments/activities are NOT already known/designed, you may want to consider starting your experiment tracker right away, adding the experiments/activities that are already known/designed, and then moving forward add each new experiment/activity as it is designed (\"as you go\" annotation). If you are later on in your study, you may want to consider using your experiment tracker to inventory a specific subset of study experiments/activities instead of all study experiments/activities; specifically, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before starting your experiment tracker; this will allow you to pick and choose, adding only the experiments/activities that produced items that support the result(s) or dataset(s) you will share. Resource Tracker : Each study should create and complete a resource tracker; it is meant to be an inventory of each data or non-data supporting file collected/produced by or for the study, plus all standard data package metadata files added during the data packaging process. If you are early on in your study, you may want to consider starting your resource tracker right away, adding the study files/resources that have already been created, and then moving forward, adding each new study file/resource as it is created (\"as you go\" annotation). If you are later on in your study, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before starting your resource tracker; this will allow you to pick and choose, adding only the study resources/files that that support the result(s) or dataset(s) you will share. Find the best fit for you \u00b6 See here for some guidance on determining the best fit for your study group","title":"Guidance for study stage: early"},{"location":"fit/guidance-1/#guidance-for-study-stage-early","text":"In general, an overview of the data packaging process looks like: Audit relevant files Organize and consistently name your files and folders - using HEAL recommendations for organizing and naming study files/resources Add Standard data package metadata - File-level Add Standard data package metadata - Study-level Please see below for detailed guidance on how and when to create and complete your data package if your study stage is early","title":"Guidance for study stage: early"},{"location":"fit/guidance-1/#audit-relevant-files","text":"Start now - complete right away - Audit all files that have so far been created/produced by or for the study.","title":"Audit relevant files"},{"location":"fit/guidance-1/#organize-and-consistently-name-your-files-and-folders","text":"Start now - complete right away - Once you've audited all of your study files/resources, consider following HEAL recommendations for file naming and organization . In order to do this, you may need to change the location of and/or rename existing study files. Establish file organization and naming conventions for all existing files, and follow these established conventions as you continue to move forward in your study and create/save new study files using these file organization and naming conventions. This may make it easier for other people (and your future self!) to intuitively understand and interpret the study organization and meaning and utility of various study files. It may also make data packaging (especially completion of your data package's Resource Tracker) easier for study staff. NOTE : Do not copy or duplicate files to implement.","title":"Organize and consistently name your files and folders"},{"location":"fit/guidance-1/#add-standard-data-package-metadata-file-level","text":"Currently, file-level standard data package metadata files are the data dictionary (heal-formatted-data-dictionary; one per tabular data file) and the results tracker (heal-formatted-results-tracker; one per manuscript, report or poster. Data Dictionary : Start now - complete right away - Determine if your study has produced any tabular data files. If yes, go ahead and create a data dictionary for each of your existing tabular datasets. Please see here [add link to cli tools and/or desktop tool approach] for instructions on how to create (or at least get a head start on creating) a HEAL formatted data dictionary using just your tabular data file as input. Start now - complete over the course of your study - Create a data dictionary for each of your tabular datasets as the dataset is created. Please see here [add link to cli tools and/or desktop tool approach] for instructions on how to create (or at least get a head start on creating) a HEAL formatted data dictionary using just your tabular data file as input. NOTE : If your study has not and will not produce any tabular data files, you will not create a data dictionary Results Tracker : Wait until your study is starting to formulate final results text, tables, or figures - complete over the remainder of your study and manuscript production process Create a result annotation file for each of your final results (i.e. text, tables, or figures) as the final result is formulated and fleshed out into a text, table, or figure. Please see here [add link to cli tools and/or desktop tool approach] for instructions on how to create a result annotation file and how to use these files to create a distinct results tracker for each final manuscript or report generated by your study. NOTE : If your study's data sharing orientation or goal is focused on dataset-sharing (i.e. sharing a dataset of general interest to the scientific/research community), and less focused on results-support (i.e. providing clear support for results you share in a manuscript), you may not need to create a results tracker.","title":"Add Standard data package metadata (File-level)"},{"location":"fit/guidance-1/#add-standard-data-package-metadata-study-level-when-and-how","text":"Currently, study-level standard data package metadata files are the experiment tracker (heal-formatted-experiment-tracker; one per study) and the resource tracker (heal-formatted-resource-tracker; one per study). Experiment Tracker : Each study should create and complete an experiment tracker; it is meant to be an inventory of each component experiment or activity that is part of your study. It is expected that some studies (e.g. clinical trials) may have just a single component study experiment/activity, whereas others may have many (e.g. many basic science studies). If you are early on in your study and all component study experiments/activities are already known/designed, go ahead and complete the experiment tracker right away. If you are early on in your study and all component study experiments/activities are NOT already known/designed, you may want to consider starting your experiment tracker right away, adding the experiments/activities that are already known/designed, and then moving forward add each new experiment/activity as it is designed (\"as you go\" annotation). If you are later on in your study, you may want to consider using your experiment tracker to inventory a specific subset of study experiments/activities instead of all study experiments/activities; specifically, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before starting your experiment tracker; this will allow you to pick and choose, adding only the experiments/activities that produced items that support the result(s) or dataset(s) you will share. Resource Tracker : Each study should create and complete a resource tracker; it is meant to be an inventory of each data or non-data supporting file collected/produced by or for the study, plus all standard data package metadata files added during the data packaging process. If you are early on in your study, you may want to consider starting your resource tracker right away, adding the study files/resources that have already been created, and then moving forward, adding each new study file/resource as it is created (\"as you go\" annotation). If you are later on in your study, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before starting your resource tracker; this will allow you to pick and choose, adding only the study resources/files that that support the result(s) or dataset(s) you will share.","title":"Add Standard data package metadata (Study-level) - when, and how"},{"location":"fit/guidance-1/#find-the-best-fit-for-you","text":"See here for some guidance on determining the best fit for your study group","title":"Find the best fit for you"},{"location":"fit/index-archive/","text":"When deciding how you will share and annotate your study data, you should consider two key questions . What is your orientation towards data sharing? How far along are you in your study? 1. What is your orientation towards data sharing? \u00b6 Determining what your goals are for data sharing will help guide you in determining what you will include in your future data package and how you will annotate the data and non-data/supporting documents in your dat package. People will have different goals in sharing their study data, which can be dictated by a number of factors such as the nature of the data, resources and time constraints, and investigator preference. There are two main \"data sharing orientations.\" They are not mutually exclusive. See more about each data sharing orientation below. Results-support This orientation is for investigators who are interested only in fulfilling the requirement to share the data that is necessary to reproduce their results. Example Your study is complete, and you have very limited time or resources budgeted for data sharing. You have published a manuscript on your results or are in the process of submitting one. You want to fulfill the minimum data sharing requirements, so you will only share the data required to reproduce the manuscript results. Dataset-oriented This orientation is for investigators who are interested in sharing a specific set of data or as much data as possible so that other researchers can get additional value from them, such as conducting additional research and analyes beyond the original study's focus. Example Your study is collecting RNA expression data. You are only focusing on a small subset of the collected data for your study, but you would like to be able to share as much of this data as possible so that other investigators can comb through the rich data for other analyses and insight. 2. How far along are you in your study? \u00b6 Where you are in your study when you start to think about data sharing will affect how you document your data. For example, if you are near the beginning of your study, you may be able to set up documentation processes such that you document as you move through your study, which will ease the process of data sharing in the future, as opposed to completing all documentation at the end. If you are early in your study and/or have collected little data and non-data/supporting elements, click here . If you are later in your study and/or have collected a large amount of data and non-data/supporting elements, click here .","title":"Index archive"},{"location":"fit/index-archive/#1-what-is-your-orientation-towards-data-sharing","text":"Determining what your goals are for data sharing will help guide you in determining what you will include in your future data package and how you will annotate the data and non-data/supporting documents in your dat package. People will have different goals in sharing their study data, which can be dictated by a number of factors such as the nature of the data, resources and time constraints, and investigator preference. There are two main \"data sharing orientations.\" They are not mutually exclusive. See more about each data sharing orientation below. Results-support This orientation is for investigators who are interested only in fulfilling the requirement to share the data that is necessary to reproduce their results. Example Your study is complete, and you have very limited time or resources budgeted for data sharing. You have published a manuscript on your results or are in the process of submitting one. You want to fulfill the minimum data sharing requirements, so you will only share the data required to reproduce the manuscript results. Dataset-oriented This orientation is for investigators who are interested in sharing a specific set of data or as much data as possible so that other researchers can get additional value from them, such as conducting additional research and analyes beyond the original study's focus. Example Your study is collecting RNA expression data. You are only focusing on a small subset of the collected data for your study, but you would like to be able to share as much of this data as possible so that other investigators can comb through the rich data for other analyses and insight.","title":"1. What is your orientation towards data sharing?"},{"location":"fit/index-archive/#2-how-far-along-are-you-in-your-study","text":"Where you are in your study when you start to think about data sharing will affect how you document your data. For example, if you are near the beginning of your study, you may be able to set up documentation processes such that you document as you move through your study, which will ease the process of data sharing in the future, as opposed to completing all documentation at the end. If you are early in your study and/or have collected little data and non-data/supporting elements, click here . If you are later in your study and/or have collected a large amount of data and non-data/supporting elements, click here .","title":"2. How far along are you in your study?"},{"location":"fit/late/","text":"LATE in study \u00b6 Use Wholistic OR Minimal Top-Down Annotation The way you annotate will vary based on your data-sharing orientation: \u00b6 Dataset-oriented Results-support If you are interested in sharing a specific dataset and documenting the necessary pieces to reproduce that dataset, recommended steps include: Wait until shareable dataset(s) are produced Annotate shareable dataset(s) in the resource tracker Annotate dependencies of shareable dataset(s) (e.g., data dictionary, protocol, raw data, code) Annotate dependencies of those dependencies, continuing with this cycle until final dependencies you annotate have no dependencies of their own See below for information on how to document dependencies If you are interested in sharing data and support files that support a specific result, recommended steps include: Wait until the manuscript or report is produced Annotate manuscript Create a results tracker for each manuscript or report. Within the results tracker, list each result and its dependencies (e.g., data, data dictionary, code, statistical analysis plan) Annotate results tracker(s) Annotate dependencies of results as listed in the results tracker(s) Annotate dependencies of those dependencies, continuing with this cycle until final dependencies you annotate have no dependencies of their own See below for information on how to document dependencies Document the results tracker(s) as an entry into the resource tracker Regardless of your data-sharing orientation, the way you document dependencies will vary based on your annotation approach: \u00b6 Wholistic Minimal List dependencies 'one-layer-deep', regardless of whether those dependencies will be shared Annotate all dependencies regardless of whether they will be shared. List dependencies 'liberally', regardless of whether those dependencies will be shared Annotate only dependencies that will be shared. If your data sharing orientation is dataset-oriented ... \u00b6 If you are interested in sharing a specific dataset and documenting the necessary pieces to reproduce that dataset, recommended steps include: Wait until shareable dataset(s) are produced Annotate shareable dataset(s) in the resource tracker Annotate dependencies of shareable dataset(s) (e.g., data dictionary, protocol, raw data, code) Annotate dependencies of those dependencies, continuing with this cycle until final dependencies you annotate have no dependencies of their own The way you annotate dependencies will vary based on your annotation approach: Wholistic Minimal List dependencies 'one-layer-deep', regardless of whether those dependencies will be shared Annotate all dependencies regardless of whether they will be shared. List dependencies 'liberally', regardless of whether those dependencies will be shared Annotate only dependencies that will be shared. If your data sharing orientation is results-support ... \u00b6 If you are interested in sharing data and support files that support a specific result, recommended steps include: Wait until the manuscript or report is produced Annotate manuscript Create a results tracker for each manuscript or report. Within the results tracker, list each result and its dependencies (e.g., data, data dictionary, code, statistical analysis plan) Annotate results tracker(s) Annotate dependencies of results as listed in the results tracker(s) Annotate dependencies of those dependencies, continuing with this cycle until final dependencies you annotate have no dependencies of their own Document the results tracker(s) as an entry into the resource tracker The way you annotate dependencies will vary based on your annotation approach: Wholistic Minimal List dependencies 'one-layer-deep', regardless of whether those dependencies will be shared Annotate all dependencies regardless of whether they will be shared. List dependencies 'liberally', regardless of whether those dependencies will be shared Annotate only dependencies that will be shared.","title":"LATE in study"},{"location":"fit/late/#late-in-study","text":"Use Wholistic OR Minimal Top-Down Annotation","title":"LATE in study"},{"location":"fit/late/#the-way-you-annotate-will-vary-based-on-your-data-sharing-orientation","text":"Dataset-oriented Results-support If you are interested in sharing a specific dataset and documenting the necessary pieces to reproduce that dataset, recommended steps include: Wait until shareable dataset(s) are produced Annotate shareable dataset(s) in the resource tracker Annotate dependencies of shareable dataset(s) (e.g., data dictionary, protocol, raw data, code) Annotate dependencies of those dependencies, continuing with this cycle until final dependencies you annotate have no dependencies of their own See below for information on how to document dependencies If you are interested in sharing data and support files that support a specific result, recommended steps include: Wait until the manuscript or report is produced Annotate manuscript Create a results tracker for each manuscript or report. Within the results tracker, list each result and its dependencies (e.g., data, data dictionary, code, statistical analysis plan) Annotate results tracker(s) Annotate dependencies of results as listed in the results tracker(s) Annotate dependencies of those dependencies, continuing with this cycle until final dependencies you annotate have no dependencies of their own See below for information on how to document dependencies Document the results tracker(s) as an entry into the resource tracker","title":"The way you annotate will vary based on your data-sharing orientation:"},{"location":"fit/late/#regardless-of-your-data-sharing-orientation-the-way-you-document-dependencies-will-vary-based-on-your-annotation-approach","text":"Wholistic Minimal List dependencies 'one-layer-deep', regardless of whether those dependencies will be shared Annotate all dependencies regardless of whether they will be shared. List dependencies 'liberally', regardless of whether those dependencies will be shared Annotate only dependencies that will be shared.","title":"Regardless of your data-sharing orientation, the way you document dependencies will vary based on your annotation approach:"},{"location":"fit/late/#if-your-data-sharing-orientation-is-dataset-oriented","text":"If you are interested in sharing a specific dataset and documenting the necessary pieces to reproduce that dataset, recommended steps include: Wait until shareable dataset(s) are produced Annotate shareable dataset(s) in the resource tracker Annotate dependencies of shareable dataset(s) (e.g., data dictionary, protocol, raw data, code) Annotate dependencies of those dependencies, continuing with this cycle until final dependencies you annotate have no dependencies of their own The way you annotate dependencies will vary based on your annotation approach: Wholistic Minimal List dependencies 'one-layer-deep', regardless of whether those dependencies will be shared Annotate all dependencies regardless of whether they will be shared. List dependencies 'liberally', regardless of whether those dependencies will be shared Annotate only dependencies that will be shared.","title":"If your data sharing orientation is dataset-oriented..."},{"location":"fit/late/#if-your-data-sharing-orientation-is-results-support","text":"If you are interested in sharing data and support files that support a specific result, recommended steps include: Wait until the manuscript or report is produced Annotate manuscript Create a results tracker for each manuscript or report. Within the results tracker, list each result and its dependencies (e.g., data, data dictionary, code, statistical analysis plan) Annotate results tracker(s) Annotate dependencies of results as listed in the results tracker(s) Annotate dependencies of those dependencies, continuing with this cycle until final dependencies you annotate have no dependencies of their own Document the results tracker(s) as an entry into the resource tracker The way you annotate dependencies will vary based on your annotation approach: Wholistic Minimal List dependencies 'one-layer-deep', regardless of whether those dependencies will be shared Annotate all dependencies regardless of whether they will be shared. List dependencies 'liberally', regardless of whether those dependencies will be shared Annotate only dependencies that will be shared.","title":"If your data sharing orientation is results-support..."},{"location":"fit/study-stage/","text":"How far along are you in your study? \u00b6 Where you are in your study when you start to think about data sharing will affect how you document your data. For example, if you are near the beginning of your study, you may be able to create your data package and set up documentation processes such that you document as you move through your study, which will lead to more complete documentation and ease the process of data sharing in the future, as opposed to completing all documentation at the end. If you are nearer the end of your study, we recommend you take a narrower, more goal-oriented approach to documentation to minimize data sharing burden on your study group, while maximizing value to potential secondary data users and collaboraters. There are two options available for study stage. Study stages \u00b6 Early Late These study stage options are greatly simplified. For now, select the study stage that most closely fits with the stage you are at in your study . Next steps \u00b6 Do you know your data sharing orientation? It's a good idea to determine your data sharing orientation before proceeding. If you havent' yet, head here to determine your data sharing orientation, then head back here to proceed. If you are early in your study and have collected or produced little data or non-data supporting files/resources, click here . If you are later in your study and have collected or produced a large amount of data and on-data supporting files/resources, click here .","title":"How far along are you in your study?"},{"location":"fit/study-stage/#how-far-along-are-you-in-your-study","text":"Where you are in your study when you start to think about data sharing will affect how you document your data. For example, if you are near the beginning of your study, you may be able to create your data package and set up documentation processes such that you document as you move through your study, which will lead to more complete documentation and ease the process of data sharing in the future, as opposed to completing all documentation at the end. If you are nearer the end of your study, we recommend you take a narrower, more goal-oriented approach to documentation to minimize data sharing burden on your study group, while maximizing value to potential secondary data users and collaboraters. There are two options available for study stage.","title":"How far along are you in your study?"},{"location":"fit/study-stage/#study-stages","text":"Early Late These study stage options are greatly simplified. For now, select the study stage that most closely fits with the stage you are at in your study .","title":"Study stages"},{"location":"fit/study-stage/#next-steps","text":"Do you know your data sharing orientation? It's a good idea to determine your data sharing orientation before proceeding. If you havent' yet, head here to determine your data sharing orientation, then head back here to proceed. If you are early in your study and have collected or produced little data or non-data supporting files/resources, click here . If you are later in your study and have collected or produced a large amount of data and on-data supporting files/resources, click here .","title":"Next steps"},{"location":"g-and-r/","text":"Guidance and Resources \u00b6 Guidance \u00b6 File organization and naming \u00b6 Resources \u00b6 Standard data package metadata schemas \u00b6","title":"Guidance and Resources"},{"location":"g-and-r/#guidance-and-resources","text":"","title":"Guidance and Resources"},{"location":"g-and-r/#guidance","text":"","title":"Guidance"},{"location":"g-and-r/#file-organization-and-naming","text":"","title":"File organization and naming"},{"location":"g-and-r/#resources","text":"","title":"Resources"},{"location":"g-and-r/#standard-data-package-metadata-schemas","text":"","title":"Standard data package metadata schemas"},{"location":"guidance/","text":"Guidance \u00b6 File organization and naming \u00b6 File organization \u00b6 File naming \u00b6","title":"Guidance"},{"location":"guidance/#guidance","text":"","title":"Guidance"},{"location":"guidance/#file-organization-and-naming","text":"","title":"File organization and naming"},{"location":"guidance/#file-organization","text":"","title":"File organization"},{"location":"guidance/#file-naming","text":"","title":"File naming"},{"location":"guidance/file-name/","text":"Guidance: File naming \u00b6 Consistently name your files and folders \u00b6 The most important guidance is to keep study file/folder naming and organization internally consistent within your study. Establish naming and organization conventions for your study files and folders at the start of your study and consistently apply them throughout. Universal conventions \u00b6 File and folder names should generally be informative/descriptive Wherever possible name your files and folders in a way that will let a human user reasonably infer quite a lot about what the file holds without any further information File and folder names should be as brief as possible It is very helpful if file and folder names are all in lower case Don't use spaces or special characters Preferred separator is a dash ('-'), e.g., my-data-file-1 Special conventions \u00b6 If you have multiple files or folders of the same type , choose a naming convention that's descriptive, and not unmanageably long, and be consistent. For example: One file per day, per cell, per protocol: Potential convention: \"day\"-YYYY-MM-DD-\"cell\"-[0-9]-\"protocol\"-{short-protocol-description} Example filename: day-2023-03-17-cell-3-protocol-200ms-pulse.asc One folder per experimental objective: Potential convention: {cell-type}-{short-experimental-approach-description}-{short-experimental-objective-description} Example foldrer name: drg-current-clamp-effect-of-K1395R-on-drg-neuronal-excitability; drg-voltage-clamp-biophysical-properties-of-K1395R If you have files that are related , choose a naming convention that's descriptive, and not unmanageably long, and be consistent. For example: A tabular/csv data file that has a data dictionary Potential convention: The data dictionary for a data file has the same filename as the data file, with a \"dd-\" prefix Example file name(s): Data file name : my-data-1.csv Corresponding data dictionary file name : dd-my-data-1.csv Several experiments that each have an experimental protocol Potential convention: The protocol of the experiment has the id of that experiment (from the experiment tracker) in the filename, with a \"protocol-\" prefix Example file name(s): Experiment id (from experiment tracker) : exp-1 Corresponding protocol file name : protocol-exp-1.txt","title":"File naming"},{"location":"guidance/file-name/#guidance-file-naming","text":"","title":"Guidance: File naming"},{"location":"guidance/file-name/#consistently-name-your-files-and-folders","text":"The most important guidance is to keep study file/folder naming and organization internally consistent within your study. Establish naming and organization conventions for your study files and folders at the start of your study and consistently apply them throughout.","title":"Consistently name your files and folders"},{"location":"guidance/file-name/#universal-conventions","text":"File and folder names should generally be informative/descriptive Wherever possible name your files and folders in a way that will let a human user reasonably infer quite a lot about what the file holds without any further information File and folder names should be as brief as possible It is very helpful if file and folder names are all in lower case Don't use spaces or special characters Preferred separator is a dash ('-'), e.g., my-data-file-1","title":"Universal conventions"},{"location":"guidance/file-name/#special-conventions","text":"If you have multiple files or folders of the same type , choose a naming convention that's descriptive, and not unmanageably long, and be consistent. For example: One file per day, per cell, per protocol: Potential convention: \"day\"-YYYY-MM-DD-\"cell\"-[0-9]-\"protocol\"-{short-protocol-description} Example filename: day-2023-03-17-cell-3-protocol-200ms-pulse.asc One folder per experimental objective: Potential convention: {cell-type}-{short-experimental-approach-description}-{short-experimental-objective-description} Example foldrer name: drg-current-clamp-effect-of-K1395R-on-drg-neuronal-excitability; drg-voltage-clamp-biophysical-properties-of-K1395R If you have files that are related , choose a naming convention that's descriptive, and not unmanageably long, and be consistent. For example: A tabular/csv data file that has a data dictionary Potential convention: The data dictionary for a data file has the same filename as the data file, with a \"dd-\" prefix Example file name(s): Data file name : my-data-1.csv Corresponding data dictionary file name : dd-my-data-1.csv Several experiments that each have an experimental protocol Potential convention: The protocol of the experiment has the id of that experiment (from the experiment tracker) in the filename, with a \"protocol-\" prefix Example file name(s): Experiment id (from experiment tracker) : exp-1 Corresponding protocol file name : protocol-exp-1.txt","title":"Special conventions"},{"location":"guidance/file-org/","text":"Guidance: File organization \u00b6 Consistently organize your files and folders \u00b6 The most important guidance is to keep study file/folder naming and organization internally consistent within your study. Establish naming and organization conventions for your study files and folders at the start of your study and consistently apply them throughout. Organize study-related files and folders into a single study folder \u00b6 Organize all study files/resources into a single study folder/directory. A study folder/directory may of course have sub-directories. See below for an example of a recommended study folder/directory structure. Organize study-related files into folders with consistent naming conventions \u00b6 Organize all your files (whether you will share them or not), into folders. In general, minimize the number of folders at each level, and the number of 'layers' of your whole directory. File naming conventions can often be used quite effectively to 'organize' files and to provide clues as to what the files are and how they relate to each other without resorting to using separation into different directories to serve that purpose. However, there is a limit to naming conventions, and you may want to separate your files into directories based on some type of logical groupings and/or based on th elevel or timing of access to the files you expect to provide. See below for further details. Reminder Before creating more directories or directory layers, consider whether file-naming conventions may be used more efficiently to create this organization. Create logical groupings \u00b6 There are a number of ways to group files logically. Below are some examples of how you can logically group your files in folder structures: Grouping strategy Examples By file category Raw data grouped together; processed data grouped together; protocols grouped together; data dictionaries grouped together By experimental subject All data from a particular experimental subjects (across all protocols grouped together, etc.) By type of experiment All (electrophysiology) current clamp experiment files grouped together; all (electrophysiology) voltage clamp experiment files grouped together By experiment All data from a particular protocol, across all experimental subjects to which the protocol applies, grouped together By file type All neuroimaging FMRI files grouped together","title":"File organization"},{"location":"guidance/file-org/#guidance-file-organization","text":"","title":"Guidance: File organization"},{"location":"guidance/file-org/#consistently-organize-your-files-and-folders","text":"The most important guidance is to keep study file/folder naming and organization internally consistent within your study. Establish naming and organization conventions for your study files and folders at the start of your study and consistently apply them throughout.","title":"Consistently organize your files and folders"},{"location":"guidance/file-org/#organize-study-related-files-and-folders-into-a-single-study-folder","text":"Organize all study files/resources into a single study folder/directory. A study folder/directory may of course have sub-directories. See below for an example of a recommended study folder/directory structure.","title":"Organize study-related files and folders into a single study folder"},{"location":"guidance/file-org/#organize-study-related-files-into-folders-with-consistent-naming-conventions","text":"Organize all your files (whether you will share them or not), into folders. In general, minimize the number of folders at each level, and the number of 'layers' of your whole directory. File naming conventions can often be used quite effectively to 'organize' files and to provide clues as to what the files are and how they relate to each other without resorting to using separation into different directories to serve that purpose. However, there is a limit to naming conventions, and you may want to separate your files into directories based on some type of logical groupings and/or based on th elevel or timing of access to the files you expect to provide. See below for further details. Reminder Before creating more directories or directory layers, consider whether file-naming conventions may be used more efficiently to create this organization.","title":"Organize study-related files into folders with consistent naming conventions"},{"location":"guidance/file-org/#create-logical-groupings","text":"There are a number of ways to group files logically. Below are some examples of how you can logically group your files in folder structures: Grouping strategy Examples By file category Raw data grouped together; processed data grouped together; protocols grouped together; data dictionaries grouped together By experimental subject All data from a particular experimental subjects (across all protocols grouped together, etc.) By type of experiment All (electrophysiology) current clamp experiment files grouped together; all (electrophysiology) voltage clamp experiment files grouped together By experiment All data from a particular protocol, across all experimental subjects to which the protocol applies, grouped together By file type All neuroimaging FMRI files grouped together","title":"Create logical groupings"},{"location":"intro/flmd/","text":"If your study produces tabular data Timing : Start on this right away; no need to take the Decision Tree Quiz ahead of time Create one data dictionary for every tabular data file If your study has a 'results-support' data sharing orientation Timing : Take Decision Tree Quiz first; note recommendations from quiz regarding whether and how your study should implement the results tracker Create one results tracker for every manuscript or report that you wish to support","title":"Flmd"},{"location":"overview/how/","text":"How should you create your data package? \u00b6 In general, an overview of the process looks like: Audit relevant files Organize and consistently name your files and folders - using HEAL recommendations for organizing and naming study files/resources Add Standard data package metadata - File-level Add Standard data package metadata - Study-level However, when and exactly how you approach each of these steps will depend on a few factors, including how early on you are in your study, what your data sharing goals are, and the preferences of you and your study group - See below for details OR Skip ahead to find the best fit for you Audit relevant files \u00b6 Variations: All, or a subset of study files If you are early in your study, you may want to start auditing your study files rights away AND you may want to audit all files already created/produced by or for the study. If you are later in your study, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before auditing your study files AND you may want to audit only files that are needed to support (e.g. interpret, use, or replicate) results you will share in a manuscript, or datasets you will share in a public data repository. Organize and consistently name your files and folders \u00b6 Variations: follow HEAL recommendations for organization and naming of study files/resources, or not If you are early in your study, you may want to consider following HEAL recommendations and guidance re: file naming and organization. In order to do this, you may need to change the location of and rename files. You should start this process right away in order to establish file naming and organizational conventions that you can apply to all study files as they are created/produced. Establishing file naming and organizational conventions based on HEAL recommendations early in your study may increase human interpretability of your files and file structure and make it easier especially to create and complete your data package's Resource Tracker. If you are late in your study, applying HEAL recommendations and guidance re: file naming and organization may be too burdensome to be realistic and it may make more sense to leave naming and organization as is. Add Standard data package metadata (File-level) \u00b6 Variations: If, when, and how Currently, file-level standard data package metadata files are the data dictionary (heal-formatted-data-dictionary; one per tabular data file) and the results tracker (heal-formatted-results-tracker; one per manuscript, report, or poster. Data Dictionary : If your study has not and will not produce any tabular data files, you will not need to create a data dictionary. If your study has or will produce any tabular data files, you will need to create a data dictionary per tabular data file (if multiple tabular data files have the same format with the same variables, you can create a single data dictionary that applies across all of these files). In many cases, the best approach nearly universally will be to go ahead and create a data dictionary for each of your tabular datasets as the dataset is created. Please see here [add link to cli tools and/or desktop tool approach] for instructions on how to create (or at least get a head start on creating) a HEAL formatted data dictionary using just your tabular data file as input. Results Tracker : If your study is interested in providing clear support for results you share in a manuscript (required by many journals and funding agen cies), you should create a results tracker. If you are late in your study and have already produced/published a manuscript or result(s) in another format (e.g. poster, white paper, NIH report), you may want to consider starting your results tracker right away. If you are early in your study and have not yet formulated final results text, tables or figures or drafted you may want to consider waiting to start your results tracker until you have reached the study stage of formulating final results text, tables, or figures. If your study is most interested in sharing a dataset of general interest to the scientific/research community, and is less interested in specifically providing clear support for results you share in a manuscript, you may not need to create a results tracker. Add Standard data package metadata (Study-level) - when, and how \u00b6 Variations: When, and how Currently, study-level standard data package metadata files are the experiment tracker (heal-formatted-experiment-tracker; one per study) and the resource tracker (heal-formatted-resource-tracker; one per study). Experiment Tracker : Each study should create and complete an experiment tracker; it is meant to be an inventory of each component experiment or activity that is part of your study. It is expected that some studies (e.g. clinical trials) may have just a single component study experiment/activity, whereas others may have many (e.g. many basic science studies). If you are early on in your study and all component study experiments/activities are already known/designed, go ahead and complete the experiment tracker right away. If you are early on in your study and all component study experiments/activities are NOT already known/designed, you may want to consider starting your experiment tracker right away, adding the experiments/activities that are already known/designed, and then moving forward add each new experiment/activity as it is designed (\"as you go\" annotation). If you are later on in your study, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before starting your experiment tracker; this will allow you to pick and choose, adding only the experiments/activities that produced items that support the result(s) or dataset(s) you will share. Resource Tracker : Each study should create and complete a resource tracker; it is meant to be an inventory of each data or non-data supporting file collected/produced by or for the study, plus all standard data package metadata files added during the data packaging process. If you are early on in your study, you may want to consider starting your resource tracker right away, adding the study files/resources that have already been created, and then moving forward, adding each new study file/resource as it is created (\"as you go\" annotation), regardless of whether the file will be shared (\"wholistic\" annotation). If you are later on in your study, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before starting your resource tracker; this will allow you to work backwards from the results or datasets your study group will share (\"top down\" annotation); you can then pick and choose, adding only the study resources/files that support the result(s) or dataset(s) you will share; when you take this \"top down\" approach, you have the option of either adding all files that support the result(s) or dataset(s) you will share, regardless of whether these files will be shared (\"wholistic\" annotation) or adding the subset of files that support the result(s) or dataset(s) you will share that will themselves be shared (\"minimal\" annotation). Find the best fit for you \u00b6 See here for some guidance on determining the best fit for your study group","title":"How should you create your data package?"},{"location":"overview/how/#how-should-you-create-your-data-package","text":"In general, an overview of the process looks like: Audit relevant files Organize and consistently name your files and folders - using HEAL recommendations for organizing and naming study files/resources Add Standard data package metadata - File-level Add Standard data package metadata - Study-level However, when and exactly how you approach each of these steps will depend on a few factors, including how early on you are in your study, what your data sharing goals are, and the preferences of you and your study group - See below for details OR Skip ahead to find the best fit for you","title":"How should you create your data package?"},{"location":"overview/how/#audit-relevant-files","text":"Variations: All, or a subset of study files If you are early in your study, you may want to start auditing your study files rights away AND you may want to audit all files already created/produced by or for the study. If you are later in your study, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before auditing your study files AND you may want to audit only files that are needed to support (e.g. interpret, use, or replicate) results you will share in a manuscript, or datasets you will share in a public data repository.","title":"Audit relevant files"},{"location":"overview/how/#organize-and-consistently-name-your-files-and-folders","text":"Variations: follow HEAL recommendations for organization and naming of study files/resources, or not If you are early in your study, you may want to consider following HEAL recommendations and guidance re: file naming and organization. In order to do this, you may need to change the location of and rename files. You should start this process right away in order to establish file naming and organizational conventions that you can apply to all study files as they are created/produced. Establishing file naming and organizational conventions based on HEAL recommendations early in your study may increase human interpretability of your files and file structure and make it easier especially to create and complete your data package's Resource Tracker. If you are late in your study, applying HEAL recommendations and guidance re: file naming and organization may be too burdensome to be realistic and it may make more sense to leave naming and organization as is.","title":"Organize and consistently name your files and folders"},{"location":"overview/how/#add-standard-data-package-metadata-file-level","text":"Variations: If, when, and how Currently, file-level standard data package metadata files are the data dictionary (heal-formatted-data-dictionary; one per tabular data file) and the results tracker (heal-formatted-results-tracker; one per manuscript, report, or poster. Data Dictionary : If your study has not and will not produce any tabular data files, you will not need to create a data dictionary. If your study has or will produce any tabular data files, you will need to create a data dictionary per tabular data file (if multiple tabular data files have the same format with the same variables, you can create a single data dictionary that applies across all of these files). In many cases, the best approach nearly universally will be to go ahead and create a data dictionary for each of your tabular datasets as the dataset is created. Please see here [add link to cli tools and/or desktop tool approach] for instructions on how to create (or at least get a head start on creating) a HEAL formatted data dictionary using just your tabular data file as input. Results Tracker : If your study is interested in providing clear support for results you share in a manuscript (required by many journals and funding agen cies), you should create a results tracker. If you are late in your study and have already produced/published a manuscript or result(s) in another format (e.g. poster, white paper, NIH report), you may want to consider starting your results tracker right away. If you are early in your study and have not yet formulated final results text, tables or figures or drafted you may want to consider waiting to start your results tracker until you have reached the study stage of formulating final results text, tables, or figures. If your study is most interested in sharing a dataset of general interest to the scientific/research community, and is less interested in specifically providing clear support for results you share in a manuscript, you may not need to create a results tracker.","title":"Add Standard data package metadata (File-level)"},{"location":"overview/how/#add-standard-data-package-metadata-study-level-when-and-how","text":"Variations: When, and how Currently, study-level standard data package metadata files are the experiment tracker (heal-formatted-experiment-tracker; one per study) and the resource tracker (heal-formatted-resource-tracker; one per study). Experiment Tracker : Each study should create and complete an experiment tracker; it is meant to be an inventory of each component experiment or activity that is part of your study. It is expected that some studies (e.g. clinical trials) may have just a single component study experiment/activity, whereas others may have many (e.g. many basic science studies). If you are early on in your study and all component study experiments/activities are already known/designed, go ahead and complete the experiment tracker right away. If you are early on in your study and all component study experiments/activities are NOT already known/designed, you may want to consider starting your experiment tracker right away, adding the experiments/activities that are already known/designed, and then moving forward add each new experiment/activity as it is designed (\"as you go\" annotation). If you are later on in your study, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before starting your experiment tracker; this will allow you to pick and choose, adding only the experiments/activities that produced items that support the result(s) or dataset(s) you will share. Resource Tracker : Each study should create and complete a resource tracker; it is meant to be an inventory of each data or non-data supporting file collected/produced by or for the study, plus all standard data package metadata files added during the data packaging process. If you are early on in your study, you may want to consider starting your resource tracker right away, adding the study files/resources that have already been created, and then moving forward, adding each new study file/resource as it is created (\"as you go\" annotation), regardless of whether the file will be shared (\"wholistic\" annotation). If you are later on in your study, you may want to wait until you are at the study stage of producing the result(s) and/or dataset(s) your study group aims to share before starting your resource tracker; this will allow you to work backwards from the results or datasets your study group will share (\"top down\" annotation); you can then pick and choose, adding only the study resources/files that support the result(s) or dataset(s) you will share; when you take this \"top down\" approach, you have the option of either adding all files that support the result(s) or dataset(s) you will share, regardless of whether these files will be shared (\"wholistic\" annotation) or adding the subset of files that support the result(s) or dataset(s) you will share that will themselves be shared (\"minimal\" annotation).","title":"Add Standard data package metadata (Study-level) - when, and how"},{"location":"overview/how/#find-the-best-fit-for-you","text":"See here for some guidance on determining the best fit for your study group","title":"Find the best fit for you"},{"location":"overview/resource/","text":"About the Resource Tracker \u00b6 The resource tracker is an inventory and annotated list of all data and supporting/non-date files for the study. Purpose \u00b6 Guides interpretation, use, and replication Gives information on all/shared study artefacts, what items are shared, where/how to find them, how to understand/interpret those items</li Documents who different items relate to each other and how they relate to the dataset and/or result the study group will share. This is useful for potential secondary data users as well as the original study group, which may need to reference/replicate past experiments/results across people and time. Preparation for submission to a repository Can allow reconfiguration of file organization for sharing both in location and tiem based on the type of itme (e.g., data, metadata, code), access level (permanent private, temporary private, restricted access, public), date of access level change (e.g., shared access date) When submitting to a reposiotry with robust API infrastructure, can allow for automated submission with interpretable organization, respecting all access settings How your study should implement the resource tracker will depend on three dimensions: \u00b6 Stage of accumulation of study artefacts (study stage) Have already accumulated many study artefacts, OR Have accumulated relatively few or no study artefacts Data sharing 'orientation' (goal) Focused on sharing items that support your published results, AND/OR Focused on sharing a useful dataset Annotation plan Documenting all study artefacts related to your data sharing goal(s), OR Documenting only study artefacts related to your data sharing goal that will be shared in a public repository For guidance on answering these key questions, click here. \u00b6","title":"Resource"},{"location":"overview/resource/#about-the-resource-tracker","text":"The resource tracker is an inventory and annotated list of all data and supporting/non-date files for the study.","title":"About the Resource Tracker"},{"location":"overview/resource/#purpose","text":"Guides interpretation, use, and replication Gives information on all/shared study artefacts, what items are shared, where/how to find them, how to understand/interpret those items</li Documents who different items relate to each other and how they relate to the dataset and/or result the study group will share. This is useful for potential secondary data users as well as the original study group, which may need to reference/replicate past experiments/results across people and time. Preparation for submission to a repository Can allow reconfiguration of file organization for sharing both in location and tiem based on the type of itme (e.g., data, metadata, code), access level (permanent private, temporary private, restricted access, public), date of access level change (e.g., shared access date) When submitting to a reposiotry with robust API infrastructure, can allow for automated submission with interpretable organization, respecting all access settings","title":"Purpose"},{"location":"overview/resource/#how-your-study-should-implement-the-resource-tracker-will-depend-on-three-dimensions","text":"Stage of accumulation of study artefacts (study stage) Have already accumulated many study artefacts, OR Have accumulated relatively few or no study artefacts Data sharing 'orientation' (goal) Focused on sharing items that support your published results, AND/OR Focused on sharing a useful dataset Annotation plan Documenting all study artefacts related to your data sharing goal(s), OR Documenting only study artefacts related to your data sharing goal that will be shared in a public repository","title":"How your study should implement the resource tracker will depend on three dimensions:"},{"location":"overview/resource/#for-guidance-on-answering-these-key-questions-click-here","text":"","title":"For guidance on answering these key questions, click here."},{"location":"overview/slmd/","text":"Every study should create one experiment tracker Timing : Start on this right away; no need to take the Decision Tree Quiz ahead of time Content : Lists all experiments/activities in the study, including high-level description, experimental questions, and hypotheses Purpose : Gives a high-level overview of what the study was trying to ask/answer and how it went about answering that (more detailed than SLMD) Every study should create one resource tracker Timing : Take Decision Tree Quiz first; note recommendations from quiz regarding how your study should implement the resource tracker Content : Lists all/shared study artefacts and dependencies Purpose : Interpreting, using, replicating : Gives information on all/shared study artefacts, what items are shared, where/how to find it, how to understand/interpret it, how to use it, how different teams relate to each other, how different items relate to the dataset and/or results the study group will share (useful for potential secondary data users and originating study group which may need to reference/replicate past experiments/results across people and time) Submitting to a repository : Can allow reconfiguration of file organization for sharing both in location and time based on type of item (e.g., data, metadata, code), access level (permanent private, temporary private, restricted access, public), date of access level change (e.g., shared access date) - when submitting to a repository with robust API infrastructure, can allow for automated submission with interpretable organization and respecting all access settings How your study should implement the resource tracker will depend on at least three dimensions: Stage of accumulation of study artefacts : Have already accumulated many study artefacts OR have accumulated relatively few study artefacts Low accumulation : Add-as-you-go annotation High accumulation : Top-down annotation Data sharing 'orientation' (goal) : Focused on sharing items that support your published results AND/OR focused on sharing a useful dataset Results support : Top-down annotation, starting with a published paper or report Dataset : Low accumulation : Add-as-you-go annotation, ending with final dataset(s) High accumulation : Top-down annotation, starting with final dataset(s) Wholistic versus minimal : Lists all study artefacts related to your data sharing goal(s) OR only study artefacts that are related to your data sharing goal(s) AND will be shared in a public repository Results support : Top-down annotation, starting with a published paper or report Wholistic annotation : More work but more useful to potential secondary data users and to originating study group members Minimal annotation : Still incredibly useful to potentail secondary data users; minimizes work Dataset : Low accumulation : Add-as-you-go, wholistic annotation, ending with final dataset(s) High accumulation : Top-down annotation, starting with final dataset(s) Wholistic annotation : More work but more useful to potential secondary data users and to originating study group members Minimal annotation : Still incredibly useful to potential secondary data users; minimizes work","title":"Slmd"},{"location":"overview/what/","text":"What is a data package? \u00b6 Data package \u00b6 A data package is a stand-alone collection of research data and supporting files. A data package contains the following: Research data files or links to data files accessible or stored elsewhere Non-data, supporting files such as protocols, data collection instruments, code for manipulating or analyzing the data, discipline-specific metadata files describing the data, and additional documentation Standard data package metadata files that are both human- and machine-readable Shareable data package \u00b6 A shareable data package is a version of the data package that has been prepared for submission to a public data repository, and meets the following three requirements: All of the files are intended for sharing, possibly under a data use agreement (DUA) or specific set of restrictions. Sufficient information on the provenance and terms of use are included to make the data reusable. Sufficient metadata and documentation are included so that the package may be used by a researcher unconnected to and without special knowledge of the original study. Producing a shareable data package & Other important points \u00b6 Typically, a data package can be converted to a shareable data package simply by redacting any items that cannot be shared, replacing them with an appropriate substitute or placeholder whenever possible. For example, a researcher may replace dates within a sensitive dataset with shifted dates or topcode certain variables to help maintain participant confidentiality. Such changes are ideally made prior to analysis so that the original investigator is analyzing the exact same data file(s) that will be shared to facilitate reproduciblity. A common and important case is when none of the data can be shared, and thus all of the data file(s) must be excluded from the shareable data package. For example, a research project may exclusively be analyzing secondary data that, while broadly accessible, may not be redistributed (research data available from public repositories often falls into this category). In such cases, the data may be physically stored outside the shareable data package and replaced with a link to the data; the resulting shareable data package may then be shared with and used by researchers who independently obtain access to the data. In this way, one may think of a shareable data package as supporting the use a particular dataset for a specific purpose (e.g. replicating an analysis performed by the authors of the data package), whether are not the data themselves are contained in the package . Metadata files provide context and usability \u00b6 Data sharing requirements by funders or journals are often stated in general terms only (e.g., \"You must share all data underlying published results\"). This creates two problems: (1) a researcher may nominally comply by simply sharing his or her existing files, which may or may not be useable by others; and (2) it leads researchers to focus on the data only, to the exclusion of metadata and other resources (e.g. codebooks, code) that are needed to use the data effectively and/or increase their scientific value. Aside from the data themselves, it is important for investigators to provide context and usability information, including: what questions was the study group trying to answer in collecting the data, how did they design study experiment(s)/activity(s) to answer those questions, how was data collected, what data was collected, what is in each data file, how to use it, how it relates to other data files, how it relates to published results, etc. In fact, supporting or metadata files that include this context and usability information are arguably as or more important than the data itself . Without the context information, it is difficult for potential secondary data users or collaborators to get even a high-level sense of whether it may be productive to request access to study data or to reach out regarding a collaboration with the originating study group. Without the usability information, it is difficult for potential secondary data users or collaborators to get a more granular view of whether the collected data may be usable for their secondary data use purposes, and it may be difficult or impossible to understand and actually use the data for their secondary data use purpose. Some research communities have clear standards for how to create and provide at least some of the critical metadata required to ensure discoverability, understanding, and usability of commonly collected data types (e.g. neuroimaging, genomice, GIS) or datasets collected in the course of a commonly conducted study type (e.g. clinical trial). These standards are incredibly valuable and essential for detailed discovery and reuse of especially very complex data and study types. Research communities should always be in charge of forging the standards very specific to their own community as they are, indeed, the only ones with the deep knowledge and experience to do so. Standard Data Package Metadata as a bridge across communities \u00b6 However, especially in the context of an effort like the NIH HEAL Initiative, which spans many research communities (e.g. pain, opioids, basic science, pre-clinical, clinical, wet-lab, computational, materials and methods development, community-based, surveys, observational, qualitative, pharma, business development), and seeks to promote discoverability, interpretability, and usability of all funded research across these communities, and by both researchers and a broader audience (e.g. community-members, community-based organizations, journalists), research community specific standards alone may fall short of enabling full discoverability, interpretability, and usability across this wide spectrum of data and knowledge producers and consumers, due to: Within specific research community standards Inconsistent or missing content or format recommendations impacting data types, datasets, and data collections collected within the context of the specific research community E.g. A standard specifies the need for a data dictionary, but does not provide further specification, so that some people provide it as a PDF, others as a Word document, and still others as an Excel document with a header row containing explanatory notes Across specific research community standards No one specific research community standard can be adopted by all communities It would miss standards needed by other communities It would require or recommend standards not needed or even nonsensical for other communities If each community adopted their own community specific standard and did nothing else , there would be a lack of discoverability and usability across communities Some communities lack clear standards, and so would implement no or inconsistent/ad hoc standards Inward-facing research community specific standards may assume knowledge and understanding that cannot be taken for granted outside of that community both with respect to organization of data and metadata, and with respect to terminology used to describe it Inconsistent content or format recommendations across research communities - E.g. Many community standards require a description of the overall study in some format, however the content and format varies - some specify to provide this is a readme.txt document, others as entered into structured forms such as when registering a clinical trial on clinicaltrials.gov or a genomic study on dbGaP , and still others do not specify a format; content varies across all of these examples The HEAL Initiative has developed Standard Data Package Metadata and best practices as to when and how to implement these metadata, to be leveraged by all studies across all research communities, as a way to bridge across research communities and other data and knowledge producer and consumer communities. Standard Data Package Metadata are a small set of standard metadata file types that, altogether, provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. These metadata files should be included in all HEAL Initiative data packages. They produce a common language and universally understandable structure for all studies across the spectrum of research communities that are a part of the HEAL Initiative. These are not meant to replace or supercede research community specific standards. Rather, they are meant to fully accommodate, never conflict with, and lightly add value to research community specific metadata standards.","title":"What is a data package?"},{"location":"overview/what/#what-is-a-data-package","text":"","title":"What is a data package?"},{"location":"overview/what/#data-package","text":"A data package is a stand-alone collection of research data and supporting files. A data package contains the following: Research data files or links to data files accessible or stored elsewhere Non-data, supporting files such as protocols, data collection instruments, code for manipulating or analyzing the data, discipline-specific metadata files describing the data, and additional documentation Standard data package metadata files that are both human- and machine-readable","title":"Data package"},{"location":"overview/what/#shareable-data-package","text":"A shareable data package is a version of the data package that has been prepared for submission to a public data repository, and meets the following three requirements: All of the files are intended for sharing, possibly under a data use agreement (DUA) or specific set of restrictions. Sufficient information on the provenance and terms of use are included to make the data reusable. Sufficient metadata and documentation are included so that the package may be used by a researcher unconnected to and without special knowledge of the original study.","title":"Shareable data package"},{"location":"overview/what/#producing-a-shareable-data-package-other-important-points","text":"Typically, a data package can be converted to a shareable data package simply by redacting any items that cannot be shared, replacing them with an appropriate substitute or placeholder whenever possible. For example, a researcher may replace dates within a sensitive dataset with shifted dates or topcode certain variables to help maintain participant confidentiality. Such changes are ideally made prior to analysis so that the original investigator is analyzing the exact same data file(s) that will be shared to facilitate reproduciblity. A common and important case is when none of the data can be shared, and thus all of the data file(s) must be excluded from the shareable data package. For example, a research project may exclusively be analyzing secondary data that, while broadly accessible, may not be redistributed (research data available from public repositories often falls into this category). In such cases, the data may be physically stored outside the shareable data package and replaced with a link to the data; the resulting shareable data package may then be shared with and used by researchers who independently obtain access to the data. In this way, one may think of a shareable data package as supporting the use a particular dataset for a specific purpose (e.g. replicating an analysis performed by the authors of the data package), whether are not the data themselves are contained in the package .","title":"Producing a shareable data package &amp; Other important points"},{"location":"overview/what/#metadata-files-provide-context-and-usability","text":"Data sharing requirements by funders or journals are often stated in general terms only (e.g., \"You must share all data underlying published results\"). This creates two problems: (1) a researcher may nominally comply by simply sharing his or her existing files, which may or may not be useable by others; and (2) it leads researchers to focus on the data only, to the exclusion of metadata and other resources (e.g. codebooks, code) that are needed to use the data effectively and/or increase their scientific value. Aside from the data themselves, it is important for investigators to provide context and usability information, including: what questions was the study group trying to answer in collecting the data, how did they design study experiment(s)/activity(s) to answer those questions, how was data collected, what data was collected, what is in each data file, how to use it, how it relates to other data files, how it relates to published results, etc. In fact, supporting or metadata files that include this context and usability information are arguably as or more important than the data itself . Without the context information, it is difficult for potential secondary data users or collaborators to get even a high-level sense of whether it may be productive to request access to study data or to reach out regarding a collaboration with the originating study group. Without the usability information, it is difficult for potential secondary data users or collaborators to get a more granular view of whether the collected data may be usable for their secondary data use purposes, and it may be difficult or impossible to understand and actually use the data for their secondary data use purpose. Some research communities have clear standards for how to create and provide at least some of the critical metadata required to ensure discoverability, understanding, and usability of commonly collected data types (e.g. neuroimaging, genomice, GIS) or datasets collected in the course of a commonly conducted study type (e.g. clinical trial). These standards are incredibly valuable and essential for detailed discovery and reuse of especially very complex data and study types. Research communities should always be in charge of forging the standards very specific to their own community as they are, indeed, the only ones with the deep knowledge and experience to do so.","title":"Metadata files provide context and usability"},{"location":"overview/what/#standard-data-package-metadata-as-a-bridge-across-communities","text":"However, especially in the context of an effort like the NIH HEAL Initiative, which spans many research communities (e.g. pain, opioids, basic science, pre-clinical, clinical, wet-lab, computational, materials and methods development, community-based, surveys, observational, qualitative, pharma, business development), and seeks to promote discoverability, interpretability, and usability of all funded research across these communities, and by both researchers and a broader audience (e.g. community-members, community-based organizations, journalists), research community specific standards alone may fall short of enabling full discoverability, interpretability, and usability across this wide spectrum of data and knowledge producers and consumers, due to: Within specific research community standards Inconsistent or missing content or format recommendations impacting data types, datasets, and data collections collected within the context of the specific research community E.g. A standard specifies the need for a data dictionary, but does not provide further specification, so that some people provide it as a PDF, others as a Word document, and still others as an Excel document with a header row containing explanatory notes Across specific research community standards No one specific research community standard can be adopted by all communities It would miss standards needed by other communities It would require or recommend standards not needed or even nonsensical for other communities If each community adopted their own community specific standard and did nothing else , there would be a lack of discoverability and usability across communities Some communities lack clear standards, and so would implement no or inconsistent/ad hoc standards Inward-facing research community specific standards may assume knowledge and understanding that cannot be taken for granted outside of that community both with respect to organization of data and metadata, and with respect to terminology used to describe it Inconsistent content or format recommendations across research communities - E.g. Many community standards require a description of the overall study in some format, however the content and format varies - some specify to provide this is a readme.txt document, others as entered into structured forms such as when registering a clinical trial on clinicaltrials.gov or a genomic study on dbGaP , and still others do not specify a format; content varies across all of these examples The HEAL Initiative has developed Standard Data Package Metadata and best practices as to when and how to implement these metadata, to be leveraged by all studies across all research communities, as a way to bridge across research communities and other data and knowledge producer and consumer communities. Standard Data Package Metadata are a small set of standard metadata file types that, altogether, provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. These metadata files should be included in all HEAL Initiative data packages. They produce a common language and universally understandable structure for all studies across the spectrum of research communities that are a part of the HEAL Initiative. These are not meant to replace or supercede research community specific standards. Rather, they are meant to fully accommodate, never conflict with, and lightly add value to research community specific metadata standards.","title":"Standard Data Package Metadata as a bridge across communities"},{"location":"overview/when/","text":"When should you start thinking about creating your data package? \u00b6 This will depend on a few factors, including how early on you are in your study, what your data sharing goals are, and the preferences of you and your study group - See below for details OR Skip ahead to find the best fit for you Start right away \u00b6 If you are early in your study, it may be that starting right away will provide the most value for your study group and for potential secondary data users and collaborators while also reducing annotation and data sharing burden at the end of your study Wait for shareable dataset or manuscript \u00b6 If you are well into or late in your study, it may be that waiting until you've produced that exciting final analytic dataset, or published that first manuscript will make most sense to minimize annotation and data sharing burden on your study team while still providing excellent value to potential secondary data users and collaborators Find the best fit for you \u00b6 See here for some guidance on determining the best fit for your study group","title":"When should you start thinking about creating your data package?"},{"location":"overview/when/#when-should-you-start-thinking-about-creating-your-data-package","text":"This will depend on a few factors, including how early on you are in your study, what your data sharing goals are, and the preferences of you and your study group - See below for details OR Skip ahead to find the best fit for you","title":"When should you start thinking about creating your data package?"},{"location":"overview/when/#start-right-away","text":"If you are early in your study, it may be that starting right away will provide the most value for your study group and for potential secondary data users and collaborators while also reducing annotation and data sharing burden at the end of your study","title":"Start right away"},{"location":"overview/when/#wait-for-shareable-dataset-or-manuscript","text":"If you are well into or late in your study, it may be that waiting until you've produced that exciting final analytic dataset, or published that first manuscript will make most sense to minimize annotation and data sharing burden on your study team while still providing excellent value to potential secondary data users and collaborators","title":"Wait for shareable dataset or manuscript"},{"location":"overview/when/#find-the-best-fit-for-you","text":"See here for some guidance on determining the best fit for your study group","title":"Find the best fit for you"},{"location":"overview/where/","text":"Where should you create a data package? \u00b6 Data Package \u00b6 Create your data package locally, wherever you store the data and non-data supporting files collected or produced by or for your study Do not move or replicate any files to accommodate the data packaging process Creating your \"data package\" amounts to creating and completing Standard data package metadata files for your study to add context and usability info about your existing data and non-data supporting files You will save these Standard data package metadata files in a single file folder/directory, and save this single file folder/directory as a sub-directory within your study file/folder directory Shareable Data Package \u00b6 Creating your data package by creating and completing Standard data package metadata files for your study will allow creation of a \"shareable\" version of your data package that: Includes only files that have been specified as shareable to a public repository, and Is organized in a manner that facilitates submission to a public repository and human interpretability of the submitted file structure This \"shareable\" version of your data package will also be created locally, and may subsequently be submitted to a public data repository by study staff.","title":"Where should you create a data package?"},{"location":"overview/where/#where-should-you-create-a-data-package","text":"","title":"Where should you create a data package?"},{"location":"overview/where/#data-package","text":"Create your data package locally, wherever you store the data and non-data supporting files collected or produced by or for your study Do not move or replicate any files to accommodate the data packaging process Creating your \"data package\" amounts to creating and completing Standard data package metadata files for your study to add context and usability info about your existing data and non-data supporting files You will save these Standard data package metadata files in a single file folder/directory, and save this single file folder/directory as a sub-directory within your study file/folder directory","title":"Data Package"},{"location":"overview/where/#shareable-data-package","text":"Creating your data package by creating and completing Standard data package metadata files for your study will allow creation of a \"shareable\" version of your data package that: Includes only files that have been specified as shareable to a public repository, and Is organized in a manner that facilitates submission to a public repository and human interpretability of the submitted file structure This \"shareable\" version of your data package will also be created locally, and may subsequently be submitted to a public data repository by study staff.","title":"Shareable Data Package"},{"location":"overview/who/","text":"Who should create a data package? \u00b6 Sharing data : Any investigator who will collect or produce data as part of their study, and who will share at least some of that data in a public data repository Not sharing data, but open to data requests and collaborators : Any investigator who will collect or produce data as part of their study, and who, though not sharing data in a public data repository, is open to data requests and collaborators creating a data package will facilitate creation of a \"shareable\" data package that contains only data or non-data supporting files the study group has indicated are to be shared in a public data repository this allows, for example, public sharing of non-data supporting files that enable potential secondary data users or collaborators to determine that they are interested in the data and to contact the originating study group for access or collaboration requests, without sharing any data publicly Sharing methods and other non-data study products : Any investigator who will collect or produce methods or other non-data study products as part of their study, and who will share at least some of those non-data study products in a public data repository","title":"Who should create a data package?"},{"location":"overview/who/#who-should-create-a-data-package","text":"Sharing data : Any investigator who will collect or produce data as part of their study, and who will share at least some of that data in a public data repository Not sharing data, but open to data requests and collaborators : Any investigator who will collect or produce data as part of their study, and who, though not sharing data in a public data repository, is open to data requests and collaborators creating a data package will facilitate creation of a \"shareable\" data package that contains only data or non-data supporting files the study group has indicated are to be shared in a public data repository this allows, for example, public sharing of non-data supporting files that enable potential secondary data users or collaborators to determine that they are interested in the data and to contact the originating study group for access or collaboration requests, without sharing any data publicly Sharing methods and other non-data study products : Any investigator who will collect or produce methods or other non-data study products as part of their study, and who will share at least some of those non-data study products in a public data repository","title":"Who should create a data package?"},{"location":"overview/why/","text":"Why should you create a data package? \u00b6 Potential Secondary Data Users \u00b6 Creating and completing a data package (including creation and completion of all Standard Data Package Metadata files ) are important for potential secondary data users as they: Allow potential secondary data users to understand enough about the study and data to determine whether the study/data may be of use for their purposes, and whether it makes sense for them to reach out to request access to the data or to propose a collaboration with the originating study group Facilitate interpretation and use of the data by requesting researchers Originating Study Group \u00b6 Importantly, this type of documentation and data 'packaging' can be incredibly valuable to the originating study group - it can: Ease creation of a \"shareable\" data package and submission of shareable data and necessary supporting files to a public data repository as required by journals and/or funding agencies Stimulate collaboration with other research groups, and re-use of data (which may lead to additional citations of the originating study/data) Facilitate retention of institutional knowledge, on-boarding of new research staff to an existing project, as well as replication and extension of study results within the originating study group","title":"Why should you create a data package?"},{"location":"overview/why/#why-should-you-create-a-data-package","text":"","title":"Why should you create a data package?"},{"location":"overview/why/#potential-secondary-data-users","text":"Creating and completing a data package (including creation and completion of all Standard Data Package Metadata files ) are important for potential secondary data users as they: Allow potential secondary data users to understand enough about the study and data to determine whether the study/data may be of use for their purposes, and whether it makes sense for them to reach out to request access to the data or to propose a collaboration with the originating study group Facilitate interpretation and use of the data by requesting researchers","title":"Potential Secondary Data Users"},{"location":"overview/why/#originating-study-group","text":"Importantly, this type of documentation and data 'packaging' can be incredibly valuable to the originating study group - it can: Ease creation of a \"shareable\" data package and submission of shareable data and necessary supporting files to a public data repository as required by journals and/or funding agencies Stimulate collaboration with other research groups, and re-use of data (which may lead to additional citations of the originating study/data) Facilitate retention of institutional knowledge, on-boarding of new research staff to an existing project, as well as replication and extension of study results within the originating study group","title":"Originating Study Group"},{"location":"resources/","text":"Resources \u00b6 Standard data package metadata schemas \u00b6 Study overview schema \u00b6 Experiment tracker schema \u00b6 Resource tracker schema \u00b6 Results tracker schema \u00b6 Data dictionary schema \u00b6","title":"Resources"},{"location":"resources/#resources","text":"","title":"Resources"},{"location":"resources/#standard-data-package-metadata-schemas","text":"","title":"Standard data package metadata schemas"},{"location":"resources/#study-overview-schema","text":"","title":"Study overview schema"},{"location":"resources/#experiment-tracker-schema","text":"","title":"Experiment tracker schema"},{"location":"resources/#resource-tracker-schema","text":"","title":"Resource tracker schema"},{"location":"resources/#results-tracker-schema","text":"","title":"Results tracker schema"},{"location":"resources/#data-dictionary-schema","text":"","title":"Data dictionary schema"},{"location":"schemas/","text":"Standard Data Package Metadata Schemas \u00b6 Schemas \u00b6 View metadata schemas. Study Overview Experiment Tracker Resource Tracker Results Tracker Data Dictionary CSV Templates \u00b6 Download metadata csv templates. Study Overview - there is no CSV template available for Study Overview Experiment Tracker Resource Tracker Results Tracker Data Dictionary","title":"Standard Data Package Metadata Schemas"},{"location":"schemas/#standard-data-package-metadata-schemas","text":"","title":"Standard Data Package Metadata Schemas"},{"location":"schemas/#schemas","text":"View metadata schemas. Study Overview Experiment Tracker Resource Tracker Results Tracker Data Dictionary","title":"Schemas"},{"location":"schemas/#csv-templates","text":"Download metadata csv templates. Study Overview - there is no CSV template available for Study Overview Experiment Tracker Resource Tracker Results Tracker Data Dictionary","title":"CSV Templates"},{"location":"schemas/jsonschema-csvtemplate-fields_v3/","text":"HEAL Variable Level Metadata Fields \u00b6 Variable level metadata individual fields integrated into the variable level metadata object within the HEAL platform metadata service. Note, only name and description are required. Listed at the end of the description are suggested \"priority\" levels in brackets (e.g., [ ]): 1. [Required]: Needs to be filled out to be valid. 2. [Highly recommended]: Greatly help using the data dictionary but not required. 3. [Optional, if applicable]: May only be applicable to certain fields. 4. [Autopopulated, if not filled]: These fields are intended to be autopopulated from other fields but can be filled out if desired. 5. [Experimental]: These fields are not currently used but are in development. Properties \u00b6 module (string) The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: ``` Demographics ``` ``` PROMIS ``` ``` Substance use ``` ``` Medical History ``` ``` Sleep questions ``` ``` Physical activity ``` name (string,required) The name of a variable (i.e., field) as it appears in the data. [Required] title (string) The human-readable title or label of the variable. [Highly recommended] Examples: ``` My Variable ``` ``` Gender identity ``` description (string,required) An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). [Required] Examples: ``` The participant's age at the time of study enrollment ``` ``` What is the highest grade or level of school you have completed or the highest degree you have received? ``` type (string) A classification or category of a particular data element or property expected or allowed in the dataset. Definitions: number (A numeric value with optional decimal places. (e.g., 3.14)) integer (A whole number without decimal places. (e.g., 42)) string (A sequence of characters. (e.g., \\\"test\\\")) any (Any type of data is allowed. (e.g., true)) boolean (A binary value representing true or false. (e.g., true)) date (A specific calendar date. (e.g., \\\"2023-05-25\\\")) datetime (A specific date and time, including timezone information. (e.g., \\\"2023-05-25T10:30:00Z\\\")) time (A specific time of day. (e.g., \\\"10:30:00\\\")) year (A specific year. (e.g., 2023) yearmonth (A specific year and month. (e.g., \\\"2023-05\\\")) duration (A length of time. (e.g., \\\"PT1H\\\") geopoint (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278])) Possible values: ``` number - integer - string - any - boolean - date - datetime - time - year - yearmonth - duration - geopoint ``` format (of below) Indicates the format of the type specified in the type property. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats. If type is one of the date-like formats, then see Date formats. Sources: Frictionless standard formats associated with types Any of the following: String Formats (of below) A format for a specialized type of string of: \" email if valid emails (e.g., test@gmail.com)\" \" uri if valid uri addresses (e.g., https://example.com/resource123)\" \" binary if a base64 binary encoded string (e.g., authentication token like aGVsbG8gd29ybGQ=)\" \" uuid if a universal unique identifier also known as a guid (eg., f47ac10b-58cc-4372-a567-0e02b2c3d479)\" Possible values: ``` uri - email - binary - uuid ``` Date Formats (string) A format for a date variable ( date , time , datetime ). default : An ISO8601 format string. any : Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. {PATTERN} : The value can be parsed according to {PATTERN} , which MUST follow the date formatting syntax of C / Python strftime such as: \" %Y-%m-%d (for date, e.g., 2023-05-25)\" \" %Y%-%d (for date, e.g., 20230525) for date without dashes\" \" %Y-%m-%dT%H:%M:%S (for datetime, e.g., 2023-05-25T10:30:45)\" \" %Y-%m-%dT%H:%M:%SZ (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z)\" \" %Y-%m-%dT%H:%M:%S%z (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300)\" \" %Y-%m-%dT%H:%M (for datetime without seconds, e.g., 2023-05-25T10:30)\" \" %Y-%m-%dT%H (for datetime without minutes and seconds, e.g., 2023-05-25T10)\" \" %H:%M:%S (for time, e.g., 10:30:45)\" \" %H:%M:%SZ (for time with UTC timezone, e.g., 10:30:45Z)\" \" %H:%M:%S%z (for time with timezone offset, e.g., 10:30:45+0300)\" Geopoint Format (string) The two types of formats for geopoint (describing a geographic point). array (if 'lat,long' (e.g., 36.63,-90.20)) object (if {'lat':36.63,'lon':-90.20}) Possible values: ``` array - object ``` Geojson Formats (string) The JSON object according to the geojson spec. Possible values: ``` topojson - default ``` constraints.maxLength (integer) Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. [Optional,if applicable] constraints.enum (string) Constrains possible values to a set of values. [Optional,if applicable] Examples: ``` 1|2|3|4|5|6|7|8 ``` ``` White|Black or African American|American Indian or Alaska Native|Native Hawaiian or Other Pacific Islander|Asian|Some other race|Multiracial ``` constraints.pattern (string) A regular expression pattern the data MUST conform to. [Optional,if applicable] constraints.maximum (integer) Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property. [Optional,if applicable] constraints.minimum (integer) Specifies the minimum value of a field. [Optional,if applicable] encodings (string) Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values. Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations). [Optional,if applicable] Examples: ``` 0=No|1=Yes ``` ``` HW=Hello world|GBW=Good bye world|HM=Hi,Mike ``` ordered (boolean) Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). [Optional,if applicable] missingValues (string) A list of missing values specific to a variable. [Optional, if applicable] Examples: ``` Missing|Skipped|No preference ``` ``` Missing ``` trueValues (string) For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. [Optional, if applicable] Examples: ``` Required|REQUIRED ``` ``` required|Yes|Y|Checked ``` ``` Checked ``` ``` Required ``` falseValues (string) For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. repo_link (string) A link to the variable as it exists on the home repository, if applicable standardsMappings.url (string) The url that links out to the published, standardized mapping. [Autopopulated, if not filled] Examples: ``` https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI ``` standardsMappings.type (string) The type of mapping linked to a published set of standard variables such as the NIH Common Data Elements program. [Autopopulated, if not filled] Examples: ``` cde ``` ``` ontology ``` ``` reference_list ``` standardsMappings.label (string) A free text label of a mapping indicating a mapping(s) to a published set of standard variables such as the NIH Common Data Elements program. [Autopopulated, if not filled] Examples: ``` substance use ``` ``` chemical compound ``` ``` promis ``` standardsMappings.source (string) The source of the standardized variable. Examples: ``` TBD (will have controlled vocabulary) ``` standardsMappings.id (string) The id locating the individual mapping within the given source. relatedConcepts.url (string) The url that links out to the published, standardized concept. [Autopopulated, if not filled] Examples: ``` https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI ``` relatedConcepts.type (string) The type of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) [Autopopulated, if not filled] relatedConcepts.label (string) A free text label of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) [Autopopulated, if not filled] relatedConcepts.source (string) The source of the related concept. [Autopopulated, if not filled] Examples: ``` TBD (will have controlled vocabulary) ``` relatedConcepts.id (string) The id locating the individual mapping within the given source. [Autopopulated, if not filled] univarStats.median (number) univarStats.mean (number) univarStats.std (number) univarStats.min (number) univarStats.max (number) univarStats.mode (number) univarStats.count (integer) univarStats.twentyFifthPercentile (number) univarStats.seventyFifthPercentile (number) univarStats.categoricalMarginals.name (string) univarStats.categoricalMarginals.count (integer)","title":"HEAL Variable Level Metadata Fields"},{"location":"schemas/jsonschema-csvtemplate-fields_v3/#heal-variable-level-metadata-fields","text":"Variable level metadata individual fields integrated into the variable level metadata object within the HEAL platform metadata service. Note, only name and description are required. Listed at the end of the description are suggested \"priority\" levels in brackets (e.g., [ ]): 1. [Required]: Needs to be filled out to be valid. 2. [Highly recommended]: Greatly help using the data dictionary but not required. 3. [Optional, if applicable]: May only be applicable to certain fields. 4. [Autopopulated, if not filled]: These fields are intended to be autopopulated from other fields but can be filled out if desired. 5. [Experimental]: These fields are not currently used but are in development.","title":"HEAL Variable Level Metadata Fields"},{"location":"schemas/jsonschema-csvtemplate-fields_v3/#properties","text":"module (string) The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: ``` Demographics ``` ``` PROMIS ``` ``` Substance use ``` ``` Medical History ``` ``` Sleep questions ``` ``` Physical activity ``` name (string,required) The name of a variable (i.e., field) as it appears in the data. [Required] title (string) The human-readable title or label of the variable. [Highly recommended] Examples: ``` My Variable ``` ``` Gender identity ``` description (string,required) An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). [Required] Examples: ``` The participant's age at the time of study enrollment ``` ``` What is the highest grade or level of school you have completed or the highest degree you have received? ``` type (string) A classification or category of a particular data element or property expected or allowed in the dataset. Definitions: number (A numeric value with optional decimal places. (e.g., 3.14)) integer (A whole number without decimal places. (e.g., 42)) string (A sequence of characters. (e.g., \\\"test\\\")) any (Any type of data is allowed. (e.g., true)) boolean (A binary value representing true or false. (e.g., true)) date (A specific calendar date. (e.g., \\\"2023-05-25\\\")) datetime (A specific date and time, including timezone information. (e.g., \\\"2023-05-25T10:30:00Z\\\")) time (A specific time of day. (e.g., \\\"10:30:00\\\")) year (A specific year. (e.g., 2023) yearmonth (A specific year and month. (e.g., \\\"2023-05\\\")) duration (A length of time. (e.g., \\\"PT1H\\\") geopoint (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278])) Possible values: ``` number - integer - string - any - boolean - date - datetime - time - year - yearmonth - duration - geopoint ``` format (of below) Indicates the format of the type specified in the type property. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats. If type is one of the date-like formats, then see Date formats. Sources: Frictionless standard formats associated with types Any of the following: String Formats (of below) A format for a specialized type of string of: \" email if valid emails (e.g., test@gmail.com)\" \" uri if valid uri addresses (e.g., https://example.com/resource123)\" \" binary if a base64 binary encoded string (e.g., authentication token like aGVsbG8gd29ybGQ=)\" \" uuid if a universal unique identifier also known as a guid (eg., f47ac10b-58cc-4372-a567-0e02b2c3d479)\" Possible values: ``` uri - email - binary - uuid ``` Date Formats (string) A format for a date variable ( date , time , datetime ). default : An ISO8601 format string. any : Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. {PATTERN} : The value can be parsed according to {PATTERN} , which MUST follow the date formatting syntax of C / Python strftime such as: \" %Y-%m-%d (for date, e.g., 2023-05-25)\" \" %Y%-%d (for date, e.g., 20230525) for date without dashes\" \" %Y-%m-%dT%H:%M:%S (for datetime, e.g., 2023-05-25T10:30:45)\" \" %Y-%m-%dT%H:%M:%SZ (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z)\" \" %Y-%m-%dT%H:%M:%S%z (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300)\" \" %Y-%m-%dT%H:%M (for datetime without seconds, e.g., 2023-05-25T10:30)\" \" %Y-%m-%dT%H (for datetime without minutes and seconds, e.g., 2023-05-25T10)\" \" %H:%M:%S (for time, e.g., 10:30:45)\" \" %H:%M:%SZ (for time with UTC timezone, e.g., 10:30:45Z)\" \" %H:%M:%S%z (for time with timezone offset, e.g., 10:30:45+0300)\" Geopoint Format (string) The two types of formats for geopoint (describing a geographic point). array (if 'lat,long' (e.g., 36.63,-90.20)) object (if {'lat':36.63,'lon':-90.20}) Possible values: ``` array - object ``` Geojson Formats (string) The JSON object according to the geojson spec. Possible values: ``` topojson - default ``` constraints.maxLength (integer) Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. [Optional,if applicable] constraints.enum (string) Constrains possible values to a set of values. [Optional,if applicable] Examples: ``` 1|2|3|4|5|6|7|8 ``` ``` White|Black or African American|American Indian or Alaska Native|Native Hawaiian or Other Pacific Islander|Asian|Some other race|Multiracial ``` constraints.pattern (string) A regular expression pattern the data MUST conform to. [Optional,if applicable] constraints.maximum (integer) Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property. [Optional,if applicable] constraints.minimum (integer) Specifies the minimum value of a field. [Optional,if applicable] encodings (string) Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values. Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations). [Optional,if applicable] Examples: ``` 0=No|1=Yes ``` ``` HW=Hello world|GBW=Good bye world|HM=Hi,Mike ``` ordered (boolean) Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). [Optional,if applicable] missingValues (string) A list of missing values specific to a variable. [Optional, if applicable] Examples: ``` Missing|Skipped|No preference ``` ``` Missing ``` trueValues (string) For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. [Optional, if applicable] Examples: ``` Required|REQUIRED ``` ``` required|Yes|Y|Checked ``` ``` Checked ``` ``` Required ``` falseValues (string) For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. repo_link (string) A link to the variable as it exists on the home repository, if applicable standardsMappings.url (string) The url that links out to the published, standardized mapping. [Autopopulated, if not filled] Examples: ``` https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI ``` standardsMappings.type (string) The type of mapping linked to a published set of standard variables such as the NIH Common Data Elements program. [Autopopulated, if not filled] Examples: ``` cde ``` ``` ontology ``` ``` reference_list ``` standardsMappings.label (string) A free text label of a mapping indicating a mapping(s) to a published set of standard variables such as the NIH Common Data Elements program. [Autopopulated, if not filled] Examples: ``` substance use ``` ``` chemical compound ``` ``` promis ``` standardsMappings.source (string) The source of the standardized variable. Examples: ``` TBD (will have controlled vocabulary) ``` standardsMappings.id (string) The id locating the individual mapping within the given source. relatedConcepts.url (string) The url that links out to the published, standardized concept. [Autopopulated, if not filled] Examples: ``` https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI ``` relatedConcepts.type (string) The type of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) [Autopopulated, if not filled] relatedConcepts.label (string) A free text label of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) [Autopopulated, if not filled] relatedConcepts.source (string) The source of the related concept. [Autopopulated, if not filled] Examples: ``` TBD (will have controlled vocabulary) ``` relatedConcepts.id (string) The id locating the individual mapping within the given source. [Autopopulated, if not filled] univarStats.median (number) univarStats.mean (number) univarStats.std (number) univarStats.min (number) univarStats.max (number) univarStats.mode (number) univarStats.count (integer) univarStats.twentyFifthPercentile (number) univarStats.seventyFifthPercentile (number) univarStats.categoricalMarginals.name (string) univarStats.categoricalMarginals.count (integer)","title":"Properties"},{"location":"schemas/jsonschema-csvtemplate-fields_v3_manual_edits/","text":"HEAL Data Dictionary \u00b6 HEAL DSC Core Metadata piece to track and provide basic information about variables in a tabular data file (i.e. a data file with rows and columns) from your HEAL study. Objective is to list all variables and descriptive information about those variables. This will ensure that potential secondary data users know what data has been collected or calculated and how to use these data. Note that a given study can have multiple tabular data files; You should create a data dictionary for each tabular data file. Thus, a study may have multiple data dictionaries This is an abridged version of the schema and only describes the fields/information you should provide for each variable. For the full HEAL Data Dictionary document schema specification, see here . NOTE: Only name and description properties are required. For categorical variables, constraints.enum and encodings (where applicable) properties are highly encouraged. For studies using HEAL or other common data elements (CDEs), standardsMappings information is highly encouraged. type and format properties may be particularly useful for some variable types (e.g. date-like variables) Properties \u00b6 module (string) : The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: Demographics PROMIS Medical History name (string,required) : The name of a variable (i.e., field) as it appears in the data. Examples: gender_id title (string) : The human-readable title or label of the variable. Examples: Gender identity description (string,required) : An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). Include measurement units in the description where relevant (e.g. temperature in celsius, distance in millimeters). Examples: The participant's age at the time of study enrollment, in years type (string) : A classification or category of a particular data element or property expected or allowed in the dataset. Must be one of: [\"number\", \"integer\", \"string\", \"any\", \"boolean\", \"date\", \"datetime\", \"time\", \"year\", \"yearmonth\", \"duration\", \"geopoint\"] . For details and examples of each of these types see here . format (of below) : Indicates the format of the type specified in the type property. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats . If type is \"date\", \"datetime\", or \"time\", default format is ISO8601 formatting for those respective types (see details on ISO8601 format for Date , Datetime , or Time ) - If you want to specify a date-like variable using standard Python/C strptime syntax, see here for details. See here for more information about appropriate format values by variable type . constraints.maxLength (integer) : Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. constraints.enum (string) : Constrains possible values to a set of values. If a variable is contrained to a set of values, you may want to consider providing more information about what each value in the set mean using the encodings property for the same variable (e.g. If a variable has possible values of 1, 2, and 3 and these correspond to \"yes\", \"no\" and \"maybe\", you can indicate the constraint of this variable to values of 1, 2, and 3 using the constraints.enum property for that variable, and you can indicate that 1=\"yes\", 2=\"no\", and 3=\"maybe\" using the encodings property for that variable). Examples: 1|2|3|4|5|6|7|8 Treated|Control constraints.pattern (string) : A regular expression pattern the data MUST conform to. constraints.maximum (integer) : Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different than maxLength property. constraints.minimum (integer) : Specifies the minimum value of a field. encodings (string) : For variables constrained to a set of values, variable value encodings provide a way to further annotate each value in the constrained set of values the variable can take on, making values easier to understand. If a variable is contrained to a set of values, you may want to consider providing the set of values to which the variable is constrained using the constraints.enum property for that variable, and then providing more information about what each value in the set means using the encodings property for the same variable (e.g. If a variable has possible values of 1, 2, and 3 and these correspond to \"yes\", \"no\" and \"maybe\", you can indicate the constraint of this variable to values of 1, 2, and 3 using the constraints.enum property for that variable, and you can indicate that 1=\"yes\", 2=\"no\", and 3=\"maybe\" using the encodings property for that variable). Examples: 0=No|1=Yes HW=Hello world|GBW=Good bye world|HM=Hi,Mike ordered (boolean) : Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). If ordered is set as True, order will be set based on ordering of values provided in the constraints.enum property for this variable. missingValues (string) : A list of missing values specific to a variable. Examples: Missing|Skipped|No preference Missing trueValues (string) : For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. Examples: REQUIRED required|Yes|Y|Checked falseValues (string) : For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. standardsMappings.url (string) : A url that links out to a published, standardized mapping for the variable. For example, if a variable corresponds to the first question in a published, validated survey instrument to measure depression (e.g. PHQ9), provide a link to the PHQ9 Questionnaire at the NIH Common Data Element Repository. This property is under development, and may be restricted in future to persistent identifier urls such as a DOI . Examples: https://cde.nlm.nih.gov/formView?tinyId=myG8MkTbwg standardsMappings.type (string) : The type of published, standardized mapping available for the variable (e.g. a variable may correspond to a Common Data Element, or CDE, within the NIH Common Data Elements program). This property is under development, and will have a controlled vocabulary. Examples: cde ontology reference_list standardsMappings.label (string) : A free text label that may be used to provide any information available about a published, standardized mapping available for the variable in an unstructured manner (e.g. if a variable corresponds to the first question in the PHQ9 Questionnaire, which is a published, validated survey instrument to measure depression, it may be useful to provide information about th standard that variable reflects using free text such as \"depression, PHQ, PHQ9\"). standardsMappings.source (string) : The source of the published, standardized mapping available for the variable (e.g. the NIH CDE Repository ). This property is under development, and will have a controlled vocabulary. standardsMappings.id (string) : The identifier (ID) of the published, standardized mapping available for the variable within the source provided in the standardsMappings.source property for this variable (e.g. the PHQ9 Questionnaire has been assigned ID \" myG8MkTbwg \" at the NIH CDE Repository CDE source). relatedConcepts.url (string) : The url that links out to the published, standardized concept. Examples: https://meshb.nlm.nih.gov/record/ui?ui=D018410 relatedConcepts.type (string) : The type of mapping to a published set of concepts related to the given field such as ontological information (eg. NCI thesaurus, bioportal etc) relatedConcepts.label (string) : A free text label of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) relatedConcepts.source (string) : The source of the related concept. This property is under development, and will have a controlled vocabulary. relatedConcepts.id (string) : The id locating the individual mapping within the given source. univarStats.median (number) univarStats.mean (number) univarStats.std (number) univarStats.min (number) univarStats.max (number) univarStats.mode (number) univarStats.count (integer) univarStats.twentyFifthPercentile (number) univarStats.seventyFifthPercentile (number) univarStats.categoricalMarginals.name (string) univarStats.categoricalMarginals.count (integer) End of schema - Extra infomation \u00b6 type enum definitions \u00b6 number (A numeric value with optional decimal places. (e.g., 3.14)) integer (A whole number without decimal places. (e.g., 42)) string (A sequence of characters. (e.g., \\\"test\\\")) any (Any type of data is allowed. (e.g., true)) boolean (A binary value representing true or false. (e.g., true)) date (A specific calendar date. (e.g., \\\"2023-05-25\\\")) datetime (A specific date and time, including timezone information. (e.g., \\\"2023-05-25T10:30:00Z\\\")) time (A specific time of day. (e.g., \\\"10:30:00\\\")) year (A specific year. (e.g., 2023) yearmonth (A specific year and month. (e.g., \\\"2023-05\\\")) duration (A length of time. (e.g., \\\"PT1H\\\") geopoint (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278])) format details for date, datetime, time type variables \u00b6 Date Formats (string) A format for a date variable ( date , time , datetime ). Highly recommended to be an ISO8601 format string default : An ISO8601 format string. any : Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. **{PATTERN}**: The value can be parsed according to `{PATTERN}`, which `MUST` follow the date formatting syntax of C / Python [strftime](http://strftime.org/) such as: - \"`%Y-%m-%d` (for date, e.g., 2023-05-25)\" - \"`%Y%-%d` (for date, e.g., 20230525) for date without dashes\" - \"`%Y-%m-%dT%H:%M:%S` (for datetime, e.g., 2023-05-25T10:30:45)\" - \"`%Y-%m-%dT%H:%M:%SZ` (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z)\" - \"`%Y-%m-%dT%H:%M:%S%z` (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300)\" - \"`%Y-%m-%dT%H:%M` (for datetime without seconds, e.g., 2023-05-25T10:30)\" - \"`%Y-%m-%dT%H` (for datetime without minutes and seconds, e.g., 2023-05-25T10)\" - \"`%H:%M:%S` (for time, e.g., 10:30:45)\" - \"`%H:%M:%SZ` (for time with UTC timezone, e.g., 10:30:45Z)\" - \"`%H:%M:%S%z` (for time with timezone offset, e.g., 10:30:45+0300)\"","title":"Data Dictionary"},{"location":"schemas/jsonschema-csvtemplate-fields_v3_manual_edits/#heal-data-dictionary","text":"HEAL DSC Core Metadata piece to track and provide basic information about variables in a tabular data file (i.e. a data file with rows and columns) from your HEAL study. Objective is to list all variables and descriptive information about those variables. This will ensure that potential secondary data users know what data has been collected or calculated and how to use these data. Note that a given study can have multiple tabular data files; You should create a data dictionary for each tabular data file. Thus, a study may have multiple data dictionaries This is an abridged version of the schema and only describes the fields/information you should provide for each variable. For the full HEAL Data Dictionary document schema specification, see here . NOTE: Only name and description properties are required. For categorical variables, constraints.enum and encodings (where applicable) properties are highly encouraged. For studies using HEAL or other common data elements (CDEs), standardsMappings information is highly encouraged. type and format properties may be particularly useful for some variable types (e.g. date-like variables)","title":"HEAL Data Dictionary"},{"location":"schemas/jsonschema-csvtemplate-fields_v3_manual_edits/#properties","text":"module (string) : The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: Demographics PROMIS Medical History name (string,required) : The name of a variable (i.e., field) as it appears in the data. Examples: gender_id title (string) : The human-readable title or label of the variable. Examples: Gender identity description (string,required) : An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). Include measurement units in the description where relevant (e.g. temperature in celsius, distance in millimeters). Examples: The participant's age at the time of study enrollment, in years type (string) : A classification or category of a particular data element or property expected or allowed in the dataset. Must be one of: [\"number\", \"integer\", \"string\", \"any\", \"boolean\", \"date\", \"datetime\", \"time\", \"year\", \"yearmonth\", \"duration\", \"geopoint\"] . For details and examples of each of these types see here . format (of below) : Indicates the format of the type specified in the type property. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats . If type is \"date\", \"datetime\", or \"time\", default format is ISO8601 formatting for those respective types (see details on ISO8601 format for Date , Datetime , or Time ) - If you want to specify a date-like variable using standard Python/C strptime syntax, see here for details. See here for more information about appropriate format values by variable type . constraints.maxLength (integer) : Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. constraints.enum (string) : Constrains possible values to a set of values. If a variable is contrained to a set of values, you may want to consider providing more information about what each value in the set mean using the encodings property for the same variable (e.g. If a variable has possible values of 1, 2, and 3 and these correspond to \"yes\", \"no\" and \"maybe\", you can indicate the constraint of this variable to values of 1, 2, and 3 using the constraints.enum property for that variable, and you can indicate that 1=\"yes\", 2=\"no\", and 3=\"maybe\" using the encodings property for that variable). Examples: 1|2|3|4|5|6|7|8 Treated|Control constraints.pattern (string) : A regular expression pattern the data MUST conform to. constraints.maximum (integer) : Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different than maxLength property. constraints.minimum (integer) : Specifies the minimum value of a field. encodings (string) : For variables constrained to a set of values, variable value encodings provide a way to further annotate each value in the constrained set of values the variable can take on, making values easier to understand. If a variable is contrained to a set of values, you may want to consider providing the set of values to which the variable is constrained using the constraints.enum property for that variable, and then providing more information about what each value in the set means using the encodings property for the same variable (e.g. If a variable has possible values of 1, 2, and 3 and these correspond to \"yes\", \"no\" and \"maybe\", you can indicate the constraint of this variable to values of 1, 2, and 3 using the constraints.enum property for that variable, and you can indicate that 1=\"yes\", 2=\"no\", and 3=\"maybe\" using the encodings property for that variable). Examples: 0=No|1=Yes HW=Hello world|GBW=Good bye world|HM=Hi,Mike ordered (boolean) : Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). If ordered is set as True, order will be set based on ordering of values provided in the constraints.enum property for this variable. missingValues (string) : A list of missing values specific to a variable. Examples: Missing|Skipped|No preference Missing trueValues (string) : For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. Examples: REQUIRED required|Yes|Y|Checked falseValues (string) : For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. standardsMappings.url (string) : A url that links out to a published, standardized mapping for the variable. For example, if a variable corresponds to the first question in a published, validated survey instrument to measure depression (e.g. PHQ9), provide a link to the PHQ9 Questionnaire at the NIH Common Data Element Repository. This property is under development, and may be restricted in future to persistent identifier urls such as a DOI . Examples: https://cde.nlm.nih.gov/formView?tinyId=myG8MkTbwg standardsMappings.type (string) : The type of published, standardized mapping available for the variable (e.g. a variable may correspond to a Common Data Element, or CDE, within the NIH Common Data Elements program). This property is under development, and will have a controlled vocabulary. Examples: cde ontology reference_list standardsMappings.label (string) : A free text label that may be used to provide any information available about a published, standardized mapping available for the variable in an unstructured manner (e.g. if a variable corresponds to the first question in the PHQ9 Questionnaire, which is a published, validated survey instrument to measure depression, it may be useful to provide information about th standard that variable reflects using free text such as \"depression, PHQ, PHQ9\"). standardsMappings.source (string) : The source of the published, standardized mapping available for the variable (e.g. the NIH CDE Repository ). This property is under development, and will have a controlled vocabulary. standardsMappings.id (string) : The identifier (ID) of the published, standardized mapping available for the variable within the source provided in the standardsMappings.source property for this variable (e.g. the PHQ9 Questionnaire has been assigned ID \" myG8MkTbwg \" at the NIH CDE Repository CDE source). relatedConcepts.url (string) : The url that links out to the published, standardized concept. Examples: https://meshb.nlm.nih.gov/record/ui?ui=D018410 relatedConcepts.type (string) : The type of mapping to a published set of concepts related to the given field such as ontological information (eg. NCI thesaurus, bioportal etc) relatedConcepts.label (string) : A free text label of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) relatedConcepts.source (string) : The source of the related concept. This property is under development, and will have a controlled vocabulary. relatedConcepts.id (string) : The id locating the individual mapping within the given source. univarStats.median (number) univarStats.mean (number) univarStats.std (number) univarStats.min (number) univarStats.max (number) univarStats.mode (number) univarStats.count (integer) univarStats.twentyFifthPercentile (number) univarStats.seventyFifthPercentile (number) univarStats.categoricalMarginals.name (string) univarStats.categoricalMarginals.count (integer)","title":"Properties"},{"location":"schemas/jsonschema-csvtemplate-fields_v3_manual_edits/#end-of-schema-extra-infomation","text":"","title":"End of schema - Extra infomation"},{"location":"schemas/jsonschema-csvtemplate-fields_v3_manual_edits/#type-enum-definitions","text":"number (A numeric value with optional decimal places. (e.g., 3.14)) integer (A whole number without decimal places. (e.g., 42)) string (A sequence of characters. (e.g., \\\"test\\\")) any (Any type of data is allowed. (e.g., true)) boolean (A binary value representing true or false. (e.g., true)) date (A specific calendar date. (e.g., \\\"2023-05-25\\\")) datetime (A specific date and time, including timezone information. (e.g., \\\"2023-05-25T10:30:00Z\\\")) time (A specific time of day. (e.g., \\\"10:30:00\\\")) year (A specific year. (e.g., 2023) yearmonth (A specific year and month. (e.g., \\\"2023-05\\\")) duration (A length of time. (e.g., \\\"PT1H\\\") geopoint (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278]))","title":"type enum definitions"},{"location":"schemas/jsonschema-csvtemplate-fields_v3_manual_edits/#format-details-for-date-datetime-time-type-variables","text":"Date Formats (string) A format for a date variable ( date , time , datetime ). Highly recommended to be an ISO8601 format string default : An ISO8601 format string. any : Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. **{PATTERN}**: The value can be parsed according to `{PATTERN}`, which `MUST` follow the date formatting syntax of C / Python [strftime](http://strftime.org/) such as: - \"`%Y-%m-%d` (for date, e.g., 2023-05-25)\" - \"`%Y%-%d` (for date, e.g., 20230525) for date without dashes\" - \"`%Y-%m-%dT%H:%M:%S` (for datetime, e.g., 2023-05-25T10:30:45)\" - \"`%Y-%m-%dT%H:%M:%SZ` (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z)\" - \"`%Y-%m-%dT%H:%M:%S%z` (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300)\" - \"`%Y-%m-%dT%H:%M` (for datetime without seconds, e.g., 2023-05-25T10:30)\" - \"`%Y-%m-%dT%H` (for datetime without minutes and seconds, e.g., 2023-05-25T10)\" - \"`%H:%M:%S` (for time, e.g., 10:30:45)\" - \"`%H:%M:%SZ` (for time with UTC timezone, e.g., 10:30:45Z)\" - \"`%H:%M:%S%z` (for time with timezone offset, e.g., 10:30:45+0300)\"","title":"format details for date, datetime, time type variables"},{"location":"schemas/md_data_dictionary/","text":"HEAL Data Dictionary \u00b6 HEAL DSC Core Metadata piece to track and provide basic information about variables in a tabular data file (i.e. a data file with rows and columns) from your HEAL study. Objective is to list all variables and descriptive information about those variables. This will ensure that potential secondary data users know what data has been collected or calculated and how to use these data. Note that a given study can have multiple tabular data files; If the tabular data files contain different variables, you should create a data dictionary for each tabular data file. Thus, a study may have multiple data dictionaries This is an abridged version of the schema and only describes the fields/information you should provide for each variable. Properties \u00b6 For each item (variable), describe the item (variable) by providing an array of descriptive information in structured fields about that item (variable). The descriptive information requested about each variable, and the structure in which it is required to be entered, is listed below. Note that the only descriptive information you are required to provide about each item (variable) is the variable name and description . You may add a field or fields in addition to those described below in order to provide additional descriptive information about any or all variables. - **`module`** *(string)*: The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: ```json \"Demographics\" ``` ```json \"Medical History\" ``` - **`name`** *(string, required)*: The name of a variable (i.e., field) as it appears in the data. <br> [Required] . - **`title`** *(string)*: The human-readable title or label of the variable. E.g. \"My Variable\" may be the title for a variable with the name \"my_variable\" <br> [Highly recommended] . - **`description`** *(string, required)*: An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). <br> [Required] . - **`type`** *(string)*: A classification or category of a particular data element or property expected or allowed in the dataset. Must be one of: `[\"number\", \"integer\", \"string\", \"any\", \"boolean\", \"date\", \"datetime\", \"time\", \"year\", \"yearmonth\", \"duration\", \"geopoint\"]`. <br> - `number` (A numeric value with optional decimal places. (e.g., 3.14)) - `integer` (A whole number without decimal places. (e.g., 42)) - `string` (A sequence of characters. (e.g., \\\"test\\\")) - `any` (Any type of data is allowed. (e.g., true)) - `boolean` (A binary value representing true or false. (e.g., true)) - `date` (A specific calendar date. (e.g., \\\"2023-05-25\\\")) - `datetime` (A specific date and time, including timezone information. (e.g., \\\"2023-05-25T10:30:00Z\\\")) - `time` (A specific time of day. (e.g., \\\"10:30:00\\\")) - `year` (A specific year. (e.g., 2023) - `yearmonth` (A specific year and month. (e.g., \\\"2023-05\\\")) - `duration` (A length of time. (e.g., \\\"PT1H\\\") - `geopoint` (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278])) - **`format`**: A format taken from one of the [frictionless specification](https://specs.frictionlessdata.io/) schemas. For example, for tabular data, there is the [Table Schema specification](https://specs.frictionlessdata.io/table-schema)<br> Each format is dependent on the `type` specified. For example: If `type` is \"string\", then see the String formats. If `type` is one of the date-like formats, then see Date formats. - **Any of** - : Must be one of: `[\"uri\", \"email\", \"binary\", \"uuid\"]`. - : A format for a date variable (`date`,`time`,`datetime`). \\n\\t* **default**: An ISO8601 format string. \\n\\t* **any**: Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. \\n\\t* **{PATTERN}**: The value can be parsed according to `{PATTERN}`, which `MUST` follow the date formatting syntax of C / Python [strftime](http://strftime.org/).<br> \\nExamples:<br> `%Y-%m-%d` (for date, e.g., 2023-05-25) `%Y%-%d` (for date, e.g., 20230525) for date without dashes\" `%Y-%m-%dT%H:%M:%S` (for datetime, e.g., 2023-05-25T10:30:45) `%Y-%m-%dT%H:%M:%SZ` (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z) `%Y-%m-%dT%H:%M:%S%z` (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300) `%Y-%m-%dT%H:%M` (for datetime without seconds, e.g., 2023-05-25T10:30) `%Y-%m-%dT%H` (for datetime without minutes and seconds, e.g., 2023-05-25T10) `%H:%M:%S` (for time, e.g., 10:30:45) `%H:%M:%SZ` (for time with UTC timezone, e.g., 10:30:45Z) `%H:%M:%S%z` (for time with timezone offset, e.g., 10:30:45+0300) . - : The two types of formats for `geopoint` (describing a geographic point). - **One of** - *array*: A JSON array or a string parsable as a JSON array where each item is a number with the first as the latitude and the second as longitude. . - *object*: Contains latitude and longitude with two keys (\"lat\" and \"long\") with number items mapped to each key. - : The JSON object according to the geojson spec. Must be one of: `[\"topojson\", \"default\"]`. - **`constraints`** *(object)* - **`maxLength`** *(integer)*: Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11.<br> [Optional,if applicable] . - **`enum`** *(array)*: Constrains possible values to a set of values.<br> [Optional,if applicable] . - **`pattern`** *(string)*: A regular expression pattern the data MUST conform to.<br> [Optional,if applicable] . - **`maximum`** *(integer)*: Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property.<br> [Optional,if applicable] . - **`minimum`** *(integer)*: Specifies the minimum value of a field.<br> [Optional,if applicable] . - **`encodings`** *(object)*: Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. <br> Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values.<br> Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations).<br> [Optional,if applicable] . Examples: ```json { \"0\": \"No\", \"1\": \"Yes\" } ``` ```json { \"HW\": \"Hello world\", \"GBW\": \"Good bye world\", \"HM\": \"Hi, Mike\" } ``` - **`ordered`** *(boolean)*: Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree).<br> [Optional,if applicable] . - **`missingValues`** *(array)*: A list of missing values specific to a variable.<br> [Highly recommended] . - **`trueValues`** *(array)*: For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values.<br> [Optional, if applicable] . - **Items** *(string)* Examples: ```json \"Required\" ``` ```json \"REQUIRED\" ``` ```json \"required\" ``` ```json \"Yes\" ``` ```json \"Checked\\\"\" ``` - **`falseValues`** *(array)*: For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. - **`repo_link`** *(string)*: A link to the variable as it exists on the home repository, if applicable . - **`standardsMappings`** *(array)*: A published set of standard variables such as the NIH Common Data Elements program. [Autopopulated, if not filled]. - **Items** *(object)* - **`type`** *(string)*: The **type** of mapping linked to a published set of standard variables such as the NIH Common Data Elements program. [Autopopulated, if not filled] . Examples: ```json \"cde\" ``` ```json \"ontology\" ``` ```json \"reference_list\" ``` - **`label`** *(string)*: A free text **label** of a mapping indicating a mapping(s) to a published set of standard variables such as the NIH Common Data Elements program.<br> [Autopopulated, if not filled] . Examples: ```json \"substance use\" ``` ```json \"chemical compound\" ``` ```json \"promis\" ``` - **`url`** *(string)*: The url that links out to the published, standardized mapping.<br> [Autopopulated, if not filled] . Examples: ```json \"https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI\" ``` - **`source`** *(string)*: The source of the standardized variable. Examples: ```json \"TBD (will have controlled vocabulary)\" ``` - **`id`** *(string)*: The id locating the individual mapping within the given source. - **`relatedConcepts`** *(array)*: Mappings to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) [Autopopulated, if not filled]. - **Items** *(object)* - **`type`** *(string)*: The **type** of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc)<br> [Autopopulated, if not filled] . - **`label`** *(string)*: A free text **label** of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc)<br> [Autopopulated, if not filled] . - **`url`** *(string)*: The url that links out to the published, standardized concept.<br> [Autopopulated, if not filled] . Examples: ```json \"https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI\" ``` - **`source`** *(string)*: The source of the related concept.<br> [Autopopulated, if not filled] . Examples: ```json \"TBD (will have controlled vocabulary)\" ``` - **`id`** *(string)*: The id locating the individual mapping within the given source.<br> [Autopopulated, if not filled] . - **`univarStats`** *(object)*: Univariate statistics inferred from the data about the given variable <br> [Experimental] . - **`median`** *(number)* - **`mean`** *(number)* - **`std`** *(number)* - **`min`** *(number)* - **`max`** *(number)* - **`mode`** *(number)* - **`count`** *(integer)*: Minimum: `0`. - **`twentyFifthPercentile`** *(number)* - **`seventyFifthPercentile`** *(number)* - **`categoricalMarginals`** *(array)* - **Items** *(object)* - **`name`** *(string)* - **`count`** *(integer)*","title":"HEAL Data Dictionary"},{"location":"schemas/md_data_dictionary/#heal-data-dictionary","text":"HEAL DSC Core Metadata piece to track and provide basic information about variables in a tabular data file (i.e. a data file with rows and columns) from your HEAL study. Objective is to list all variables and descriptive information about those variables. This will ensure that potential secondary data users know what data has been collected or calculated and how to use these data. Note that a given study can have multiple tabular data files; If the tabular data files contain different variables, you should create a data dictionary for each tabular data file. Thus, a study may have multiple data dictionaries This is an abridged version of the schema and only describes the fields/information you should provide for each variable.","title":"HEAL Data Dictionary"},{"location":"schemas/md_data_dictionary/#properties","text":"For each item (variable), describe the item (variable) by providing an array of descriptive information in structured fields about that item (variable). The descriptive information requested about each variable, and the structure in which it is required to be entered, is listed below. Note that the only descriptive information you are required to provide about each item (variable) is the variable name and description . You may add a field or fields in addition to those described below in order to provide additional descriptive information about any or all variables. - **`module`** *(string)*: The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: ```json \"Demographics\" ``` ```json \"Medical History\" ``` - **`name`** *(string, required)*: The name of a variable (i.e., field) as it appears in the data. <br> [Required] . - **`title`** *(string)*: The human-readable title or label of the variable. E.g. \"My Variable\" may be the title for a variable with the name \"my_variable\" <br> [Highly recommended] . - **`description`** *(string, required)*: An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). <br> [Required] . - **`type`** *(string)*: A classification or category of a particular data element or property expected or allowed in the dataset. Must be one of: `[\"number\", \"integer\", \"string\", \"any\", \"boolean\", \"date\", \"datetime\", \"time\", \"year\", \"yearmonth\", \"duration\", \"geopoint\"]`. <br> - `number` (A numeric value with optional decimal places. (e.g., 3.14)) - `integer` (A whole number without decimal places. (e.g., 42)) - `string` (A sequence of characters. (e.g., \\\"test\\\")) - `any` (Any type of data is allowed. (e.g., true)) - `boolean` (A binary value representing true or false. (e.g., true)) - `date` (A specific calendar date. (e.g., \\\"2023-05-25\\\")) - `datetime` (A specific date and time, including timezone information. (e.g., \\\"2023-05-25T10:30:00Z\\\")) - `time` (A specific time of day. (e.g., \\\"10:30:00\\\")) - `year` (A specific year. (e.g., 2023) - `yearmonth` (A specific year and month. (e.g., \\\"2023-05\\\")) - `duration` (A length of time. (e.g., \\\"PT1H\\\") - `geopoint` (A pair of latitude and longitude coordinates. (e.g., [51.5074, -0.1278])) - **`format`**: A format taken from one of the [frictionless specification](https://specs.frictionlessdata.io/) schemas. For example, for tabular data, there is the [Table Schema specification](https://specs.frictionlessdata.io/table-schema)<br> Each format is dependent on the `type` specified. For example: If `type` is \"string\", then see the String formats. If `type` is one of the date-like formats, then see Date formats. - **Any of** - : Must be one of: `[\"uri\", \"email\", \"binary\", \"uuid\"]`. - : A format for a date variable (`date`,`time`,`datetime`). \\n\\t* **default**: An ISO8601 format string. \\n\\t* **any**: Any parsable representation of a date/time/datetime. The implementing library can attempt to parse the datetime via a range of strategies. \\n\\t* **{PATTERN}**: The value can be parsed according to `{PATTERN}`, which `MUST` follow the date formatting syntax of C / Python [strftime](http://strftime.org/).<br> \\nExamples:<br> `%Y-%m-%d` (for date, e.g., 2023-05-25) `%Y%-%d` (for date, e.g., 20230525) for date without dashes\" `%Y-%m-%dT%H:%M:%S` (for datetime, e.g., 2023-05-25T10:30:45) `%Y-%m-%dT%H:%M:%SZ` (for datetime with UTC timezone, e.g., 2023-05-25T10:30:45Z) `%Y-%m-%dT%H:%M:%S%z` (for datetime with timezone offset, e.g., 2023-05-25T10:30:45+0300) `%Y-%m-%dT%H:%M` (for datetime without seconds, e.g., 2023-05-25T10:30) `%Y-%m-%dT%H` (for datetime without minutes and seconds, e.g., 2023-05-25T10) `%H:%M:%S` (for time, e.g., 10:30:45) `%H:%M:%SZ` (for time with UTC timezone, e.g., 10:30:45Z) `%H:%M:%S%z` (for time with timezone offset, e.g., 10:30:45+0300) . - : The two types of formats for `geopoint` (describing a geographic point). - **One of** - *array*: A JSON array or a string parsable as a JSON array where each item is a number with the first as the latitude and the second as longitude. . - *object*: Contains latitude and longitude with two keys (\"lat\" and \"long\") with number items mapped to each key. - : The JSON object according to the geojson spec. Must be one of: `[\"topojson\", \"default\"]`. - **`constraints`** *(object)* - **`maxLength`** *(integer)*: Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11.<br> [Optional,if applicable] . - **`enum`** *(array)*: Constrains possible values to a set of values.<br> [Optional,if applicable] . - **`pattern`** *(string)*: A regular expression pattern the data MUST conform to.<br> [Optional,if applicable] . - **`maximum`** *(integer)*: Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property.<br> [Optional,if applicable] . - **`minimum`** *(integer)*: Specifies the minimum value of a field.<br> [Optional,if applicable] . - **`encodings`** *(object)*: Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. <br> Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values.<br> Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations).<br> [Optional,if applicable] . Examples: ```json { \"0\": \"No\", \"1\": \"Yes\" } ``` ```json { \"HW\": \"Hello world\", \"GBW\": \"Good bye world\", \"HM\": \"Hi, Mike\" } ``` - **`ordered`** *(boolean)*: Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree).<br> [Optional,if applicable] . - **`missingValues`** *(array)*: A list of missing values specific to a variable.<br> [Highly recommended] . - **`trueValues`** *(array)*: For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values.<br> [Optional, if applicable] . - **Items** *(string)* Examples: ```json \"Required\" ``` ```json \"REQUIRED\" ``` ```json \"required\" ``` ```json \"Yes\" ``` ```json \"Checked\\\"\" ``` - **`falseValues`** *(array)*: For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. - **`repo_link`** *(string)*: A link to the variable as it exists on the home repository, if applicable . - **`standardsMappings`** *(array)*: A published set of standard variables such as the NIH Common Data Elements program. [Autopopulated, if not filled]. - **Items** *(object)* - **`type`** *(string)*: The **type** of mapping linked to a published set of standard variables such as the NIH Common Data Elements program. [Autopopulated, if not filled] . Examples: ```json \"cde\" ``` ```json \"ontology\" ``` ```json \"reference_list\" ``` - **`label`** *(string)*: A free text **label** of a mapping indicating a mapping(s) to a published set of standard variables such as the NIH Common Data Elements program.<br> [Autopopulated, if not filled] . Examples: ```json \"substance use\" ``` ```json \"chemical compound\" ``` ```json \"promis\" ``` - **`url`** *(string)*: The url that links out to the published, standardized mapping.<br> [Autopopulated, if not filled] . Examples: ```json \"https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI\" ``` - **`source`** *(string)*: The source of the standardized variable. Examples: ```json \"TBD (will have controlled vocabulary)\" ``` - **`id`** *(string)*: The id locating the individual mapping within the given source. - **`relatedConcepts`** *(array)*: Mappings to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) [Autopopulated, if not filled]. - **Items** *(object)* - **`type`** *(string)*: The **type** of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc)<br> [Autopopulated, if not filled] . - **`label`** *(string)*: A free text **label** of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc)<br> [Autopopulated, if not filled] . - **`url`** *(string)*: The url that links out to the published, standardized concept.<br> [Autopopulated, if not filled] . Examples: ```json \"https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI\" ``` - **`source`** *(string)*: The source of the related concept.<br> [Autopopulated, if not filled] . Examples: ```json \"TBD (will have controlled vocabulary)\" ``` - **`id`** *(string)*: The id locating the individual mapping within the given source.<br> [Autopopulated, if not filled] . - **`univarStats`** *(object)*: Univariate statistics inferred from the data about the given variable <br> [Experimental] . - **`median`** *(number)* - **`mean`** *(number)* - **`std`** *(number)* - **`min`** *(number)* - **`max`** *(number)* - **`mode`** *(number)* - **`count`** *(integer)*: Minimum: `0`. - **`twentyFifthPercentile`** *(number)* - **`seventyFifthPercentile`** *(number)* - **`categoricalMarginals`** *(array)* - **Items** *(object)* - **`name`** *(string)* - **`count`** *(integer)*","title":"Properties"},{"location":"schemas/md_data_dictionary_fields/","text":"HEAL Data Dictionary \u00b6 HEAL DSC Core Metadata piece to track and provide basic information about variables in a tabular data file (i.e. a data file with rows and columns) from your HEAL study. Objective is to list all variables and descriptive information about those variables. This will ensure that potential secondary data users know what data has been collected or calculated and how to use these data. Note that a given study can have multiple tabular data files; If the tabular data files contain different variables, you should create a data dictionary for each tabular data file. Thus, a study may have multiple data dictionaries This is an abridged version of the schema and only describes the fields/information you should provide for each variable. For the full HEAL Data Dictionary document schema specification, see here . Properties \u00b6 module (string) : The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: \"Demographics\" \"PROMIS\" \"Substance use\" \"Medical History\" \"Sleep questions\" \"Physical activity\" name (string, required) : The name of a variable (i.e., field) as it appears in the data. title (string) : The human-readable title or label of the variable. Examples: \"My Variable (for name of my_variable)\" description (string, required) : An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). Examples: \"Definition\" \"Question text (if a survey)\" type (string) : A classification or category of a particular data element or property expected or allowed in the dataset. Must be one of: [\"number\", \"integer\", \"string\", \"any\", \"boolean\", \"date\", \"datetime\", \"time\", \"year\", \"yearmonth\", \"duration\", \"geopoint\"] . For details and examples of each of these types see [here]. format : A format taken from one of the frictionless specification schemas. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats. If type is one of the date-like formats, then see Date formats. For format type details and examples and technical details on how to populate this field see [here]. constraints (object) maxLength (integer) : Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. enum (array) : Constrains possible values to a set of values. pattern (string) : A regular expression pattern the data MUST conform to. maximum (integer) : Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property. minimum (integer) : Specifies the minimum value of a field. encodings (object) : Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values. Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations). Examples: { \"0\" : \"No\" , \"1\" : \"Yes\" } { \"HW\" : \"Hello world\" , \"GBW\" : \"Good bye world\" , \"HM\" : \"Hi, Mike\" } ordered (boolean) : Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). missingValues (array) : A list of missing values specific to a variable. trueValues (array) : For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. Items (string) Examples: \"Required\" \"REQUIRED\" \"required\" \"Yes\" \"Checked\\\"\" falseValues (array) : For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. repo_link (string) : A link to the variable as it exists on the home repository, if applicable . standardsMappings (array) : A published set of standard variables such as the NIH Common Data Elements program. Items (object) type (string) : The type of mapping linked to a published set of standard variables such as the NIH Common Data Elements program. Examples: \"cde\" \"ontology\" \"reference_list\" label (string) : A free text label of a mapping indicating a mapping(s) to a published set of standard variables such as the NIH Common Data Elements program. Examples: \"substance use\" \"chemical compound\" \"promis\" url (string) : The url that links out to the published, standardized mapping. Examples: \"https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI\" source (string) : The source of the standardized variable. Examples: \"TBD (will have controlled vocabulary)\" id (string) : The id locating the individual mapping within the given source. relatedConcepts (array) : Mappings to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) Items (object) type (string) : The type of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc). label (string) : A free text label of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc). url (string) : The url that links out to the published, standardized concept. Examples: \"https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI\" source (string) : The source of the related concept. Examples: \"TBD (will have controlled vocabulary)\" id (string) : The id locating the individual mapping within the given source. univarStats (object) : Univariate statistics inferred from the data about the given variable. median (number) mean (number) std (number) min (number) max (number) mode (number) count (integer) : Minimum: 0 . twentyFifthPercentile (number) seventyFifthPercentile (number) categoricalMarginals (array) Items (object) name (string) count (integer)","title":"HEAL Data Dictionary"},{"location":"schemas/md_data_dictionary_fields/#heal-data-dictionary","text":"HEAL DSC Core Metadata piece to track and provide basic information about variables in a tabular data file (i.e. a data file with rows and columns) from your HEAL study. Objective is to list all variables and descriptive information about those variables. This will ensure that potential secondary data users know what data has been collected or calculated and how to use these data. Note that a given study can have multiple tabular data files; If the tabular data files contain different variables, you should create a data dictionary for each tabular data file. Thus, a study may have multiple data dictionaries This is an abridged version of the schema and only describes the fields/information you should provide for each variable. For the full HEAL Data Dictionary document schema specification, see here .","title":"HEAL Data Dictionary"},{"location":"schemas/md_data_dictionary_fields/#properties","text":"module (string) : The section, form, survey instrument, set of measures or other broad category used to group variables. Examples: \"Demographics\" \"PROMIS\" \"Substance use\" \"Medical History\" \"Sleep questions\" \"Physical activity\" name (string, required) : The name of a variable (i.e., field) as it appears in the data. title (string) : The human-readable title or label of the variable. Examples: \"My Variable (for name of my_variable)\" description (string, required) : An extended description of the variable. This could be the definition of a variable or the question text (e.g., if a survey). Examples: \"Definition\" \"Question text (if a survey)\" type (string) : A classification or category of a particular data element or property expected or allowed in the dataset. Must be one of: [\"number\", \"integer\", \"string\", \"any\", \"boolean\", \"date\", \"datetime\", \"time\", \"year\", \"yearmonth\", \"duration\", \"geopoint\"] . For details and examples of each of these types see [here]. format : A format taken from one of the frictionless specification schemas. Each format is dependent on the type specified. For example: If type is \"string\", then see the String formats. If type is one of the date-like formats, then see Date formats. For format type details and examples and technical details on how to populate this field see [here]. constraints (object) maxLength (integer) : Indicates the maximum length of an iterable (e.g., array, string, or object). For example, if 'Hello World' is the longest value of a categorical variable, this would be a maxLength of 11. enum (array) : Constrains possible values to a set of values. pattern (string) : A regular expression pattern the data MUST conform to. maximum (integer) : Specifies the maximum value of a field (e.g., maximum -- or most recent -- date, maximum integer etc). Note, this is different then maxLength property. minimum (integer) : Specifies the minimum value of a field. encodings (object) : Variable value encodings provide a way to further annotate any value within a any variable type, making values easier to understand. Many analytic software programs (e.g., SPSS,Stata, and SAS) use numerical encodings and some algorithms only support numerical values. Encodings (and mappings) allow categorical values to be stored as numerical values. Additionally, as another use case, this field provides a way to store categoricals that are stored as \"short\" labels (such as abbreviations). Examples: { \"0\" : \"No\" , \"1\" : \"Yes\" } { \"HW\" : \"Hello world\" , \"GBW\" : \"Good bye world\" , \"HM\" : \"Hi, Mike\" } ordered (boolean) : Indicates whether a categorical variable is ordered. This variable is relevant for variables that have an ordered relationship but not necessarily a numerical relationship (e.g., Strongly disagree < Disagree < Neutral < Agree). missingValues (array) : A list of missing values specific to a variable. trueValues (array) : For boolean (true) variable (as defined in type field), this field allows a physical string representation to be cast as true (increasing readability of the field). It can include one or more values. Items (string) Examples: \"Required\" \"REQUIRED\" \"required\" \"Yes\" \"Checked\\\"\" falseValues (array) : For boolean (false) variable (as defined in type field), this field allows a physical string representation to be cast as false (increasing readability of the field) that is not a standard false value. It can include one or more values. repo_link (string) : A link to the variable as it exists on the home repository, if applicable . standardsMappings (array) : A published set of standard variables such as the NIH Common Data Elements program. Items (object) type (string) : The type of mapping linked to a published set of standard variables such as the NIH Common Data Elements program. Examples: \"cde\" \"ontology\" \"reference_list\" label (string) : A free text label of a mapping indicating a mapping(s) to a published set of standard variables such as the NIH Common Data Elements program. Examples: \"substance use\" \"chemical compound\" \"promis\" url (string) : The url that links out to the published, standardized mapping. Examples: \"https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI\" source (string) : The source of the standardized variable. Examples: \"TBD (will have controlled vocabulary)\" id (string) : The id locating the individual mapping within the given source. relatedConcepts (array) : Mappings to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc) Items (object) type (string) : The type of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc). label (string) : A free text label of mapping to a published set of concepts related to the given field such as ontological information (eg., NCI thesaurus, bioportal etc). url (string) : The url that links out to the published, standardized concept. Examples: \"https://cde.nlm.nih.gov/deView?tinyId=XyuSGdTTI\" source (string) : The source of the related concept. Examples: \"TBD (will have controlled vocabulary)\" id (string) : The id locating the individual mapping within the given source. univarStats (object) : Univariate statistics inferred from the data about the given variable. median (number) mean (number) std (number) min (number) max (number) mode (number) count (integer) : Minimum: 0 . twentyFifthPercentile (number) seventyFifthPercentile (number) categoricalMarginals (array) Items (object) name (string) count (integer)","title":"Properties"},{"location":"schemas/md_experiment_tracker/","text":"HEAL Experiment Tracker \u00b6 HEAL DSC Core Metadata piece to track and provide basic information about experiment(s) you will perform as part of your HEAL study. Clinical studies will often have only one experiment to report, while basic science studies often have several experiments that are grouped together under a single study. Properties \u00b6 experiment.id (string) : id assigned to each experiment relevant to the data package; prefix is 'exp-' followed by a number starting with 1 for the first experiment, and iterating by 1 for each successive experiment (i.e. exp-1, exp-2, etc.). experiment.type (string) : discovery|materials and methods development. Must be one of: [\"discovery\", \"materials and methods development\"] . experiment.description (string) : provide a brief description of the experiment; this is NOT a protocol. experiment.question (array) : what question(s) does the experimentalist hope to address with this experiment? be as specific as possible. Items (string) experiment.hypothesis (array) : for each question the experimentalist hopes to address with this experiment, what does the experimentalist hypothesize will be the result(s) of the experiment? Be as specific as possible. Items (string)","title":"Experiment Tracker"},{"location":"schemas/md_experiment_tracker/#heal-experiment-tracker","text":"HEAL DSC Core Metadata piece to track and provide basic information about experiment(s) you will perform as part of your HEAL study. Clinical studies will often have only one experiment to report, while basic science studies often have several experiments that are grouped together under a single study.","title":"HEAL Experiment Tracker"},{"location":"schemas/md_experiment_tracker/#properties","text":"experiment.id (string) : id assigned to each experiment relevant to the data package; prefix is 'exp-' followed by a number starting with 1 for the first experiment, and iterating by 1 for each successive experiment (i.e. exp-1, exp-2, etc.). experiment.type (string) : discovery|materials and methods development. Must be one of: [\"discovery\", \"materials and methods development\"] . experiment.description (string) : provide a brief description of the experiment; this is NOT a protocol. experiment.question (array) : what question(s) does the experimentalist hope to address with this experiment? be as specific as possible. Items (string) experiment.hypothesis (array) : for each question the experimentalist hopes to address with this experiment, what does the experimentalist hypothesize will be the result(s) of the experiment? Be as specific as possible. Items (string)","title":"Properties"},{"location":"schemas/md_resource_tracker/","text":"HEAL Resource Tracker \u00b6 HEAL DSC Core Metadata piece to track and provide basic information about resource(s)/file(s) that support/are produced by/result from experiments you perform/will perform as part of your HEAL study.Objective is to list at least all files that will be submitted to a data repository in order to describe what each file is, how they relate to each other/how to use them, and how they relate to results/publications shared by the study group. Files may include results files (e.g. publications or draft publications/pieces of publications), processed and raw data files, protocol and analytic plan files, data dictionaries for tabular data files, other metadata as appropriate to data/field type, etc. Properties \u00b6 resource.id (string) : Unique ID assigned to each resource file; If using the DSC Packaging application to annotate your resource(s), these IDs will be auto-assigned when you use the Add DSC Package button above the form to add your DSC Package Directory. Auto-assignment of IDs occurs by searching the directory for any resource annotation files already saved, identifying the resource ID with the highest resource ID number, and adding 1 to that number to get the resource ID number and unique resource ID for the current resource. path (string) : The full file path to your resource file. If you are using the DSC Packaging application and would like to use a single form to annotate multiple 'like' files, click the 'Add Multiple like Files' button above the form and drag and drop all of the like files you want to annotate together in that box. The file path for the first of the file paths you dropped in the box will be added to this field. description (string) : A description of your resource. For resources that consist of multiple 'like' files, provide a description of the multi-file resource here and use the Resource File Description field to provide any description specific to each/any one specific file in the set. category (string) : Broad category your resource falls into; Generally, these categories are: results, data, metadata, code. However, the actual category options parse the categories just a bit finer (e.g. options for data resources include either 'tabular-data' or 'non-tabular-data'). Must be one of: [\"single-result\", \"manuscript or report\", \"tabular-data\", \"non-tabular-data\", \"metadata\", \"code\"] . exp.belongs.to (string) : If the file pertains specifically to one of the study experiments, list the experiment ID for that experiment here; If the file pertains to more than one experiment, or to all experiments/the study as a whole, leave this blank; Use the experiment ID as assigned/formatted in your Experiment Tracker file (prefix is 'exp-' followed by a number starting with 1 for the first experiment, and iterating by 1 for each successive experiment - i.e. exp-1, exp-2, etc.). name (string) : File path stem; Auto-inferred from file path. title (string) : Human-readable title/name of resource. description.file.name.convention (string) : For multi-file resource containing multiple files of the same type (multiple 'like' files), provide the naming convention of the files (e.g. for a file set: [subject-01-protocol-A-day-2020-06-05.csv, subject-02-protocol-A-day-2020-06-05.csv, subject-02-protocol-B-day-2020-12-05.csv], you would specify the naming convention as: subject-{subject ID}-protocol-{protocol ID}-day-{date of measurment in YYYY-MM-DD}). If you are using the DSC Packaging application, you can use the Apply Name Convention button above the form to validate your name convention format and use a valid file name convention to generate a minimal 'Resource File Description' that is a minimal description specific to each file in the multi-file resource set. description.file (string) : For a multi-file resource containing multiple files of the same type (multiple 'like' files), a description specific to the specific current file that is a component of that multi-file set. description.row (string) : For a tabular data resource, a description of what one row in the tabular data resource represents; e.g. one row represents one subject at one timepoint. category.sub.metadata (string) : Sub-category for a metadata resource. Must be one of: [\"\", \"heal-formatted-data-dictionary\", \"other-formatted-data-dictionary\", \"protocol\", \"id-map\", \"analysis-plan\", \"heal-formatted-results-tracker\", \"heal-formatted-experiment-tracker\"] . category.sub.data (string) : Sub-category for a data resource. Must be one of: [\"\", \"raw\", \"processed-intermediate\", \"processed-final\"] . category.sub.results (string) : Sub-category for a results resource. Must be one of: [\"\", \"figure\", \"table\", \"text\", \"draft-publication\", \"publication\", \"report\", \"white-paper\", \"presentation\", \"poster\"] . assoc.file.dd (array) : For a tabular data file resources, a reference/file path to associated data dictionary file(s) - preferably in heal csv data dictionary format. Items (string) assoc.file.protocol (array) : For a data file resource, a reference/file path to associated protocol file(s). Items (string) assoc.file.result.tracker (array) : For a manuscript or report, a reference/file path to associated HEAL results tracker file - HEAL results tracker is a file that tracks each result in a publication, report or poster, along with the data and other supporting files that underly/support each result. If you are using the DSC Packaging Desktop Application, you can head to the Result Tracker tab of the application to create a HEAL formatted result tracker for your published paper or report. Items (string) assoc.file.depends.on (array) : For all resource files, if the current resource file has dependencies/if other files are necessary to make this file (e.g. raw data file necessary to make processed data file), or to interpret/understand this file (e.g. protocol, analysis plan, etc.), list them here; if documenting resources wholistically (i.e. documenting all resources related to a study), only list dependencies one layer deep; if documenting resources minimally (i.e. only documenting resources that will be publicly shared), list dependencies liberally; dependencies can be data, code, protocol, etc.; if already listed under assoc.file.dd, assoc.file.protocol, or assoc.file.id.map no need to repeat here. Items (string) assoc.file.result.depends.on (array) : if the current resource file is a heal formatted result tracker (this tracks the single results in a publication or report), use this field to list each result in the tracker along with its corresponding dependencies (i.e. files the result depends on, or are necessary to make/reach/interpret the result); if documenting resources wholistically (i.e. documenting all resources related to a study), only list dependencies one layer deep; if documenting resources minimally (i.e. only documenting resources that will be publicly shared), list dependencies liberally; dependencies can be data, code, protocol, etc. Items (object) result.id (string) result.id.depends.on (array) assoc.file.multi.like.file (array) : if the current resource file is annotating a resource that is one of multiple 'like' files, this field will list all files that are part of the resources. Items (string) access (array) : What is the current/final access level anticipated for this resource? Options are permanent-private (current and final access level is private), temporary-private (current access level is private but final access level will be either restricted-access or public), restricted-access (current, final, or current AND final access level will allow request of data with barriers/restrictions to access), public (current, final, or current AND final access level will allow largely unrestricted request of/access to data); Many investigators will designate data as currently temporary-private, with a final access level of either restricted-access or public: In this case choose both temporary-private AND either 1) restricted-access or 2) public, then add the date at which you expect to transition from temporary-private to either restricted-access or public in the Access Date field below; Private means members of the public cannot request access; Restricted access means they can request access but there is gate-keeping; Public access means they can often access the data without requesting access, and with minimal barriers to access. Items (string) : Must be one of: [\"\", \"permanent-private\", \"temporary-private\", \"restricted-access\", \"public\"] . access.date (string) : If the resource file is currently being held as temporary-private access level and will transition to either restricted-access or public access level at some point, please provide an anticipated date at which this transition will occur - Best guesses are appreciated, however you will NOT be held to this date and may update this date at any time. format (string) : auto inferred; e.g. csv. format.software (string) : If the file format of the resource file is proprietary and requires specific software to open/interpret, provide the software name and version used by the study group to produce/work with the file; e.g. Origin 11.0, CorelDraw 5.6. profile (string) : auto inferred; e.g. tabular-data-resource. mediatype (string) : auto inferred; e.g. text/csv. encoding (string) : auto inferred; e.g. utf-8. schema (string) : auto inferred; for tabular resource, schema of fields contained in tabular resource; might replace this with ref to either heal csv dd or heal json dd. resource.create.date.time (string) : Date time of resource creation; auto-inferred. resource.mod.date.time (string) : Date time at which the resource was last modified; auto-inferred. restrk.create.date.time (string) : Date time at which the resource tracker file for the resource was created; auto-inferred. restrk.mod.date.time (string) : Date time at which the resource tracker file for the resource was last modified; auto-inferred.","title":"Resource Tracker"},{"location":"schemas/md_resource_tracker/#heal-resource-tracker","text":"HEAL DSC Core Metadata piece to track and provide basic information about resource(s)/file(s) that support/are produced by/result from experiments you perform/will perform as part of your HEAL study.Objective is to list at least all files that will be submitted to a data repository in order to describe what each file is, how they relate to each other/how to use them, and how they relate to results/publications shared by the study group. Files may include results files (e.g. publications or draft publications/pieces of publications), processed and raw data files, protocol and analytic plan files, data dictionaries for tabular data files, other metadata as appropriate to data/field type, etc.","title":"HEAL Resource Tracker"},{"location":"schemas/md_resource_tracker/#properties","text":"resource.id (string) : Unique ID assigned to each resource file; If using the DSC Packaging application to annotate your resource(s), these IDs will be auto-assigned when you use the Add DSC Package button above the form to add your DSC Package Directory. Auto-assignment of IDs occurs by searching the directory for any resource annotation files already saved, identifying the resource ID with the highest resource ID number, and adding 1 to that number to get the resource ID number and unique resource ID for the current resource. path (string) : The full file path to your resource file. If you are using the DSC Packaging application and would like to use a single form to annotate multiple 'like' files, click the 'Add Multiple like Files' button above the form and drag and drop all of the like files you want to annotate together in that box. The file path for the first of the file paths you dropped in the box will be added to this field. description (string) : A description of your resource. For resources that consist of multiple 'like' files, provide a description of the multi-file resource here and use the Resource File Description field to provide any description specific to each/any one specific file in the set. category (string) : Broad category your resource falls into; Generally, these categories are: results, data, metadata, code. However, the actual category options parse the categories just a bit finer (e.g. options for data resources include either 'tabular-data' or 'non-tabular-data'). Must be one of: [\"single-result\", \"manuscript or report\", \"tabular-data\", \"non-tabular-data\", \"metadata\", \"code\"] . exp.belongs.to (string) : If the file pertains specifically to one of the study experiments, list the experiment ID for that experiment here; If the file pertains to more than one experiment, or to all experiments/the study as a whole, leave this blank; Use the experiment ID as assigned/formatted in your Experiment Tracker file (prefix is 'exp-' followed by a number starting with 1 for the first experiment, and iterating by 1 for each successive experiment - i.e. exp-1, exp-2, etc.). name (string) : File path stem; Auto-inferred from file path. title (string) : Human-readable title/name of resource. description.file.name.convention (string) : For multi-file resource containing multiple files of the same type (multiple 'like' files), provide the naming convention of the files (e.g. for a file set: [subject-01-protocol-A-day-2020-06-05.csv, subject-02-protocol-A-day-2020-06-05.csv, subject-02-protocol-B-day-2020-12-05.csv], you would specify the naming convention as: subject-{subject ID}-protocol-{protocol ID}-day-{date of measurment in YYYY-MM-DD}). If you are using the DSC Packaging application, you can use the Apply Name Convention button above the form to validate your name convention format and use a valid file name convention to generate a minimal 'Resource File Description' that is a minimal description specific to each file in the multi-file resource set. description.file (string) : For a multi-file resource containing multiple files of the same type (multiple 'like' files), a description specific to the specific current file that is a component of that multi-file set. description.row (string) : For a tabular data resource, a description of what one row in the tabular data resource represents; e.g. one row represents one subject at one timepoint. category.sub.metadata (string) : Sub-category for a metadata resource. Must be one of: [\"\", \"heal-formatted-data-dictionary\", \"other-formatted-data-dictionary\", \"protocol\", \"id-map\", \"analysis-plan\", \"heal-formatted-results-tracker\", \"heal-formatted-experiment-tracker\"] . category.sub.data (string) : Sub-category for a data resource. Must be one of: [\"\", \"raw\", \"processed-intermediate\", \"processed-final\"] . category.sub.results (string) : Sub-category for a results resource. Must be one of: [\"\", \"figure\", \"table\", \"text\", \"draft-publication\", \"publication\", \"report\", \"white-paper\", \"presentation\", \"poster\"] . assoc.file.dd (array) : For a tabular data file resources, a reference/file path to associated data dictionary file(s) - preferably in heal csv data dictionary format. Items (string) assoc.file.protocol (array) : For a data file resource, a reference/file path to associated protocol file(s). Items (string) assoc.file.result.tracker (array) : For a manuscript or report, a reference/file path to associated HEAL results tracker file - HEAL results tracker is a file that tracks each result in a publication, report or poster, along with the data and other supporting files that underly/support each result. If you are using the DSC Packaging Desktop Application, you can head to the Result Tracker tab of the application to create a HEAL formatted result tracker for your published paper or report. Items (string) assoc.file.depends.on (array) : For all resource files, if the current resource file has dependencies/if other files are necessary to make this file (e.g. raw data file necessary to make processed data file), or to interpret/understand this file (e.g. protocol, analysis plan, etc.), list them here; if documenting resources wholistically (i.e. documenting all resources related to a study), only list dependencies one layer deep; if documenting resources minimally (i.e. only documenting resources that will be publicly shared), list dependencies liberally; dependencies can be data, code, protocol, etc.; if already listed under assoc.file.dd, assoc.file.protocol, or assoc.file.id.map no need to repeat here. Items (string) assoc.file.result.depends.on (array) : if the current resource file is a heal formatted result tracker (this tracks the single results in a publication or report), use this field to list each result in the tracker along with its corresponding dependencies (i.e. files the result depends on, or are necessary to make/reach/interpret the result); if documenting resources wholistically (i.e. documenting all resources related to a study), only list dependencies one layer deep; if documenting resources minimally (i.e. only documenting resources that will be publicly shared), list dependencies liberally; dependencies can be data, code, protocol, etc. Items (object) result.id (string) result.id.depends.on (array) assoc.file.multi.like.file (array) : if the current resource file is annotating a resource that is one of multiple 'like' files, this field will list all files that are part of the resources. Items (string) access (array) : What is the current/final access level anticipated for this resource? Options are permanent-private (current and final access level is private), temporary-private (current access level is private but final access level will be either restricted-access or public), restricted-access (current, final, or current AND final access level will allow request of data with barriers/restrictions to access), public (current, final, or current AND final access level will allow largely unrestricted request of/access to data); Many investigators will designate data as currently temporary-private, with a final access level of either restricted-access or public: In this case choose both temporary-private AND either 1) restricted-access or 2) public, then add the date at which you expect to transition from temporary-private to either restricted-access or public in the Access Date field below; Private means members of the public cannot request access; Restricted access means they can request access but there is gate-keeping; Public access means they can often access the data without requesting access, and with minimal barriers to access. Items (string) : Must be one of: [\"\", \"permanent-private\", \"temporary-private\", \"restricted-access\", \"public\"] . access.date (string) : If the resource file is currently being held as temporary-private access level and will transition to either restricted-access or public access level at some point, please provide an anticipated date at which this transition will occur - Best guesses are appreciated, however you will NOT be held to this date and may update this date at any time. format (string) : auto inferred; e.g. csv. format.software (string) : If the file format of the resource file is proprietary and requires specific software to open/interpret, provide the software name and version used by the study group to produce/work with the file; e.g. Origin 11.0, CorelDraw 5.6. profile (string) : auto inferred; e.g. tabular-data-resource. mediatype (string) : auto inferred; e.g. text/csv. encoding (string) : auto inferred; e.g. utf-8. schema (string) : auto inferred; for tabular resource, schema of fields contained in tabular resource; might replace this with ref to either heal csv dd or heal json dd. resource.create.date.time (string) : Date time of resource creation; auto-inferred. resource.mod.date.time (string) : Date time at which the resource was last modified; auto-inferred. restrk.create.date.time (string) : Date time at which the resource tracker file for the resource was created; auto-inferred. restrk.mod.date.time (string) : Date time at which the resource tracker file for the resource was last modified; auto-inferred.","title":"Properties"},{"location":"schemas/md_results_tracker/","text":"HEAL Results Tracker \u00b6 HEAL DSC Core Metadata piece to track and provide basic information about results statements or figures in a publication or report that presents results from your HEAL study. Objective is to list at least all results that have been/will be published in order to describe each result, the data/non-data files each result depends on, and how to use these data/non-data files to reproduce published results. Properties \u00b6 result.id (string) : Unique ID assigned to each result; If using the DSC Packaging application to annotate your resource(s), these IDs will be auto-assigned when you use the Add Result Tracker button above the form to add your Result Tracker Directory. Auto-assignment of IDs occurs by searching the directory for any result annotation files already saved, identifying the result ID with the highest result ID number, and adding 1 to that number to get the result ID number and unique result ID for the current result. description (string) : A description of your result. For figure results this may be the figure caption. For text results, it is recommended that this text be identical or very similar to the text of result as shared in text of the published paper or report or provided as part of the data package. category (string) : Broad category your result falls into; Generally, these categories are: figure, or text. Must be one of: [\"\", \"figure\", \"table\", \"text\"] . assoc.multi.result.file (array) : The publication or report in which this result has been shared. Items (string) figure.number (array) : If the result is a figure result, provide the number of the figure as it appears in the corresponding publication or report; Examples include '1' if the result is in figure 1, or '1a' if the result is in figure 1A. If the result is included in more than one publication or report, use the Associated Publication(s) field above to specify all publications or reports in which the result appears, and add the figure number at which the result appears in each of those files in this field, using the same order (e.g. file-1, file-2; figure-number-in-file-1, figure-number-in-file-2). Items (string) table.number (array) : If the result is a table result, provide the number of the table as it appears in the corresponding publication or report; Examples include '1' if the result is in table 1, or '1a' if the result is in table 1A. If the result is included in more than one publication or report, use the Associated Publication(s) field above to specify all publications or reports in which the result appears, and add the table number at which the result appears in each of those files in this field, using the same order (e.g. file-1, file-2; table-number-in-file-1, table-number-in-file-2). Items (string) assoc.file.depends.on (array) : Data and/or non-data supporting files the result depends upon (e.g. data, analysis plan/code, etc.). If you are using the DSC Packaging App and have many result dependencies to add, you can use the Add Multiple Result Dependencies button above the form to reveal an interface where you can drag and drop many files at once. If documenting resources wholistically (i.e. documenting all resources related to a study), only list dependencies one layer deep; if documenting resources minimally (i.e. only documenting resources that will be publicly shared), list dependencies liberally; dependencies can be data, code, protocol, etc. Items (string) result.supports (array) : Describe a larger claim(s) that this result is used to support in text that is published or provided as part of the data package. Items (string) restrk.create.date.time (string) : Date time at which the result annotation file for the result was created; auto-inferred. restrk.mod.date.time (string) : Date time at which the result annotation file for the result was last modified; auto-inferred.","title":"Results Tracker"},{"location":"schemas/md_results_tracker/#heal-results-tracker","text":"HEAL DSC Core Metadata piece to track and provide basic information about results statements or figures in a publication or report that presents results from your HEAL study. Objective is to list at least all results that have been/will be published in order to describe each result, the data/non-data files each result depends on, and how to use these data/non-data files to reproduce published results.","title":"HEAL Results Tracker"},{"location":"schemas/md_results_tracker/#properties","text":"result.id (string) : Unique ID assigned to each result; If using the DSC Packaging application to annotate your resource(s), these IDs will be auto-assigned when you use the Add Result Tracker button above the form to add your Result Tracker Directory. Auto-assignment of IDs occurs by searching the directory for any result annotation files already saved, identifying the result ID with the highest result ID number, and adding 1 to that number to get the result ID number and unique result ID for the current result. description (string) : A description of your result. For figure results this may be the figure caption. For text results, it is recommended that this text be identical or very similar to the text of result as shared in text of the published paper or report or provided as part of the data package. category (string) : Broad category your result falls into; Generally, these categories are: figure, or text. Must be one of: [\"\", \"figure\", \"table\", \"text\"] . assoc.multi.result.file (array) : The publication or report in which this result has been shared. Items (string) figure.number (array) : If the result is a figure result, provide the number of the figure as it appears in the corresponding publication or report; Examples include '1' if the result is in figure 1, or '1a' if the result is in figure 1A. If the result is included in more than one publication or report, use the Associated Publication(s) field above to specify all publications or reports in which the result appears, and add the figure number at which the result appears in each of those files in this field, using the same order (e.g. file-1, file-2; figure-number-in-file-1, figure-number-in-file-2). Items (string) table.number (array) : If the result is a table result, provide the number of the table as it appears in the corresponding publication or report; Examples include '1' if the result is in table 1, or '1a' if the result is in table 1A. If the result is included in more than one publication or report, use the Associated Publication(s) field above to specify all publications or reports in which the result appears, and add the table number at which the result appears in each of those files in this field, using the same order (e.g. file-1, file-2; table-number-in-file-1, table-number-in-file-2). Items (string) assoc.file.depends.on (array) : Data and/or non-data supporting files the result depends upon (e.g. data, analysis plan/code, etc.). If you are using the DSC Packaging App and have many result dependencies to add, you can use the Add Multiple Result Dependencies button above the form to reveal an interface where you can drag and drop many files at once. If documenting resources wholistically (i.e. documenting all resources related to a study), only list dependencies one layer deep; if documenting resources minimally (i.e. only documenting resources that will be publicly shared), list dependencies liberally; dependencies can be data, code, protocol, etc. Items (string) result.supports (array) : Describe a larger claim(s) that this result is used to support in text that is published or provided as part of the data package. Items (string) restrk.create.date.time (string) : Date time at which the result annotation file for the result was created; auto-inferred. restrk.mod.date.time (string) : Date time at which the result annotation file for the result was last modified; auto-inferred.","title":"Properties"},{"location":"schemas/study-metadata-schema-for-humans/","text":"HEAL Study Overview \u00b6 HEAL DSC Core Metadata piece to provide an overview and enable basic discoverability of your HEAL study, especially when your study is registered and indexed on the HEAL Data Platform where powerful search and discovery tools leverage this core metadata to make your study easily findable. There are general sections, and HEAL-specific sections. HEAL-specific sections have been developed via test use-cases, and are targeted to facilitate surfacing of your study when relevant to a wide variety of stakeholders intensely interested in questions at the heart of HEAL\u2019s mission to concretely address the many facets of the pain and opioid crises and speed solutions that work for affected individuals and communities. Note that if the study has more than one aim, a single form should be completed and the multi-select answer options should be used to provide information that reflects the whole study, across all aims. NOTE: There is no CSV template for the Study Overview. Currently, you will complete your Study Overview using a simple interactive web-form, which you will be able to access once you have registered your study on the HEAL Data Platform . Head to the following links for instructions on how to register your study, and then to submit your Study Overview form Register your study on the HEAL Data Platform Submit your Study Overview form Properties \u00b6 minimal_info (object) study_name (string) : The title or name of the study. For NIH-funded studies, this will generally be equivalent to the NIH application ID title. study_description (string) : A description of or abstract for the study. For NIH-funded studies , this will generally be equivalent to the NIH application ID abstract text. alternative_study_name (string) : Study nickname, alternative title, abbreviation, or acronym (e.g. many people shorten the name of the HEAL Adolescent Brain Cognitive Development study to ABCD). alternative_study_description (string) : An alternative description of or abstract for the study. - Generally, for studies with an NIH appl id, if this field is filled out, the text will be searchable on the Platform, but the NIH appl id abstract text will be the study description displayed for the study in the Platform study table entry. metadata_location (object) nih_application_id (string) : NIH application ID; only applicable if study is funded by NIH. nih_reporter_link (string) : URL link to the NIH application ID NIH RePORTER webpage; only applicable if study is funded by NIH. Refer to #/definitions/saneUrl . clinical_trials_study_ID (string) : ClinicalTrials.gov study ID; only applicable if study is a reportable clinical trial and registered on ClinicalTrials.gov. data_repositories (array) : Describe the data repositories to which data or other shareable products produced by the study will be submitted by the study authors and stored for long-term access; one item in this array per repository. Items (object) repository_name (string) : Name of a repository in which data or other shareable research products are submitted for storage by study author. repository_study_ID (string) : Unique identifier assigned to the study at that repository; usually a number or combination of standard letters and numbers (e.g. study identifiers on ClinicalTrials.gov are generally in the format: NCT12345678). repository_persistent_ID (string) : Unique persistent identifier assigned to the study at that repository; usually a doi. repository_citation (string) : The official citation the repository requests be used to cite the study/data when the study/data is discovered/accessed via the repository; will likely follow the format: Principal Investigator(s). Title. Place-of-Distribution and Distributor, Date-of-Distribution. DOI. version (where distributor will be the repository name). cedar_study_level_metadata_template_instance_ID (string) : ID of the CEDAR HEAL Study-level Core Metadata Template instance created for this study. other_study_websites (array) : any other websites officially associated with this study that provide additional information about the study. Items (string) : Refer to #/definitions/saneUrl . citation (object) heal_funded_status (boolean) : Whether or not the study is funded by the NIH HEAL initiative. study_collection_status (boolean) : Whether or not the study is related to a study group or collection(s) by some administrative mechanism, or belongs to a group or collection of studies (e.g. SAMHSA performs a survey called the National Survey of Substance Abuse Treatment Services (N-SSATS) every year; each annual survey may be registered as a separate study that belongs to a collection called the N-SSATS collection). study_collections (array) : If this study belongs to a study group or collection(s), this is the name or identifier of the study group or collection(s) (e.g. SAMHSA performs a survey called the National Survey of Substance Abuse Treatment Services (N-SSATS) every year; each annual survey may be registered as a separate study that belongs to a collection called the N-SSATS collection). Items (string) funding (array) : Describe the grants and other funding supporting the study; one item in this array per grant/award or funding source. Items (object) funder_name (array) : Name of a the granting agency or organization funding the study; include sub-agency administrative entity as second element in array if applicable (e.g. National Institute of Health, National Institute on Drug Abuse). Items (string) funder_abbreviation (array) : Abbreviation for the name of the granting agency or organization funding the study; include abbreviation for sub-agency administrative entity as second element in array if applicable (e.g. NIH, NIDA). Items (string) funder_type (string) : Type of granting agency or organization funding the study. Must be one of: ['governmental', 'non-governmental, non-profit, not corporate affiliated', 'non-governmental, non-profit, corporate affiliated', 'non-governmental, for-profit'] . funder_geographic_reach (string) : The geographic reach of the granting agency or organization funding the study. Must be one of: ['international', 'national - non-US', 'national - US', 'state - US', 'local - US'] . funding_award_ID (string) : The grant award ID or number. For NIH-funded studies this will be the project or award number and is distinct from the NIH application ID. funding_award_name (string) : The grant award name. For NIH-funded studies this will be the project or award name and is distinct from the name or title associated with the NIH application ID. investigators (array) : Describe the primary and co-investigators of the study; one item in this array per investigator. Items (object) investigator_first_name (string) : First name of study primary or co-investigator. investigator_middle_initial (string) : Middle initial of study primary or co-investigator. investigator_last_name (string) : Last name of study primary or co-investigator. investigator_affiliation (string) : Institutional affiliation of study primary or co-investigator. investigator_ID (array) : Add a structured identifier(s) for the investigator; one item in this array per structured identifier; e.g. one item for providing ORCID, another for providing RAS passport. Items (object) investigator_ID_type (string) : Type of identifier that will be provided. Must be one of: ['internal NIH RePORTER ID', 'doi', 'ORCID', 'eRA Commons ID', 'RAS Passport'] . investigator_ID_value (string) : Value of the identifier of the type specified by ID_type. heal_platform_persistent_ID (string) : Persistent identifier assigned to the study on the HEAL Platform; probably a HEAL Platform-branded doi. heal_platform_citation (string) : The official citation the HEAL Platform will request be used to cite the study/data when the study/data is discovered/accessed via the Platform; will likely follow the format: Principal Investigator(s). Title. Place-of-Distribution and Distributor, Date-of-Distribution. DOI. version (where distributor will be: Platform via [Repository Name]). contacts_and_registrants (object) contacts (array) : Describe the contact person(s) for the study. This is the person(s) who should be contacted for questions about the study; will be auto-set as NIH contact PI(s) if NIH-funded; one item in this array per contact person. Items (object) contact_first_name (string) : First name of study contact. contact_middle_initial (string) : Middle initial of study contact. contact_last_name (string) : Last name of study contact. contact_affiliation (string) : Institutional affiliation of study contact. contact_email (string) : Institutional email of study contact. registrants (array) : Describe the person(s) who will register the study on the HEAL Platform. This person(s) must be authorized to access the study registration page/process on the HEAL Platform; will be auto-set as NIH contact PI(s) if NIH-funded; one item in this array per registrant. Items (object) registrant_first_name (string) : First name of study registrant. registrant_middle_initial (string) : Middle initial of study registrant. registrant_last_name (string) : Last name of study registrant. registrant_affiliation (string) : Institutional affiliation of study registrant. registrant_email (string) : Institutional email of study registrant. data_availability (object) produce_data (boolean) : Indicate whether or not the study will collect/produce (primary or secondary) data. data_available (string) : If study will collect/produce data, indicate whether all, some, or none of the data will be made available. Must be one of: ['all', 'some', 'none'] . data_restricted (string) : If study will collect/produce data, and make at least some of that data available, indicate whether all, some, or none of the data will have restriction(s) on access beyond acknowledgement and signing of a minimal DSA. Must be one of: ['all', 'some', 'none'] . data_collection_status (string) : If study will collect/produce data, indicate whether the study has not started, has started, or has finished data collection/production activities. Must be one of: ['not started', 'started', 'finished'] . data_release_status (string) : If the study will collect/produce data and make at least some of the data available, indicate whether the study has not started, has started, or has finished data release activities. Must be one of: ['not started', 'started', 'finished'] . data_collection_start_date (string) : If the study will collect/produce data, indicate the anticipated date when data collection/production will begin. data_collection_finish_date (string) : If the study will collect/produce data, indicate the anticipated date when data collection/production will end (data collection/production is complete). data_release_start_date (string) : If study will collect/produce data and make at least some of the data available, indicate the anticipated date when first data will be released. data_release_finish_date (string) : If study will produce data and make at least some of the data available, indicate the anticipated date when last data will be released (data release is complete). produce_other (boolean) : Indicate whether or not the study will produce shareable products other than data. findings (object) primary_publications (array) : List of the doi for any/all primary study publications that use the study data; include any papers, books, articles, blog posts, etc. authored by the investigators or team responsible for the study; one item in this array per publication/doi. Items (string) primary_study_findings (array) : Simple, one-sentence take-away conclusion(s) from study author publications; one item in this array per single-sentence conclusion (e.g. item #1: Provision of MAT rx to men with documented OUD leaving jail reduces overdose risk by 9%; item #2: Provision of MAT rx plus case management support to men with documented OUD leaving jail reduces overdose risk by 30%; item 3: Provision of MAT rx plus case management support to men with documented OUD leaving jail is more effective than provision of MAT rx alone at reducing risk of OUD overdose). Items (string) secondary_publications (array) : List of the doi for any/all secondary study publications that use the study data; include any papers, books, articles, blog posts, etc. authored by persons OTHER THAN the investigators or team responsible for the study; one item in this array per publication/doi. Items (string) study_translational_focus (object) study_translational_focus (string) : Whether the study is primarily focused on learning more about a human opioid or pain CONDITION or on discovering, or learning more about a TREATMENT OF A CONDITION, intervention, or solution for a human opioid or pain condition. This may be quite simple to infer for human subject studies. For studies not involving human subjects, please consider how/where the study will most substantially contribute to the HEAL Initiative translational purpose of learning more about human opioid and pain conditions and driving development of solutions to these conditions. Note there are some studies that are not related to a specific condition or the treatment of a condition. An example of a study like this is the National Institute on Drug Abuse (NIDA) study Incarceration Effects on Medicaid Status. If a study like this does not apply to a condition or treatment, this question can be left blank.Choose one. Must be one of: ['Condition', 'Treatment of a Condition'] . study_translational_topic_grouping (array) : What types of determinants and/or mechanisms related to human opioid use or pain conditions is the study investigating and/or measuring (e.g. A study investigating differential pain/nociception signaling in persons living in neighborhoods with high versus low social cohesion might choose both 'Biology and Health' and 'Social Determinants'). Choose as many as apply. Items (string) : Must be one of: ['Biology and Health', 'Mental Health', 'Social Determinants', 'Public Attitudes or Stigma'] . study_type (object) study_stage (array) : The high-level category or type/stage of research being conducted. Items (string) : Must be one of: ['Pre-Research/Protocol Development', 'Basic Research', 'Pre-Clinical Research', 'Clinical Research', 'Implementation Research', 'Post-market Research', 'Business Development', 'Epidemiologic Research'] . study_primary_or_secondary (string) : Is the study collecting data itself to analyze and draw conclusions or is it leveraging data collected by other entities to analyze and draw conclusions. Must be one of: ['Primary Research', 'Secondary Research'] . study_observational_or_experimental (string) : Whether the study is observational (no explicit intervention) or experimental (some explicit intervention). Must be one of: ['Observational Research', 'Experimental Research'] . study_subject_type (array) : What is the actual species or entity that is experimentally or observationally studied in the research; each HEAL study will have translational relevance to humans, but not all HEAL studies directly study humans. Items (string) : Must be one of: ['Human', 'Animal', 'Human cell/tissue/tissue model', 'Animal cell/tissue/tissue model', 'Molecule (e.g. chemical/protein engineering, protein crystallization)'] . study_type_design (array) : The overarching study type and more granular study design. Choose as many as apply. Items (string) : Must be one of: ['Basic Research - Theory/Modeling', 'Basic Research - Applied', 'Basic Research - Method/Protocol development', 'Basic Research - Animal study', 'Basic Research - Cell study', 'Basic Research - Genetic study', 'Basic Research - Biochemistry study', 'Basic Research - Biophysical study', 'Basic Research - Gene sequencing', 'Basic Research - Genetic Engineering', 'Basic Research - Material Development', 'Basic Research - Device Development', 'Basic Research - Small molecule screen', 'Basic Research - High throughput screen', 'Basic Research - Screen, other', 'Clinical Research - Phase I', 'Clinical Research - Phase II', 'Clinical Research - Phase III', 'Clinical Research - Phase IV', 'Clinical Research - Therapy/drug study without intervention', 'Clinical Research - Case Report or Series', 'Preclinical Research', 'Randomized Control Trial', 'Pre/Post', 'Stepped Wedge', 'Regression Discontinuity', 'Difference in Differences', 'Cross-over', 'Natural Experiment', 'Cohort, prospective', 'Cohort, retrospective', 'Cohort - panel, longitudinal', 'Cohort - cross-sectional, longitudinal', 'Cohort - cross-sectional, single time-point', 'Case control', 'Ecological', 'Monitoring/surveillance', 'Inventory', 'Curation', 'Meta-analysis', 'Review', 'Review, Systematic', 'Review, Simple (narrative)'] . human_treatment_applicability (object) treatment_investigation_stage_or_type (array) : If the study is investigating a treatment, intervention, or solution to a human opioid or pain condition, what type of investigation is the study undertaking? at what stage in the treatment investigation process does this study lie? Items (string) : Must be one of: ['target discovery', 'target mechanism', 'treatment discovery', 'treatment mechanism', 'treatment efficacy', 'differential treatment efficacy', 'treatment implementation', 'treatment availability or accessibility'] . treatment_mode (array) : Is the treatment, intervention, or solution that my study is relevant to meant to prevent the occurrance of a condition (e.g. development of a non-opioid drug for pain to prevent opioid exposure and addiction, case management and MAT for pregnant women to prevent in utero exposure to opioids, safe needle exchange center to prevent spread of blood borne diseases among people using heroin), treat an occurrance of a condition (e.g. naloxone to treat opioid overdose, physical therapy to treat existing pain from a back injury) , and/or is it a harm reduction mechanism (e.g. safe needle exchange reduces harm associated with heroin addiction/use, decriminalization of heroin possession or use reduces the harm associated with heroin addiction/use). Items (string) : Must be one of: ['Preventive (prevention of an occurrence)', 'Therapeutic (treating an occurrence)', 'Harm Reduction (preventing more of an occurrence, or preventing harm from an occurrence)'] . treatment_novelty (array) : Is the treatment, intervention, or solution that my study is relevant to totally novel, is it totally established, does it have some element of novelty added to an established intervention (novel added to established, established used in a novel population, setting, or combination)? . Items (string) : Must be one of: ['Novel', 'Novel, added to established', 'Established', 'Established, used in novel population, setting, or combination'] . treatment_application_level (array) : Is the treatment, intervention, or solution that my study is relevant to applied at the individual level (e.g. a doctor prescribes a drug or physical therapy regimen to a patient) or at the population/community/group level (e.g. a city changes good samaritan naloxone laws, or implements a safe needle exchange center, or adds park maintenance resources to the budget to provide more access to outside places to exercise)? Items (string) : Must be one of: ['Individual', 'Population'] . treatment_type (array) : Classify the treatment, intervention, or solution the study is relevant to by broad type of intervention. Choose as many as apply. Items (string) : Must be one of: ['drug', 'non-opioid drug', 'opioid drug', 'non-biologic drug', 'biologic drug', 'non-drug', 'device', 'behavioral', 'psychotherapy', 'group therapy', 'physical therapy', 'exercise', 'education', 'community resources/supports', 'economic supports', 'housing supports', 'case management', 'environmental (indoors)', 'environmental (outdoors)', 'law/policy', 'surgical procedure', 'nutritional', 'nutritional supplement', 'gene therapy', 'immunotherapy', 'electrotherapy'] . human_condition_applicability (object) condition_category (array) : Considering the study in a translational context, to humans with which broad category(ies) of opioid use or pain conditions are the study results intended to apply. Choose as many as apply. Items (string) : Must be one of: ['Opioid use and opioid use disorder', 'Opioid use and opioid use disorder, primary occurrence', 'Opioid use and opioid use disorder, relapse', 'Opioid use and opioid use disorder, chronic', 'Opioid overdose', 'Opioid exposure', '(Non-opioid) Substance use', 'Co-occurring (Opioid or Non-opioid) Substance use and Mental Health disorder', 'Pain', 'Pain, acute', 'Pain, chronic', 'Pain, acute to chronic transition', 'Pain, resulting from injury', 'Pain, resulting from surgery', 'Pain, resulting from chronic illness', 'Pain, idiopathic (unknown origin)'] . condition_investigation_stage_or_type (array) : Each HEAL study will be doing work that is relevant to a human opioid and/or pain condition. What does this study intend to learn about the condition? If the study is investigating a treatment, intervention, or solution, please choose 'Treatment of Condition'. Choose as many as apply. Items (string) : Must be one of: ['Incidence of condition', 'Risk for condition', 'Differential risk for condition', 'Mechanism of condition', 'Impact of condition', 'Public attitudes towards or perception of condition', 'Treatment of condition'] . pain_causal_condition (array) : If the study will be doing work that is relevant to a human PAIN condition category, is the study focusing on pain caused by a specific condition? e.g. Cancer, fibromyalgia, arthritis, occupational injury, sports injury, car accident, surgical procedure. If yes, please indicate the causal condition(s) here - Values will be restricted to NLM MeSH Diseases - see Branch 'C' here: https://meshb-prev.nlm.nih.gov/treeView. Items (string) pain_treatment_or_study_target_condition (array) : If the study will be doing work that is relevant to a human PAIN condition category, and the pain condition has a causal condition (specified above), is the study focusing its investigation on the causal condition, or on the pain resulting from the causal condition? e.g. In a study of effective treatments for fibromyalgia/fibromyalgia pain, is the study focused on fibromyalgia disease-modifying treatments, or on treatments that address the symptomology of pain? Choose as many as apply. Items (string) : Must be one of: ['Causal Condition', 'Pain', 'Pain, acute', 'Pain, chronic'] . all_treatment_or_study_target_condition (array) : Considering the study in a translational context, to humans with which detailed type of opioid use or pain conditions are the study results intended to apply? Each HEAL study will be doing work that is relevant to a human opioid and/or pain condition; You indicated the broad category of condition(s) the study is focusing on or relevant to above. Here, please indicate the study focus condition(s) in detail. Choose as many as apply - Values will be restricted to NLM MeSH Diseases - see Branch 'C' here: https://meshb-prev.nlm.nih.gov/treeView. Items (string) all_outcome_condition (array) : If the outcome condition(s) the study hopes to impact or measure is different than the treatment or study target condition (e.g. testing a disease-modifying fibromyaligia treatment but measuring depression, physical and social function, and neurologic pain such as headaches as the main outcomes). Please indicate the study outcome condition(s) here. Choose as many as apply - Values will be restricted to NLM MeSH Diseases - see Branch 'C' here: https://meshb-prev.nlm.nih.gov/treeView. Items (string) all_other_condition (array) : If the study is measuring or tracking other human health conditions besides the treatment or study target condition and outcome condition, please indicate the other tracked/measured condition(s) here. Choose as many as apply - Values will be restricted to NLM MeSH Diseases - see Branch 'C' here: https://meshb-prev.nlm.nih.gov/treeView. Items (string) human_subject_applicability (object) gender_applicability (array) : Considering the study in a translational context, to humans of which gender identity(ies) are the study results intended to apply and/or likely to generalize to. Items (string) : Must be one of: ['Female', 'Female-to-male transsexual', 'Intersexed', 'Male', 'Male-to-female transsexual', 'Other'] . sexual_identity_applicability (array) : Considering the study in a translational context, to humans of which sexual identity(ies) are the study results intended to apply and/or likely to generalize to. Items (string) : Must be one of: ['Heterosexual', 'Homosexual (gay/lesbian)', 'Bisexual', 'Asexual', 'Other'] . biological_sex_applicability (array) : Considering the study in a translational context, to humans of which biological sexes are the study results intended to apply and/or likely to generalize to. Items (string) : Must be one of: ['Ambiguous', 'Female', 'Male'] . age_applicability (array) : Considering the study in a translational context, to humans of which age or developmental stage are the study results intended to apply and/or likely to generalize to. Items (string) : Must be one of: ['Fetus', 'Infant, newborn (Birth to 1 month)', 'Infant (1 to 23 months)', 'Child, preschool (2 to 5 years)', 'Child (6 to 12 years)', 'Adolescent (13 to 18 years)', 'Adult (19 to 44 years)', 'Middle aged adult (45 to 64 years)', 'Aged adult (65 to 79 years)', 'Aged, 80 and over (80 years and over)'] . irb_vulnerability_conditions_applicability (array) : Considering the study in a translational context, which special IRB human vulnerability conditions are the study results intended to address or apply to? Items (string) : Must be one of: ['Pregnant women', 'Human fetuses', 'Neonates', 'Prisoners', 'Children', 'Individuals with physical disabilities', 'Individuals with mental disabilities or cognitive impairments', 'Economically disadvantaged', 'Socially disadvantaged', 'Terminally ill or very sick', 'Racial or ethnic minorities', 'Institutionalized persons (for example, persons in correctional facilities, nursing homes or mental health facilities)'] . geographic_applicability (array) : For studies with human subjects, from which geographic location(s) are subjects recruited. If subjects are recruited in the US nationally, choose only US - national. If subject are recruited in the US only in specific state(s), choose US - Specific States, then please also select the abbreviation of each of the specific states from which subjects are recruited. If subjects are recruited in the US from specific county(ies) within specific state(s), please choose US - Specific States and US - Specific Counties, and choose the abbreviation of each specific state from which subjects are recruited. Items (string) : Must be one of: ['Non US', 'US - National', 'US - Specific States', 'US - Specific Counties', 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'] . data (object) data_orientation (array) : Does the study produce/collect quantitative data?, qualitative data? Choose as many as apply. Items (string) : Must be one of: ['Quantitative', 'Qualitative'] . data_source (array) : What is the source of the data the study will produce/collect. Choose as many as apply. Items (string) : Must be one of: ['Administrative', 'Government Collected/Measured', 'Clinical/EHR', 'Laboratory', 'Researcher Collected/Measured', 'Patient-Reported'] . data_type (array) : What is the type of the data the study will produce/collect. Choose as many as apply. Items (string) : Must be one of: ['Genomic', 'Proteomic', 'Biochemical', 'Biophysical', 'Imaging', 'Interview/Focus Group', 'Interview/Focus Group - structured', 'Interview/Focus Group - semi-structured', 'Interview/Focus Group - unstructured', 'Questionnaire/Survey/Assessment', 'Questionnaire/Survey/Assessment - validated instrument', 'Questionnaire/Survey/Assessment - unvalidated instrument'] . subject_data_unit_of_collection (array) : For studies with human subjects, is the data collected at the individual or population level? Items (string) : Must be one of: ['Individual', 'Community, Group, Cluster, Population'] . subject_data_unit_of_collection_expected_number (integer) : For studies with human subjects, what is the expected number of subjects for which data will be collected? If unit of data collection is individuals this is the expected number of individuals, and if the unit of data collection is populations/clusters this is the expected number of populations/clusters. subject_data_unit_of_analysis (array) : For studies with human subjects, is the data analyzed at the individual or population level? Items (string) : Must be one of: ['Individual', 'Community, Group, Cluster, Population'] . subject_data_unit_of_analysis_expected_number (integer) : For studies with human subjects, what is the expected number of subjects at the level at which data will be analyzed? If unit of data analysis is individuals this is the expected number of individuals, and if the unit of data analysis is populations/clusters this is the expected number of populations/clusters. subject_data_level_available (array) : For studies with human subjects that will make at least some data available, will data be made available at the individual subject level or some level of aggregation? Items (string) : Must be one of: ['Individual - Unaggregated', 'Individual - Aggregated', 'Community, Group, Cluster, Population - Unaggregated', 'Community, Group, Cluster, Population - Aggregated', 'Unaggregated', 'Aggregated', 'Aggregated, across all (population summary statistics)', 'Aggregated, by treatment/exposure group', 'Aggregated, by demographic variable(s)', 'Aggregated, by geographic variable(s)'] . subject_geographic_data_level_collected (array) : For studies with human subjects, will geographic data be collected? if so, what level of geographic detail will be collected? Items (string) : Must be one of: ['Exact location', 'Census block', 'Census tract', 'Zip code', 'County', 'Sub-state region', 'State', 'Multi-state region', 'Nation', 'No geographic data will be collected'] . subject_geographic_data_level_available (array) : For studies with human subjects that collect geographic data and will make at least some data available, will geographic data be made available? if so, what level of geographic detail will be made available? Items (string) : Must be one of: ['Exact location', 'Census block', 'Census tract', 'Zip code', 'County', 'Sub-state region', 'State', 'Multi-state region', 'Nation', 'No geographic data will be made available'] . Definitions \u00b6 saneUrl","title":"Study Overview"},{"location":"schemas/study-metadata-schema-for-humans/#heal-study-overview","text":"HEAL DSC Core Metadata piece to provide an overview and enable basic discoverability of your HEAL study, especially when your study is registered and indexed on the HEAL Data Platform where powerful search and discovery tools leverage this core metadata to make your study easily findable. There are general sections, and HEAL-specific sections. HEAL-specific sections have been developed via test use-cases, and are targeted to facilitate surfacing of your study when relevant to a wide variety of stakeholders intensely interested in questions at the heart of HEAL\u2019s mission to concretely address the many facets of the pain and opioid crises and speed solutions that work for affected individuals and communities. Note that if the study has more than one aim, a single form should be completed and the multi-select answer options should be used to provide information that reflects the whole study, across all aims. NOTE: There is no CSV template for the Study Overview. Currently, you will complete your Study Overview using a simple interactive web-form, which you will be able to access once you have registered your study on the HEAL Data Platform . Head to the following links for instructions on how to register your study, and then to submit your Study Overview form Register your study on the HEAL Data Platform Submit your Study Overview form","title":"HEAL Study Overview"},{"location":"schemas/study-metadata-schema-for-humans/#properties","text":"minimal_info (object) study_name (string) : The title or name of the study. For NIH-funded studies, this will generally be equivalent to the NIH application ID title. study_description (string) : A description of or abstract for the study. For NIH-funded studies , this will generally be equivalent to the NIH application ID abstract text. alternative_study_name (string) : Study nickname, alternative title, abbreviation, or acronym (e.g. many people shorten the name of the HEAL Adolescent Brain Cognitive Development study to ABCD). alternative_study_description (string) : An alternative description of or abstract for the study. - Generally, for studies with an NIH appl id, if this field is filled out, the text will be searchable on the Platform, but the NIH appl id abstract text will be the study description displayed for the study in the Platform study table entry. metadata_location (object) nih_application_id (string) : NIH application ID; only applicable if study is funded by NIH. nih_reporter_link (string) : URL link to the NIH application ID NIH RePORTER webpage; only applicable if study is funded by NIH. Refer to #/definitions/saneUrl . clinical_trials_study_ID (string) : ClinicalTrials.gov study ID; only applicable if study is a reportable clinical trial and registered on ClinicalTrials.gov. data_repositories (array) : Describe the data repositories to which data or other shareable products produced by the study will be submitted by the study authors and stored for long-term access; one item in this array per repository. Items (object) repository_name (string) : Name of a repository in which data or other shareable research products are submitted for storage by study author. repository_study_ID (string) : Unique identifier assigned to the study at that repository; usually a number or combination of standard letters and numbers (e.g. study identifiers on ClinicalTrials.gov are generally in the format: NCT12345678). repository_persistent_ID (string) : Unique persistent identifier assigned to the study at that repository; usually a doi. repository_citation (string) : The official citation the repository requests be used to cite the study/data when the study/data is discovered/accessed via the repository; will likely follow the format: Principal Investigator(s). Title. Place-of-Distribution and Distributor, Date-of-Distribution. DOI. version (where distributor will be the repository name). cedar_study_level_metadata_template_instance_ID (string) : ID of the CEDAR HEAL Study-level Core Metadata Template instance created for this study. other_study_websites (array) : any other websites officially associated with this study that provide additional information about the study. Items (string) : Refer to #/definitions/saneUrl . citation (object) heal_funded_status (boolean) : Whether or not the study is funded by the NIH HEAL initiative. study_collection_status (boolean) : Whether or not the study is related to a study group or collection(s) by some administrative mechanism, or belongs to a group or collection of studies (e.g. SAMHSA performs a survey called the National Survey of Substance Abuse Treatment Services (N-SSATS) every year; each annual survey may be registered as a separate study that belongs to a collection called the N-SSATS collection). study_collections (array) : If this study belongs to a study group or collection(s), this is the name or identifier of the study group or collection(s) (e.g. SAMHSA performs a survey called the National Survey of Substance Abuse Treatment Services (N-SSATS) every year; each annual survey may be registered as a separate study that belongs to a collection called the N-SSATS collection). Items (string) funding (array) : Describe the grants and other funding supporting the study; one item in this array per grant/award or funding source. Items (object) funder_name (array) : Name of a the granting agency or organization funding the study; include sub-agency administrative entity as second element in array if applicable (e.g. National Institute of Health, National Institute on Drug Abuse). Items (string) funder_abbreviation (array) : Abbreviation for the name of the granting agency or organization funding the study; include abbreviation for sub-agency administrative entity as second element in array if applicable (e.g. NIH, NIDA). Items (string) funder_type (string) : Type of granting agency or organization funding the study. Must be one of: ['governmental', 'non-governmental, non-profit, not corporate affiliated', 'non-governmental, non-profit, corporate affiliated', 'non-governmental, for-profit'] . funder_geographic_reach (string) : The geographic reach of the granting agency or organization funding the study. Must be one of: ['international', 'national - non-US', 'national - US', 'state - US', 'local - US'] . funding_award_ID (string) : The grant award ID or number. For NIH-funded studies this will be the project or award number and is distinct from the NIH application ID. funding_award_name (string) : The grant award name. For NIH-funded studies this will be the project or award name and is distinct from the name or title associated with the NIH application ID. investigators (array) : Describe the primary and co-investigators of the study; one item in this array per investigator. Items (object) investigator_first_name (string) : First name of study primary or co-investigator. investigator_middle_initial (string) : Middle initial of study primary or co-investigator. investigator_last_name (string) : Last name of study primary or co-investigator. investigator_affiliation (string) : Institutional affiliation of study primary or co-investigator. investigator_ID (array) : Add a structured identifier(s) for the investigator; one item in this array per structured identifier; e.g. one item for providing ORCID, another for providing RAS passport. Items (object) investigator_ID_type (string) : Type of identifier that will be provided. Must be one of: ['internal NIH RePORTER ID', 'doi', 'ORCID', 'eRA Commons ID', 'RAS Passport'] . investigator_ID_value (string) : Value of the identifier of the type specified by ID_type. heal_platform_persistent_ID (string) : Persistent identifier assigned to the study on the HEAL Platform; probably a HEAL Platform-branded doi. heal_platform_citation (string) : The official citation the HEAL Platform will request be used to cite the study/data when the study/data is discovered/accessed via the Platform; will likely follow the format: Principal Investigator(s). Title. Place-of-Distribution and Distributor, Date-of-Distribution. DOI. version (where distributor will be: Platform via [Repository Name]). contacts_and_registrants (object) contacts (array) : Describe the contact person(s) for the study. This is the person(s) who should be contacted for questions about the study; will be auto-set as NIH contact PI(s) if NIH-funded; one item in this array per contact person. Items (object) contact_first_name (string) : First name of study contact. contact_middle_initial (string) : Middle initial of study contact. contact_last_name (string) : Last name of study contact. contact_affiliation (string) : Institutional affiliation of study contact. contact_email (string) : Institutional email of study contact. registrants (array) : Describe the person(s) who will register the study on the HEAL Platform. This person(s) must be authorized to access the study registration page/process on the HEAL Platform; will be auto-set as NIH contact PI(s) if NIH-funded; one item in this array per registrant. Items (object) registrant_first_name (string) : First name of study registrant. registrant_middle_initial (string) : Middle initial of study registrant. registrant_last_name (string) : Last name of study registrant. registrant_affiliation (string) : Institutional affiliation of study registrant. registrant_email (string) : Institutional email of study registrant. data_availability (object) produce_data (boolean) : Indicate whether or not the study will collect/produce (primary or secondary) data. data_available (string) : If study will collect/produce data, indicate whether all, some, or none of the data will be made available. Must be one of: ['all', 'some', 'none'] . data_restricted (string) : If study will collect/produce data, and make at least some of that data available, indicate whether all, some, or none of the data will have restriction(s) on access beyond acknowledgement and signing of a minimal DSA. Must be one of: ['all', 'some', 'none'] . data_collection_status (string) : If study will collect/produce data, indicate whether the study has not started, has started, or has finished data collection/production activities. Must be one of: ['not started', 'started', 'finished'] . data_release_status (string) : If the study will collect/produce data and make at least some of the data available, indicate whether the study has not started, has started, or has finished data release activities. Must be one of: ['not started', 'started', 'finished'] . data_collection_start_date (string) : If the study will collect/produce data, indicate the anticipated date when data collection/production will begin. data_collection_finish_date (string) : If the study will collect/produce data, indicate the anticipated date when data collection/production will end (data collection/production is complete). data_release_start_date (string) : If study will collect/produce data and make at least some of the data available, indicate the anticipated date when first data will be released. data_release_finish_date (string) : If study will produce data and make at least some of the data available, indicate the anticipated date when last data will be released (data release is complete). produce_other (boolean) : Indicate whether or not the study will produce shareable products other than data. findings (object) primary_publications (array) : List of the doi for any/all primary study publications that use the study data; include any papers, books, articles, blog posts, etc. authored by the investigators or team responsible for the study; one item in this array per publication/doi. Items (string) primary_study_findings (array) : Simple, one-sentence take-away conclusion(s) from study author publications; one item in this array per single-sentence conclusion (e.g. item #1: Provision of MAT rx to men with documented OUD leaving jail reduces overdose risk by 9%; item #2: Provision of MAT rx plus case management support to men with documented OUD leaving jail reduces overdose risk by 30%; item 3: Provision of MAT rx plus case management support to men with documented OUD leaving jail is more effective than provision of MAT rx alone at reducing risk of OUD overdose). Items (string) secondary_publications (array) : List of the doi for any/all secondary study publications that use the study data; include any papers, books, articles, blog posts, etc. authored by persons OTHER THAN the investigators or team responsible for the study; one item in this array per publication/doi. Items (string) study_translational_focus (object) study_translational_focus (string) : Whether the study is primarily focused on learning more about a human opioid or pain CONDITION or on discovering, or learning more about a TREATMENT OF A CONDITION, intervention, or solution for a human opioid or pain condition. This may be quite simple to infer for human subject studies. For studies not involving human subjects, please consider how/where the study will most substantially contribute to the HEAL Initiative translational purpose of learning more about human opioid and pain conditions and driving development of solutions to these conditions. Note there are some studies that are not related to a specific condition or the treatment of a condition. An example of a study like this is the National Institute on Drug Abuse (NIDA) study Incarceration Effects on Medicaid Status. If a study like this does not apply to a condition or treatment, this question can be left blank.Choose one. Must be one of: ['Condition', 'Treatment of a Condition'] . study_translational_topic_grouping (array) : What types of determinants and/or mechanisms related to human opioid use or pain conditions is the study investigating and/or measuring (e.g. A study investigating differential pain/nociception signaling in persons living in neighborhoods with high versus low social cohesion might choose both 'Biology and Health' and 'Social Determinants'). Choose as many as apply. Items (string) : Must be one of: ['Biology and Health', 'Mental Health', 'Social Determinants', 'Public Attitudes or Stigma'] . study_type (object) study_stage (array) : The high-level category or type/stage of research being conducted. Items (string) : Must be one of: ['Pre-Research/Protocol Development', 'Basic Research', 'Pre-Clinical Research', 'Clinical Research', 'Implementation Research', 'Post-market Research', 'Business Development', 'Epidemiologic Research'] . study_primary_or_secondary (string) : Is the study collecting data itself to analyze and draw conclusions or is it leveraging data collected by other entities to analyze and draw conclusions. Must be one of: ['Primary Research', 'Secondary Research'] . study_observational_or_experimental (string) : Whether the study is observational (no explicit intervention) or experimental (some explicit intervention). Must be one of: ['Observational Research', 'Experimental Research'] . study_subject_type (array) : What is the actual species or entity that is experimentally or observationally studied in the research; each HEAL study will have translational relevance to humans, but not all HEAL studies directly study humans. Items (string) : Must be one of: ['Human', 'Animal', 'Human cell/tissue/tissue model', 'Animal cell/tissue/tissue model', 'Molecule (e.g. chemical/protein engineering, protein crystallization)'] . study_type_design (array) : The overarching study type and more granular study design. Choose as many as apply. Items (string) : Must be one of: ['Basic Research - Theory/Modeling', 'Basic Research - Applied', 'Basic Research - Method/Protocol development', 'Basic Research - Animal study', 'Basic Research - Cell study', 'Basic Research - Genetic study', 'Basic Research - Biochemistry study', 'Basic Research - Biophysical study', 'Basic Research - Gene sequencing', 'Basic Research - Genetic Engineering', 'Basic Research - Material Development', 'Basic Research - Device Development', 'Basic Research - Small molecule screen', 'Basic Research - High throughput screen', 'Basic Research - Screen, other', 'Clinical Research - Phase I', 'Clinical Research - Phase II', 'Clinical Research - Phase III', 'Clinical Research - Phase IV', 'Clinical Research - Therapy/drug study without intervention', 'Clinical Research - Case Report or Series', 'Preclinical Research', 'Randomized Control Trial', 'Pre/Post', 'Stepped Wedge', 'Regression Discontinuity', 'Difference in Differences', 'Cross-over', 'Natural Experiment', 'Cohort, prospective', 'Cohort, retrospective', 'Cohort - panel, longitudinal', 'Cohort - cross-sectional, longitudinal', 'Cohort - cross-sectional, single time-point', 'Case control', 'Ecological', 'Monitoring/surveillance', 'Inventory', 'Curation', 'Meta-analysis', 'Review', 'Review, Systematic', 'Review, Simple (narrative)'] . human_treatment_applicability (object) treatment_investigation_stage_or_type (array) : If the study is investigating a treatment, intervention, or solution to a human opioid or pain condition, what type of investigation is the study undertaking? at what stage in the treatment investigation process does this study lie? Items (string) : Must be one of: ['target discovery', 'target mechanism', 'treatment discovery', 'treatment mechanism', 'treatment efficacy', 'differential treatment efficacy', 'treatment implementation', 'treatment availability or accessibility'] . treatment_mode (array) : Is the treatment, intervention, or solution that my study is relevant to meant to prevent the occurrance of a condition (e.g. development of a non-opioid drug for pain to prevent opioid exposure and addiction, case management and MAT for pregnant women to prevent in utero exposure to opioids, safe needle exchange center to prevent spread of blood borne diseases among people using heroin), treat an occurrance of a condition (e.g. naloxone to treat opioid overdose, physical therapy to treat existing pain from a back injury) , and/or is it a harm reduction mechanism (e.g. safe needle exchange reduces harm associated with heroin addiction/use, decriminalization of heroin possession or use reduces the harm associated with heroin addiction/use). Items (string) : Must be one of: ['Preventive (prevention of an occurrence)', 'Therapeutic (treating an occurrence)', 'Harm Reduction (preventing more of an occurrence, or preventing harm from an occurrence)'] . treatment_novelty (array) : Is the treatment, intervention, or solution that my study is relevant to totally novel, is it totally established, does it have some element of novelty added to an established intervention (novel added to established, established used in a novel population, setting, or combination)? . Items (string) : Must be one of: ['Novel', 'Novel, added to established', 'Established', 'Established, used in novel population, setting, or combination'] . treatment_application_level (array) : Is the treatment, intervention, or solution that my study is relevant to applied at the individual level (e.g. a doctor prescribes a drug or physical therapy regimen to a patient) or at the population/community/group level (e.g. a city changes good samaritan naloxone laws, or implements a safe needle exchange center, or adds park maintenance resources to the budget to provide more access to outside places to exercise)? Items (string) : Must be one of: ['Individual', 'Population'] . treatment_type (array) : Classify the treatment, intervention, or solution the study is relevant to by broad type of intervention. Choose as many as apply. Items (string) : Must be one of: ['drug', 'non-opioid drug', 'opioid drug', 'non-biologic drug', 'biologic drug', 'non-drug', 'device', 'behavioral', 'psychotherapy', 'group therapy', 'physical therapy', 'exercise', 'education', 'community resources/supports', 'economic supports', 'housing supports', 'case management', 'environmental (indoors)', 'environmental (outdoors)', 'law/policy', 'surgical procedure', 'nutritional', 'nutritional supplement', 'gene therapy', 'immunotherapy', 'electrotherapy'] . human_condition_applicability (object) condition_category (array) : Considering the study in a translational context, to humans with which broad category(ies) of opioid use or pain conditions are the study results intended to apply. Choose as many as apply. Items (string) : Must be one of: ['Opioid use and opioid use disorder', 'Opioid use and opioid use disorder, primary occurrence', 'Opioid use and opioid use disorder, relapse', 'Opioid use and opioid use disorder, chronic', 'Opioid overdose', 'Opioid exposure', '(Non-opioid) Substance use', 'Co-occurring (Opioid or Non-opioid) Substance use and Mental Health disorder', 'Pain', 'Pain, acute', 'Pain, chronic', 'Pain, acute to chronic transition', 'Pain, resulting from injury', 'Pain, resulting from surgery', 'Pain, resulting from chronic illness', 'Pain, idiopathic (unknown origin)'] . condition_investigation_stage_or_type (array) : Each HEAL study will be doing work that is relevant to a human opioid and/or pain condition. What does this study intend to learn about the condition? If the study is investigating a treatment, intervention, or solution, please choose 'Treatment of Condition'. Choose as many as apply. Items (string) : Must be one of: ['Incidence of condition', 'Risk for condition', 'Differential risk for condition', 'Mechanism of condition', 'Impact of condition', 'Public attitudes towards or perception of condition', 'Treatment of condition'] . pain_causal_condition (array) : If the study will be doing work that is relevant to a human PAIN condition category, is the study focusing on pain caused by a specific condition? e.g. Cancer, fibromyalgia, arthritis, occupational injury, sports injury, car accident, surgical procedure. If yes, please indicate the causal condition(s) here - Values will be restricted to NLM MeSH Diseases - see Branch 'C' here: https://meshb-prev.nlm.nih.gov/treeView. Items (string) pain_treatment_or_study_target_condition (array) : If the study will be doing work that is relevant to a human PAIN condition category, and the pain condition has a causal condition (specified above), is the study focusing its investigation on the causal condition, or on the pain resulting from the causal condition? e.g. In a study of effective treatments for fibromyalgia/fibromyalgia pain, is the study focused on fibromyalgia disease-modifying treatments, or on treatments that address the symptomology of pain? Choose as many as apply. Items (string) : Must be one of: ['Causal Condition', 'Pain', 'Pain, acute', 'Pain, chronic'] . all_treatment_or_study_target_condition (array) : Considering the study in a translational context, to humans with which detailed type of opioid use or pain conditions are the study results intended to apply? Each HEAL study will be doing work that is relevant to a human opioid and/or pain condition; You indicated the broad category of condition(s) the study is focusing on or relevant to above. Here, please indicate the study focus condition(s) in detail. Choose as many as apply - Values will be restricted to NLM MeSH Diseases - see Branch 'C' here: https://meshb-prev.nlm.nih.gov/treeView. Items (string) all_outcome_condition (array) : If the outcome condition(s) the study hopes to impact or measure is different than the treatment or study target condition (e.g. testing a disease-modifying fibromyaligia treatment but measuring depression, physical and social function, and neurologic pain such as headaches as the main outcomes). Please indicate the study outcome condition(s) here. Choose as many as apply - Values will be restricted to NLM MeSH Diseases - see Branch 'C' here: https://meshb-prev.nlm.nih.gov/treeView. Items (string) all_other_condition (array) : If the study is measuring or tracking other human health conditions besides the treatment or study target condition and outcome condition, please indicate the other tracked/measured condition(s) here. Choose as many as apply - Values will be restricted to NLM MeSH Diseases - see Branch 'C' here: https://meshb-prev.nlm.nih.gov/treeView. Items (string) human_subject_applicability (object) gender_applicability (array) : Considering the study in a translational context, to humans of which gender identity(ies) are the study results intended to apply and/or likely to generalize to. Items (string) : Must be one of: ['Female', 'Female-to-male transsexual', 'Intersexed', 'Male', 'Male-to-female transsexual', 'Other'] . sexual_identity_applicability (array) : Considering the study in a translational context, to humans of which sexual identity(ies) are the study results intended to apply and/or likely to generalize to. Items (string) : Must be one of: ['Heterosexual', 'Homosexual (gay/lesbian)', 'Bisexual', 'Asexual', 'Other'] . biological_sex_applicability (array) : Considering the study in a translational context, to humans of which biological sexes are the study results intended to apply and/or likely to generalize to. Items (string) : Must be one of: ['Ambiguous', 'Female', 'Male'] . age_applicability (array) : Considering the study in a translational context, to humans of which age or developmental stage are the study results intended to apply and/or likely to generalize to. Items (string) : Must be one of: ['Fetus', 'Infant, newborn (Birth to 1 month)', 'Infant (1 to 23 months)', 'Child, preschool (2 to 5 years)', 'Child (6 to 12 years)', 'Adolescent (13 to 18 years)', 'Adult (19 to 44 years)', 'Middle aged adult (45 to 64 years)', 'Aged adult (65 to 79 years)', 'Aged, 80 and over (80 years and over)'] . irb_vulnerability_conditions_applicability (array) : Considering the study in a translational context, which special IRB human vulnerability conditions are the study results intended to address or apply to? Items (string) : Must be one of: ['Pregnant women', 'Human fetuses', 'Neonates', 'Prisoners', 'Children', 'Individuals with physical disabilities', 'Individuals with mental disabilities or cognitive impairments', 'Economically disadvantaged', 'Socially disadvantaged', 'Terminally ill or very sick', 'Racial or ethnic minorities', 'Institutionalized persons (for example, persons in correctional facilities, nursing homes or mental health facilities)'] . geographic_applicability (array) : For studies with human subjects, from which geographic location(s) are subjects recruited. If subjects are recruited in the US nationally, choose only US - national. If subject are recruited in the US only in specific state(s), choose US - Specific States, then please also select the abbreviation of each of the specific states from which subjects are recruited. If subjects are recruited in the US from specific county(ies) within specific state(s), please choose US - Specific States and US - Specific Counties, and choose the abbreviation of each specific state from which subjects are recruited. Items (string) : Must be one of: ['Non US', 'US - National', 'US - Specific States', 'US - Specific Counties', 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'] . data (object) data_orientation (array) : Does the study produce/collect quantitative data?, qualitative data? Choose as many as apply. Items (string) : Must be one of: ['Quantitative', 'Qualitative'] . data_source (array) : What is the source of the data the study will produce/collect. Choose as many as apply. Items (string) : Must be one of: ['Administrative', 'Government Collected/Measured', 'Clinical/EHR', 'Laboratory', 'Researcher Collected/Measured', 'Patient-Reported'] . data_type (array) : What is the type of the data the study will produce/collect. Choose as many as apply. Items (string) : Must be one of: ['Genomic', 'Proteomic', 'Biochemical', 'Biophysical', 'Imaging', 'Interview/Focus Group', 'Interview/Focus Group - structured', 'Interview/Focus Group - semi-structured', 'Interview/Focus Group - unstructured', 'Questionnaire/Survey/Assessment', 'Questionnaire/Survey/Assessment - validated instrument', 'Questionnaire/Survey/Assessment - unvalidated instrument'] . subject_data_unit_of_collection (array) : For studies with human subjects, is the data collected at the individual or population level? Items (string) : Must be one of: ['Individual', 'Community, Group, Cluster, Population'] . subject_data_unit_of_collection_expected_number (integer) : For studies with human subjects, what is the expected number of subjects for which data will be collected? If unit of data collection is individuals this is the expected number of individuals, and if the unit of data collection is populations/clusters this is the expected number of populations/clusters. subject_data_unit_of_analysis (array) : For studies with human subjects, is the data analyzed at the individual or population level? Items (string) : Must be one of: ['Individual', 'Community, Group, Cluster, Population'] . subject_data_unit_of_analysis_expected_number (integer) : For studies with human subjects, what is the expected number of subjects at the level at which data will be analyzed? If unit of data analysis is individuals this is the expected number of individuals, and if the unit of data analysis is populations/clusters this is the expected number of populations/clusters. subject_data_level_available (array) : For studies with human subjects that will make at least some data available, will data be made available at the individual subject level or some level of aggregation? Items (string) : Must be one of: ['Individual - Unaggregated', 'Individual - Aggregated', 'Community, Group, Cluster, Population - Unaggregated', 'Community, Group, Cluster, Population - Aggregated', 'Unaggregated', 'Aggregated', 'Aggregated, across all (population summary statistics)', 'Aggregated, by treatment/exposure group', 'Aggregated, by demographic variable(s)', 'Aggregated, by geographic variable(s)'] . subject_geographic_data_level_collected (array) : For studies with human subjects, will geographic data be collected? if so, what level of geographic detail will be collected? Items (string) : Must be one of: ['Exact location', 'Census block', 'Census tract', 'Zip code', 'County', 'Sub-state region', 'State', 'Multi-state region', 'Nation', 'No geographic data will be collected'] . subject_geographic_data_level_available (array) : For studies with human subjects that collect geographic data and will make at least some data available, will geographic data be made available? if so, what level of geographic detail will be made available? Items (string) : Must be one of: ['Exact location', 'Census block', 'Census tract', 'Zip code', 'County', 'Sub-state region', 'State', 'Multi-state region', 'Nation', 'No geographic data will be made available'] .","title":"Properties"},{"location":"schemas/study-metadata-schema-for-humans/#definitions","text":"saneUrl","title":"Definitions"},{"location":"section3/","text":"Are you early in collection/production of study artefacts? \u00b6 Yes No Early In Study \u00b6 Consider implementing HEAL recommendations for organization and naming of study artefacts (Note: Do not copy/duplicate files to implement) Consider wholistic annotation of study artefacts (annotate all study artefacts) Consider 'add as you go' annotation of study artefacts to reduce annotation burden at the end of the study If you are not interested wholistically annotating your study as you go, click here . Late In Study \u00b6 Choose the response which best represents your study's situation. The number of total study artefacts is relatively small OR Your study group is particularly interested in implementing high levels of transparency to increase understandability and continuity of data knowledge internally or externally? The number of total study artefacts is relatively large OR Your study group is more interested in implementing only mandated levels of transparency sufficient to provide context about the study and data Small number of study artefacts or interested in high levels of transparency in data sharing \u00b6 Consider implementing HEAL recommendations for organization and naming of study artefacts. Consider wholistic annotation of study artefacts (annotate all study artefacts). If you have not completed collecting/producing all study artefacts, consider annotating all artefacts already collected, then proceed with 'add as you' annotation of new study artefacts to reduce annotation burden at the end of the study. If you have complete collecting/producing all study artefacts, use 'top down' annotation. If you are not interested in this approach, click here . Large number of study artefacts or interested in implementing mandated levels of transparency \u00b6 Leave your original organization and naming of study artefacts in place. Exception If your study collected relatively large sets of 'like' files (e.g., a tabular file pre study subject per day with several measurements that have the same format across files) and these files do not have a consistent naming convention, we recommend implementing a consistent file name convention to increase interpretability/usability to potential secondary data users and make annotation of these files much simpler and faster\" Consider minimal annotation of study artefacts (annotate only study artefacts that will be shared) to reduce annotation burden while providing helpful context for potential secondary data users. Consider 'top down' annotation of study artefacts based on your data sharing 'orientation' goal(s) to minimize annotation to a set that directly serves your data sharing goals. What is your data sharing 'orientation'? \u00b6 Which of these best describes your orientation towards data sharing. You may be interested in more than one of these, in which case, you may want to review the recommendations for both (if they are different). Interested in sharing a dataset that other investigators could use to gain new insights Interested in sharing only the data that supports my results (a publication, presentation, etc.) Interested in sharing the methods underlying my data so other investigators can apply or build upon them","title":"Are you early in collection/production of study artefacts?"},{"location":"section3/#are-you-early-in-collectionproduction-of-study-artefacts","text":"Yes No","title":"Are you early in collection/production of study artefacts?"},{"location":"section3/#early-in-study","text":"Consider implementing HEAL recommendations for organization and naming of study artefacts (Note: Do not copy/duplicate files to implement) Consider wholistic annotation of study artefacts (annotate all study artefacts) Consider 'add as you go' annotation of study artefacts to reduce annotation burden at the end of the study If you are not interested wholistically annotating your study as you go, click here .","title":"Early In Study"},{"location":"section3/#late-in-study","text":"Choose the response which best represents your study's situation. The number of total study artefacts is relatively small OR Your study group is particularly interested in implementing high levels of transparency to increase understandability and continuity of data knowledge internally or externally? The number of total study artefacts is relatively large OR Your study group is more interested in implementing only mandated levels of transparency sufficient to provide context about the study and data","title":"Late In Study"},{"location":"section3/#small-number-of-study-artefacts-or-interested-in-high-levels-of-transparency-in-data-sharing","text":"Consider implementing HEAL recommendations for organization and naming of study artefacts. Consider wholistic annotation of study artefacts (annotate all study artefacts). If you have not completed collecting/producing all study artefacts, consider annotating all artefacts already collected, then proceed with 'add as you' annotation of new study artefacts to reduce annotation burden at the end of the study. If you have complete collecting/producing all study artefacts, use 'top down' annotation. If you are not interested in this approach, click here .","title":"Small number of study artefacts or interested in high levels of transparency in data sharing"},{"location":"section3/#large-number-of-study-artefacts-or-interested-in-implementing-mandated-levels-of-transparency","text":"Leave your original organization and naming of study artefacts in place. Exception If your study collected relatively large sets of 'like' files (e.g., a tabular file pre study subject per day with several measurements that have the same format across files) and these files do not have a consistent naming convention, we recommend implementing a consistent file name convention to increase interpretability/usability to potential secondary data users and make annotation of these files much simpler and faster\" Consider minimal annotation of study artefacts (annotate only study artefacts that will be shared) to reduce annotation burden while providing helpful context for potential secondary data users. Consider 'top down' annotation of study artefacts based on your data sharing 'orientation' goal(s) to minimize annotation to a set that directly serves your data sharing goals.","title":"Large number of study artefacts or interested in implementing mandated levels of transparency"},{"location":"section3/#what-is-your-data-sharing-orientation","text":"Which of these best describes your orientation towards data sharing. You may be interested in more than one of these, in which case, you may want to review the recommendations for both (if they are different). Interested in sharing a dataset that other investigators could use to gain new insights Interested in sharing only the data that supports my results (a publication, presentation, etc.) Interested in sharing the methods underlying my data so other investigators can apply or build upon them","title":"What is your data sharing 'orientation'?"},{"location":"section3/dec/","text":"Are you early in collection/production of study artefacts? \u00b6 Yes No","title":"Are you early in collection/production of study artefacts?"},{"location":"section3/dec/#are-you-early-in-collectionproduction-of-study-artefacts","text":"Yes No","title":"Are you early in collection/production of study artefacts?"},{"location":"section3/dec/early/","text":"Early In Study \u00b6 Consider implementing HEAL recommendations for organization and naming of study artefacts (Note: Do not copy/duplicate files to implement) Consider wholistic annotation of study artefacts (annotate all study artefacts) Consider 'add as you go' annotation of study artefacts to reduce annotation burden at the end of the study If you are not interested wholistically annotating your study as you go, click here .","title":"Early In Study"},{"location":"section3/dec/early/#early-in-study","text":"Consider implementing HEAL recommendations for organization and naming of study artefacts (Note: Do not copy/duplicate files to implement) Consider wholistic annotation of study artefacts (annotate all study artefacts) Consider 'add as you go' annotation of study artefacts to reduce annotation burden at the end of the study If you are not interested wholistically annotating your study as you go, click here .","title":"Early In Study"},{"location":"section3/dec/late/","text":"Late In Study \u00b6 Choose the response which best represents your study's situation. The number of total study artefacts is relatively small OR Your study group is particularly interested in implementing high levels of transparency to increase understandability and continuity of data knowledge internally or externally The number of total study artefacts is relatively large OR Your study group is more interested in implementing only mandated levels of transparency sufficient to provide context about the study and data","title":"Late In Study"},{"location":"section3/dec/late/#late-in-study","text":"Choose the response which best represents your study's situation. The number of total study artefacts is relatively small OR Your study group is particularly interested in implementing high levels of transparency to increase understandability and continuity of data knowledge internally or externally The number of total study artefacts is relatively large OR Your study group is more interested in implementing only mandated levels of transparency sufficient to provide context about the study and data","title":"Late In Study"},{"location":"section3/dec/late/large/","text":"Leave your original organization and naming of study artefacts in place. Exception If your study collected relatively large sets of 'like' files (e.g., a tabular file pre study subject per day with several measurements that have the same format across files) and these files do not have a consistent naming convention, we recommend implementing a consistent file name convention to increase interpretability/usability to potential secondary data users and make annotation of these files much simpler and faster\" Consider minimal annotation of study artefacts (annotate only study artefacts that will be shared) to reduce annotation burden while providing helpful context for potential secondary data users. Consider 'top down' annotation of study artefacts based on your data sharing 'orientation' goal(s) to minimize annotation to a set that directly serves your data sharing goals.","title":"Large"},{"location":"section3/dec/late/small/","text":"Consider implementing HEAL recommendations for organization and naming of study artefacts . Consider wholistic annotation of study artefacts (annotate all study artefacts). If you have not completed collecting/producing all study artefacts, consider annotating all artefacts already collected, then proceed with ['add as you' go annotation] of new study artefacts to reduce annotation burden at the end of the study. If you have complete collecting/producing all study artefacts, use ['top down' annotation]. If you are not interested in this approach, click here .","title":"Small"},{"location":"section3/orient/orient/","text":"Determining what your goals are for data sharing will help guide you in determing what you will include in your future data package and how you will annotate the data and non-data/supporting documents in your data package. People will have different goals in sharing their study data, which can be dictated by a number of factors such as the nature of the data, resource and time constraints, and investigator preference. There are three main orientations. You do not have to pick just one of these orientations, although results-support is more likely to stand alone. For example, investigators may be interested in sharing a specific dataset in addition to data on their materials and methods. See more about each data sharing orientation below. Results-support This orientation is for investigators who are interested only in fulfilling the requirement to share the data that is necessary to reproduce their results. Example Your study is complete, and you have very limited time or resources budgeted for data sharing. You have published a manuscript on your results or are in the process of submitting one. You want to fulfill the minimum data sharing requirements, so you will only share the data required to reproduce the manuscript results. If this best describes your data sharing orientation, click [here](resultsupp.md). Dataset-oriented This orientation is for investigators who are interested in sharing a specific set of data or as much data as possible so that other researchers can get additional value from them, such as conducting additional research and analyes beyond the original study's focus. Example Your study is collecting RNA expression data. You are only focusing on a small subset of the collected data for your study, but you would like to be able to share as much of this data as possible so that other investigators can comb through the rich data for other analyses and insight. If this best describes your data sharing orientation, click here. Materials- and methods-oriented This orientation is for investigators who are inteersted in sharing inforamtion about their materials and methods that would allow others to reproduce or build upon them in future research. Example Your study is building an organ-on-a-chip model. You want to share information about the formulation of the chip and results demonstrating that the chip performs similarly as the organ in vivo, so that other investigators can understand the process and reassemble and build upon your successful model in future research. If this best describes your data sharing orientation, click here.","title":"Orient"},{"location":"section3/orient/resultsupp/","text":"If you are planning to share only files that are necessary to support the results in a publication, report or presentation: Use 'top-down' annotation. Do not use 'add-as-you-go' annotation. Audit publication(s) or report(s) that need support Audit supporting files for each result in publication(s), including sharing status and access level Audit dependencies of supporting files, including sharing status and access level Add data dictionary for any tabular supporting files or dependencies Create result tracker for each publication or report (list supporting files for each result in the result tracker 'depends on' field). To determine what approach you should take to listing supporting files for each result, click here . Add result tracker to resource tracker Add supporting files to resource tracker (list dependencies for each supporting resource tracker 'depends on' field) Notes on wholistic vs. minimal annotation If using wholistic annotation : When adding supporting files for results in the results tracker, add 'one-layer-deep' dependencies . When adding dependencies of supporting files in the resource tracker, add 'one-layer-deep' dependencies . If using minimal annotation When adding supporting files for results in the results tracker, add 'liberal' dependencies . When adding dependencies of supporting files in the resource tracker, add 'liberal' dependencies .","title":"Resultsupp"},{"location":"submit/","text":"Placeholder \u00b6","title":"Placeholder"},{"location":"submit/#placeholder","text":"","title":"Placeholder"},{"location":"terms/","text":"Terms and Concepts \u00b6 Tabular data file \u00b6 A data file that is organized in a table with rows and columns. Study files/resources \u00b6 All data and non-data supporting files/documents/resources generated by or for or otherwise associated with your study, regardless of whether you are planning to share them. Also known as: study resources, study files, study documents, study artefacts. NOTE: For most studies, study resources will all be files. However for some studies, this may also include (for example) bio samples or other non-file items. Associated Files/Dependencies \u00b6 The term Associated Files/Dependencies may be used with respect to a Resource (i.e. study file/resource ) or with respect to a Result (i.e. final result product ). Resource When adding a study file/resource to your study's Resource Tracker , you will be asked to supply Associated Files/Dependencies for that study file/resource. An Associated File/Dependency, is any file that the current study file/resource depends on to be interpreted, replicated, or used. For example, consider a processed tabular data file (e.g. a final analytic dataset): a raw data file(s) , plus a code file that merges and cleans the raw data file(s) may be required to replicate the processed tabular data file, and a data dictionary for the processed tabular data file is likely to be required to interpret/use the processed tabular data file. Note Data dictionaries and protocols should be shared in the more specific Associated Data Dictionary , Associated Protocol fields in the Resource Tracker wherever possible When adding a publication or report to the Resource Tracker, the only dependency should be the Results Tracker for the publication, and the associated Results Tracker for the publication should be added in the more specific Associated Results Tracker field in the Resource Tracker wherever possible Result When adding a final result product to your study's Results Tracker , you will be asked to supply Associated Files/Dependencies for that result. An Associated File/Dependency, is any file that the current result depends on to be interpreted, replicated, or used. For example, consider a figure that has been created as a potential candidate for inclusion as Figure 1A (in the context of a Figure 1 with panels A, B, and C) in a draft manuscript: a png or jpg image file exported from Corel Draw or Illustrator may directly underly the whole of Figure 1 (where image files for panel A, B, and C were formatted into a single Figure 1 in the context of a Corel Draw or Illustrator file ); a png or jpg image file produced and written to file by R or python code may directly underly specifically Figure 1A; this image file in turn depends upon the code file that produced it, as well as the data file(s) the code file read in and operated upon to create the final image file; the code file may have been created to implement a specific statistical analysis plan that is laid out in a larger study protocol file , or in a statistical analysis plan file ; the data in the data file may have been collected using a protocol laid out in a specific protocol file , and if the data file(s) underlying the result in 1A is/are tabular data file(s), a data dictionary for the data file(s) is likely to be required to interpret/use the data file(s) Note When listing Associated Files/Dependencies for a result in the Results Tracker, start by following the full dependency trail backwards from the result, as demonstrated in the example above Specifically, think about all Associated Files/Dependencies directly underlying or associated with the result, then the Associated Files/Dependencies directly underlying those files, etc. Continue this \"stepping backwards\" process until the files you are listing as Associated Files/Dependencies have no direct Associated Files/Dependencies themselves List all of these files as Associated Files/Dependencies for the result in the Results Tracker Associated Data Dictionary \u00b6 The Data Dictionary that inventories and provides detailed information regarding all variables in the specific tabular data file for which it is the Associated Data Dictionary. Associated Protocol \u00b6 Placeholder Associated Results Tracker \u00b6 The Results Tracker that inventories and provides detailed information regarding all final result products (e.g. figures, figure panels, tables, text statements) shared in the specific publication or report for which it is the Associated Results Tracker. Final Result Products \u00b6 A figure, figure panel, table, or text statement that communicates a result and is, will, or may be shared in the context of a publication, report or presentation dsc-pkg Folder \u00b6 A file folder/directory that will hold all of the Standard Data Package Metadata Files for your Data Package . It is generally recommended that you create a single overarching study file folder/directory to hold all study files and folders and that you create your dsc-pkg folder as a direct sub-directory of your study folder and name it \"dsc-pkg\". Standard data package metadata files \u00b6 Standard metadata file types that, altogether, provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. These metadata files should be included in all data packages . They should be stored together in a single file directory, preferably as a sub-directory within your study file directory called \"dsc-pkg\". See below for an example directory structure. NOTE : All standard data package metadata files have a standardized csv format in which they should be completed and provided. See here for csv templates and schemas/field definitions to aid you in completing the templates. Standard data package metadata files - Study-level \u00b6 Experiment Tracker \u00b6 One per study; An inventory of experiments or activities included in the study; For a clinical trial, this may be simply one experiment equal to the registered clinical trial activity; For a basic biology study, this may be a listing of several orthogonal experiments used altogether to address and advance the study aims - See here for more detail Resource Tracker \u00b6 One per study; An inventory of all data and non-data supporting files produced during the course of the study (or, in some cases, only those which will be shared in a public data repository), including a description of what is in the file or what the file represents, file relationships and dependencies, and whether/how each file is shareable in a public repository or not - See here for more detail Standard data package metadata files - File-level \u00b6 Data Dictionary \u00b6 One per tabular data file; An inventory of variables included in a tabular data file - See here for more detail Results Tracker \u00b6 One per publication or report; An inventory of figure, table, and text statement results included in a publication or report - See here for more detail Experiment Tracker - Overview \u00b6 The Experiment Tracker is an inventory and annotated list of all component experiments or activities that are part of the larger study. Each row of the experiment tracker corresponds to one component experiment or activity. Information in the tracker about each experiment includes the research question(s), approach, and hypotheses. The Experiment Tracker is one of the standard data package metadata files which should always be included in a data package to provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. There are study-level and file-level standard data package metadata files. The Experiment Tracker is a study-level standard data package metadata file ( you should create and complete one Experiment Tracker per study ). Please follow the links below for additional information on: Standard data package metadata files Standard data package metadata files - Study-level Experiment Tracker overview Experiment Tracker csv template Experiment Tracker schema/field definitions Resource Tracker - Overview \u00b6 The Resource Tracker is an inventory and annotated list of data and non-data supporting files/resources for the study. Each row of the resource tracker corresponds to one data or non-data resource. Information in the tracker about each resource includes file path, description, access restrictions, and dependencies (i.e. files necessary to interpret, replicate, or use the resource). The Resource Tracker is one of the standard data package metadata files which should always be included in a data package to provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. There are study-level and file-level standard data package metadata files. The Resource Tracker is a study-level standard data package metadata file ( you should create and complete one Resource Tracker per study ). Please follow the links below for additional information on: Standard data package metadata files Standard data package metadata files - Study-level Resource Tracker overview Resource Tracker csv template Resource Tracker schema/field definitions Data Dictionary - Overview \u00b6 The Data Dictionary is an inventory and annotated list of variables within a single tabular data file (e.g. subject ID, blood pressure, zip code, protein activitiy, etc.). Each row of the data dictionary corresponds to one variable within a tabular data file. Information in the data dictionary about each variable includes the name of the variable, a description of the variable, type of variable (string, numeric, integer), etc. The Data Dictionary is one of the standard data package metadata files which should always be included in a data package to provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. There are study-level and file-level standard data package metadata files. The Data Dictionary is a file-level standard data package metadata file ( you should create and complete one Data Dictionary per tabular data file in your data package ). Please follow the links below for additional information on: Standard data package metadata files Standard data package metadata files - File-level Data Dictionary overview Data Dictionary csv template Data Dictionary schema/field definitions Results Tracker - Overview \u00b6 The Results Tracker is an inventory and annotated list of results within a single publication or report. Each row of the results tracker corresponds to one result (e.g. a figure, table, or textual statement) within a publication or report. Information in the tracker about each result includes the type of result (figure, table, text), description, and dependencies (i.e. files necessary to interpret, replicate, or use the result). The Results Tracker is one of the standard data package metadata files which should always be included in a data package to provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. There are study-level and file-level standard data package metadata files. The Results Tracker is a file-level standard data package metadata file ( you should create and complete one Results Tracker per publication or report in your data package ). Please follow the links below for additional information on: Standard data package metadata files Standard data package metadata files - File-level Results Tracker overview Results Tracker csv template Results Tracker schema/field definitions Minimal annotation \u00b6 When completing the Resource Tracker for your study : Minimal annotation implies that you will list and annotate relevant study resources in the resource tracker ONLY if you will share those files in a public data repository When completing the Resource Tracker you will NOT list and annotate relevant study files if they will not be shared in a public data repository See the alternative: Wholistic Annotation Advantages You only catalog the data and non-data supporting files that you will share/submit to a repository. Especially if you are late in your study, Minimal Annotation may be less time consuming than the alternative (Wholistic Annotation), because you are listing and annotating fewer files (i.e. only the files that will be shared , versus all files that will be shared AND all files that will NOT be shared) in the Resource Tracker for your study. This approach still provides a lot of value to researchers who may find your study - It will help them to parse what the study was trying to do, how the study was designed, what has been made available, whether or not the data that has been made available may be useful for their purposes (e.g. secondary data analysis, comparison to their own results, etc.), and even whether it may be useful to reach out to the study group of origin to request data that has not been provided or to set up a formal collaboration. This approach allows you to fulfill minimal data sharing requirements. Caveats As compared to the alternative (Wholistic Annotation), you don\u2019t get the full local annotation benefit that would come with fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), and how they relate to each other and to published results \u2013 these benefits include facilitating continuity and passed-down knowledge within study groups , and discovery, sharing, and re-use of the data and knowledge produced by the study outside of the original study group. As compared to the alternative (Wholistic Annotation), you may not get the full benefit of added study discoverability and transparencty for potential secondary data users and collaborators that the Resource Tracker can provide. Wholistic annotation \u00b6 When completing the Resource Tracker for your study : Wholistic annotation implies that you will list and annotate relevant study resources without regard for whether (or not) you will share those files in a public data repository When completing the Resource Tracker you will list and annotate relevant study files that will be shared AND those that will NOT be shared When listing/annotating a file that will NOT be shared, access level should be set to \"permanent-private\" to indicate that the file will not be shared See the alternative: Minimal Annotation Advantages Maximizes transparency and allows other researchers interested in the data to understand the full scope of the project and the data when accessing study documentation. allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by another researcher. You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Caveats Especially if you are late in your study, Wholistic Annotation may be more time consuming than the alternative (Minimal Annotation), because you are listing and annotating more files (i.e. files that will be shared AND files that will NOT be shared, versus ONLY files that will be shared) in the Resource Tracker for your study. As-you-go annotation \u00b6 As-you-go annotation implies that you will begin the data packaging and annotation process right away; you will audit and annotate all study resources already created, and keep up with annotation as you move through the remainder of your study timeline; you will generally audit and annotate all study resources regardless of whether these resources will ultimately be shared at a public data repository ( \"wholistic\" annotation ) See the alternative: Top-down Annotation As you move through the data packaging process steps , \"as-you-go\" annotation implies that you will : Audit relevant files start this right away audit all files/resources already collected or produced by or for your study determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) Organize and consistently name your files and folders start this right away consider using HEAL recommendations for organizing and naming study files/resources to establish naming and organization conventions that should be followed for all study files, including for files that already exist and for new study files/resources as they are collected or produced align file name/organization of existing study files to the conventions you established in the step above - this may require you to change file names and locations for already-existing files (NOTE: do not duplicate files in order to align with name/org conventions as this may lead to confusion down the road as to which copy is the \"source of truth\") Add Standard data package metadata - File-level Data Dictionary start this right away audit existing study files to identify any tabular data files that may have already been collected or produced by or for the study create a data dictionary for any existing tabular data files right away if/when any new tabular data files are collected or produced by or for the study, create a data dictionary for the new tabular data file(s) right away and save them using file name and organization conventions you have established for your study (see step 2 above) Results Tracker start this right away : determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) wait until your study is at the point of producing final result products (e.g. figures, tables, text statements) once your study has reached this point, and if your study has determined that a data sharing goal of the study is to provide results-support , start annotating final result products right away, even if you have not formally begun to add them to a publication or report as new final result products are produced by your study, annotate each final result product right away once your study has started to formally add final results to a publication, start adding result annotation files you've already produced (in the step just above) to a results tracker document for the publication - one results tracker per publication as new publications or reports are produced by your study, use the result annotation files you create for each final result product in the publication to create a results tracker document for the new publication Add Standard data package metadata - Study-level Experiment Tracker start this right away audit all component experiments or other activities that will be part of your study start annotating study component experiments or activities that have already been designed if/when any new study component experiments or activities are designed, annotate each new study component experiment or activity right away Resource Tracker start this right away audit all files/resources already collected or produced by or for your study (see step 1 above) establish file naming and organization conventions for your study files and make any necessary adjustments to current study file naming/organization to align with these conventions (see step 2 above) systematically annotate all files/resources already collected or produced by or for your study right away once you've annotated all files/resources already produced by or for your study, add these resource annotation files to your resource tracker as new files/resources are collected or produced by or for your study, save them using the file naming and organization conventions you established previously, and annotate each new resource right away Advantages Spreads out annotation and data packaging work across the course of the study so that burden at the end of the stusy is minimal. You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Maximizes transparency and allows other researchers interested in the data to understand the full scope of the project and the data when accessing study documentation. Allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by another researcher. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Caveats The \"as-you-go\" annotation approach, **when applied broadly as outlined above** is strongly recommended for study groups that are early on in their study as the burden of starting up is relatively light when few study files/resources have so far been collected or produced by or for the study. However, the start up burden of this approach may be quite substantial for studies groups that are late or even well into their study and have already accumulated many study files/resources, and we generally recommend these groups consider the alternative, more goal-focused and narrow annotation approach: [Top-down annotation](#top-down-annotation). **The \"as-you-go\" annotation approach may also be applied in a narrower sense**, especially by study groups that are later in their study and who will not apply the \"as-you-go\" annotation approach in the broadest sense. This implies that studies will consider the whole packaging overview process and complete items as they can, as opposed to waiting until the very end (for example, when they are about to submit a study manuscript for peer review) to start the process. Some examples include, 1) auditing study files for tabular data files, creating data dictionaries for existing tabular data files right away, and creating data dictionaries right away for new tabular data files as the study collects or produces them, 2) annotating final result products the study group knows will or likely will be included in a final manuscript as they are produced, and creating a results tracker for final manuscript documents as drafts begin to be forumulated by the study group, 3) annotating component experiments and other activities that are part of the study right away if already designed, or as soon as they are designed (especially if it is clear that the experiment or activity will or likely will produce data that will be used to support/produce final result products that will be included in a final manuscript). Top-down annotation \u00b6 Top-down annotation implies that you will generally implement the data packaging and annotation process in a somewhat narrower, more goal-oriented manner as compared to how you would implement if using the alternative As-you-go annotation approach; You will generally determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation), then wait until your study has produced the goal sharing product (i.e. respectively, the final result product(s) or manuscripts containing a set of final result product(s) for which your data sharing will provide support, or the final dataset your study is interested in sharing/disseminating); you will then audit and annotate the subset of study resources required to interpret, use, and/or reproduce the results or dataset already created, and annotate this subset of study resources right away; when you take the \"top down\" annotation approach, you will generally audit the full subset of study resources required to support your result(s) and/or dataset regardless of whether these resources will ultimately be shared at a public data repository, however you may choose to 1) annotate all resources in this subset regardless of whether they will be shared in a public repository ( \"wholistic\" annotation ), or 2) annotate only the resources in this subset that will be shared in a public repository ( \"minimal\" annotation ) See the alternative: As-you-go Annotation As you move through the data packaging process steps , \"top-down\" annotation implies that you will : Audit relevant files start this right away : determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) wait until your study has produced the goal sharing product (i.e. respectively, the final result product(s) or manuscripts containing a set of final result product(s) for which your data sharing will provide support, or the final dataset your study is interested in sharing/disseminating) audit the full subset of study resources required to interpret, use, and/or reproduce the goal sharing product(s) (i.e. final study result product(s)/manuscipt(s) or dataset) Organize and consistently name your files and folders start this right away : determine if you are early in your study if you are early in your study: consider using HEAL recommendations for organizing and naming study files/resources to establish naming and organization conventions that should be followed for all study files, including for files that already exist and for new study files/resources as they are collected or produced align file name/organization of existing study files to the conventions you established in the step above - this may require you to change file names and locations for already-existing files (NOTE: do not duplicate files in order to align with name/org conventions as this may lead to confusion down the road as to which copy is the \"source of truth\") if you are late in your study: we generally recommend that you leave file names and organization as is you may use the approach laid out above for those early in their study, however if you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error if you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so will make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Add Standard data package metadata - File-level Data Dictionary start this right away : determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) wait until your study has produced the goal sharing product (i.e. respectively, the final result product(s) or manuscripts containing a set of final result product(s) for which your data sharing will provide support, or the final dataset your study is interested in sharing/disseminating) audit the full subset of study resources required to interpret, use, and/or reproduce the goal sharing product(s) (i.e. final study result product(s)/manuscipt(s) or dataset) among this subset of files, identify any tabular data files create a data dictionary for these tabular data files right away Results Tracker start this right away : determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) wait until your study is at the point of producing final result products (e.g. figures, tables, text statements) once your study has reached this point, and if your study has determined that a data sharing goal of the study is to provide results-support , start annotating final result products right away that are likely to be included in a final manuscript or report you will share and for which you would like to provide results-support, even if you have not formally begun to add them to a publication or report as new final result products are produced by your study, annotate each final result product right away once your study has started to formally add final results to a publication, start adding result annotation files you've already produced (in the step just above) to a results tracker document for the publication - one results tracker per publication as new publications are produced by your study, use the result annotation files you create for each final result product in the publication or report to create a results tracker document for the new publication or report alternatively, you may wait until your study is at the point of a final manuscript or report for which you would like to provide results-support; then start annotating only the final result products that are positively contained in this publication Add Standard data package metadata - Study-level Experiment Tracker start this right away OR wait until your study has produced the goal sharing product audit all component experiments or other activities that are or will be part of your study identify the subset of component experiments or other activities that positively produced or are likely to produce data or other products that are required to interpret, use, or reproduce your goal data sharing product (i.e. final result product(s), manuscripts, or a dataset you want to share/disseminate) start annotating this subset of study component experiments or activities if they have already been designed if/when any new study component experiments or activities are designed and it is determined that the new experiment/activity produced or is likely to produce data or other products that are required to interpret, use, or reproduce your goal data sharing product, annotate this new study component experiment(s) or activity(s) right away Resource Tracker wait until your study has produced the goal sharing product (i.e. final result product(s), manuscripts, or a dataset you want to share/disseminate) audit the full subset of study resources required to interpret, use, and/or reproduce the goal sharing product(s) (i.e. final study result product(s)/manuscipt(s) or dataset) (see step 1 above) starting with your goal data sharing product (i.e. manuscript, or a dataset you want to share/disseminate), systematically annotate either : all files/resources in this subset of study resources regardless of whether they will be shared in a public data repository ( \"wholistic\" annotation ) only the files/resources in this subset of study resources that will be shared in a public data repository ( \"minimal\" annotation ) once you've annotated all files/resources, add these resource annotation files to your resource tracker Advantages \"Top-down\" annotation allows for a narrower and more goal-focused approach to annotation and data-sharing as compared to the alternative \"As-you-go\" annotation; This approach may be very appealing to study groups that are well into their study as it reduces annotation and data sharing burden by focusing on producing the essential annotation needed to ensure that the results or datasets they share are discoverable, interpretable, reusable, and replicable by researchers and other potential collaborators. Concentrates annotation and data packaging work at the end of the study which allows singular focus on the task and may speed the process. If you choose to use \"wholistic\" annotation with the \"top-down\" annotation approach, you get the benefit of full local annotation of the files underlying your study's goal data sharing product (manuscript results or shared dataset), which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Even if you choose to use \"minimal\" annotation with the \"top down\" approach you still get the benefit of some local annotation to support preserving institutional knowledge within your study group. If you choose to use \"wholistic\" annotation with the \"top-down\" annotation approach, you maximize transparency and allow other researchers interested in the data to understand the full scope of the project and the data that explicitly underlies your study's goal data sharing product (manuscript results or shared dataset) when accessing study documentation. Even if you choose to use \"minimal\" annotation with the \"top down\" approach you still provide a lot of transparency about what data and non-data supporting files you are sharing to support your study's goal data sharing product, how these files relate to each other and how to use and interpret these files; this presents a huge value to potential secondary data users or collaborators. If you choose to use \"wholistic\" annotation with the \"top-down\" annotation approach, this still allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility, interpretability and use of the study's goal data sharing product (manuscript results or shared dataset) and can perhaps be requested directly from the study team by another researcher. If you choose to use \"wholistic\" annotation with the \"top-down\" annotation approach, this allows for documenting and sharing all metadata associated with the study's goal data sharing product (manuscript results or shared dataset) and this can increase the discoverability of your study. Caveats Placeholder Placeholder Data Package \u00b6 Placeholder Shareable Data Package \u00b6 Placeholder Data-sharing Orientation \u00b6 A study group's goal(s) for data-sharing. People will have different goals in sharing data and non-data supporting files/resources from their study. These goals, or 'orientations' may be dictated by a number of factors such as the nature of the data, study staff resources and time constraints, investigator preference, and requirements imposed by a funder, journal, or other entity somehow governing the study. Determining what your goals are for data sharing will help guide you in determining what you will include in your data package and how and when you will annotate the data and non-data supporting files/resources in your data package. There are two main data-sharing \"orientations.\" These data sharing goals or \"orientations\" are not mutually exclusive. You may choose both Results-support and Dataset-sharing as goals for your study's data sharing goal. Results-support \u00b6 The study group wants to share data and non-data supporting files specifically to support results shared in a published manuscript or other venue (e.g. presentation, poster, report, etc.) They will share items required to interpret, replicate, or use these results Dataset-sharing \u00b6 The study group wants to share a specific dataset(s); perhaps the group has collected/created a very rich dataset and used this dataset to ask and publish results related to specific scientific questions; they believe other study groups may be able to leverage this dataset to ask and publish results related to other scientific questions that may be related or unrelated to the questions the dataset was originally collected to help investigate The study group wants to share this specific dataset(s), as well as the data and non-data supporting files specifically to support use of this dataset(s) They will share items required to interpret, replicate, or use this dataset(s)","title":"Terms and Concepts"},{"location":"terms/#terms-and-concepts","text":"","title":"Terms and Concepts"},{"location":"terms/#tabular-data-file","text":"A data file that is organized in a table with rows and columns.","title":"Tabular data file"},{"location":"terms/#study-filesresources","text":"All data and non-data supporting files/documents/resources generated by or for or otherwise associated with your study, regardless of whether you are planning to share them. Also known as: study resources, study files, study documents, study artefacts. NOTE: For most studies, study resources will all be files. However for some studies, this may also include (for example) bio samples or other non-file items.","title":"Study files/resources"},{"location":"terms/#associated-filesdependencies","text":"The term Associated Files/Dependencies may be used with respect to a Resource (i.e. study file/resource ) or with respect to a Result (i.e. final result product ). Resource When adding a study file/resource to your study's Resource Tracker , you will be asked to supply Associated Files/Dependencies for that study file/resource. An Associated File/Dependency, is any file that the current study file/resource depends on to be interpreted, replicated, or used. For example, consider a processed tabular data file (e.g. a final analytic dataset): a raw data file(s) , plus a code file that merges and cleans the raw data file(s) may be required to replicate the processed tabular data file, and a data dictionary for the processed tabular data file is likely to be required to interpret/use the processed tabular data file. Note Data dictionaries and protocols should be shared in the more specific Associated Data Dictionary , Associated Protocol fields in the Resource Tracker wherever possible When adding a publication or report to the Resource Tracker, the only dependency should be the Results Tracker for the publication, and the associated Results Tracker for the publication should be added in the more specific Associated Results Tracker field in the Resource Tracker wherever possible Result When adding a final result product to your study's Results Tracker , you will be asked to supply Associated Files/Dependencies for that result. An Associated File/Dependency, is any file that the current result depends on to be interpreted, replicated, or used. For example, consider a figure that has been created as a potential candidate for inclusion as Figure 1A (in the context of a Figure 1 with panels A, B, and C) in a draft manuscript: a png or jpg image file exported from Corel Draw or Illustrator may directly underly the whole of Figure 1 (where image files for panel A, B, and C were formatted into a single Figure 1 in the context of a Corel Draw or Illustrator file ); a png or jpg image file produced and written to file by R or python code may directly underly specifically Figure 1A; this image file in turn depends upon the code file that produced it, as well as the data file(s) the code file read in and operated upon to create the final image file; the code file may have been created to implement a specific statistical analysis plan that is laid out in a larger study protocol file , or in a statistical analysis plan file ; the data in the data file may have been collected using a protocol laid out in a specific protocol file , and if the data file(s) underlying the result in 1A is/are tabular data file(s), a data dictionary for the data file(s) is likely to be required to interpret/use the data file(s) Note When listing Associated Files/Dependencies for a result in the Results Tracker, start by following the full dependency trail backwards from the result, as demonstrated in the example above Specifically, think about all Associated Files/Dependencies directly underlying or associated with the result, then the Associated Files/Dependencies directly underlying those files, etc. Continue this \"stepping backwards\" process until the files you are listing as Associated Files/Dependencies have no direct Associated Files/Dependencies themselves List all of these files as Associated Files/Dependencies for the result in the Results Tracker","title":"Associated Files/Dependencies"},{"location":"terms/#associated-data-dictionary","text":"The Data Dictionary that inventories and provides detailed information regarding all variables in the specific tabular data file for which it is the Associated Data Dictionary.","title":"Associated Data Dictionary"},{"location":"terms/#associated-protocol","text":"Placeholder","title":"Associated Protocol"},{"location":"terms/#associated-results-tracker","text":"The Results Tracker that inventories and provides detailed information regarding all final result products (e.g. figures, figure panels, tables, text statements) shared in the specific publication or report for which it is the Associated Results Tracker.","title":"Associated Results Tracker"},{"location":"terms/#final-result-products","text":"A figure, figure panel, table, or text statement that communicates a result and is, will, or may be shared in the context of a publication, report or presentation","title":"Final Result Products"},{"location":"terms/#dsc-pkg-folder","text":"A file folder/directory that will hold all of the Standard Data Package Metadata Files for your Data Package . It is generally recommended that you create a single overarching study file folder/directory to hold all study files and folders and that you create your dsc-pkg folder as a direct sub-directory of your study folder and name it \"dsc-pkg\".","title":"dsc-pkg Folder"},{"location":"terms/#standard-data-package-metadata-files","text":"Standard metadata file types that, altogether, provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. These metadata files should be included in all data packages . They should be stored together in a single file directory, preferably as a sub-directory within your study file directory called \"dsc-pkg\". See below for an example directory structure. NOTE : All standard data package metadata files have a standardized csv format in which they should be completed and provided. See here for csv templates and schemas/field definitions to aid you in completing the templates.","title":"Standard data package metadata files"},{"location":"terms/#standard-data-package-metadata-files-study-level","text":"","title":"Standard data package metadata files - Study-level"},{"location":"terms/#experiment-tracker","text":"One per study; An inventory of experiments or activities included in the study; For a clinical trial, this may be simply one experiment equal to the registered clinical trial activity; For a basic biology study, this may be a listing of several orthogonal experiments used altogether to address and advance the study aims - See here for more detail","title":"Experiment Tracker"},{"location":"terms/#resource-tracker","text":"One per study; An inventory of all data and non-data supporting files produced during the course of the study (or, in some cases, only those which will be shared in a public data repository), including a description of what is in the file or what the file represents, file relationships and dependencies, and whether/how each file is shareable in a public repository or not - See here for more detail","title":"Resource Tracker"},{"location":"terms/#standard-data-package-metadata-files-file-level","text":"","title":"Standard data package metadata files - File-level"},{"location":"terms/#data-dictionary","text":"One per tabular data file; An inventory of variables included in a tabular data file - See here for more detail","title":"Data Dictionary"},{"location":"terms/#results-tracker","text":"One per publication or report; An inventory of figure, table, and text statement results included in a publication or report - See here for more detail","title":"Results Tracker"},{"location":"terms/#experiment-tracker-overview","text":"The Experiment Tracker is an inventory and annotated list of all component experiments or activities that are part of the larger study. Each row of the experiment tracker corresponds to one component experiment or activity. Information in the tracker about each experiment includes the research question(s), approach, and hypotheses. The Experiment Tracker is one of the standard data package metadata files which should always be included in a data package to provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. There are study-level and file-level standard data package metadata files. The Experiment Tracker is a study-level standard data package metadata file ( you should create and complete one Experiment Tracker per study ). Please follow the links below for additional information on: Standard data package metadata files Standard data package metadata files - Study-level Experiment Tracker overview Experiment Tracker csv template Experiment Tracker schema/field definitions","title":"Experiment Tracker - Overview"},{"location":"terms/#resource-tracker-overview","text":"The Resource Tracker is an inventory and annotated list of data and non-data supporting files/resources for the study. Each row of the resource tracker corresponds to one data or non-data resource. Information in the tracker about each resource includes file path, description, access restrictions, and dependencies (i.e. files necessary to interpret, replicate, or use the resource). The Resource Tracker is one of the standard data package metadata files which should always be included in a data package to provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. There are study-level and file-level standard data package metadata files. The Resource Tracker is a study-level standard data package metadata file ( you should create and complete one Resource Tracker per study ). Please follow the links below for additional information on: Standard data package metadata files Standard data package metadata files - Study-level Resource Tracker overview Resource Tracker csv template Resource Tracker schema/field definitions","title":"Resource Tracker - Overview"},{"location":"terms/#data-dictionary-overview","text":"The Data Dictionary is an inventory and annotated list of variables within a single tabular data file (e.g. subject ID, blood pressure, zip code, protein activitiy, etc.). Each row of the data dictionary corresponds to one variable within a tabular data file. Information in the data dictionary about each variable includes the name of the variable, a description of the variable, type of variable (string, numeric, integer), etc. The Data Dictionary is one of the standard data package metadata files which should always be included in a data package to provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. There are study-level and file-level standard data package metadata files. The Data Dictionary is a file-level standard data package metadata file ( you should create and complete one Data Dictionary per tabular data file in your data package ). Please follow the links below for additional information on: Standard data package metadata files Standard data package metadata files - File-level Data Dictionary overview Data Dictionary csv template Data Dictionary schema/field definitions","title":"Data Dictionary - Overview"},{"location":"terms/#results-tracker-overview","text":"The Results Tracker is an inventory and annotated list of results within a single publication or report. Each row of the results tracker corresponds to one result (e.g. a figure, table, or textual statement) within a publication or report. Information in the tracker about each result includes the type of result (figure, table, text), description, and dependencies (i.e. files necessary to interpret, replicate, or use the result). The Results Tracker is one of the standard data package metadata files which should always be included in a data package to provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. There are study-level and file-level standard data package metadata files. The Results Tracker is a file-level standard data package metadata file ( you should create and complete one Results Tracker per publication or report in your data package ). Please follow the links below for additional information on: Standard data package metadata files Standard data package metadata files - File-level Results Tracker overview Results Tracker csv template Results Tracker schema/field definitions","title":"Results Tracker - Overview"},{"location":"terms/#minimal-annotation","text":"When completing the Resource Tracker for your study : Minimal annotation implies that you will list and annotate relevant study resources in the resource tracker ONLY if you will share those files in a public data repository When completing the Resource Tracker you will NOT list and annotate relevant study files if they will not be shared in a public data repository See the alternative: Wholistic Annotation Advantages You only catalog the data and non-data supporting files that you will share/submit to a repository. Especially if you are late in your study, Minimal Annotation may be less time consuming than the alternative (Wholistic Annotation), because you are listing and annotating fewer files (i.e. only the files that will be shared , versus all files that will be shared AND all files that will NOT be shared) in the Resource Tracker for your study. This approach still provides a lot of value to researchers who may find your study - It will help them to parse what the study was trying to do, how the study was designed, what has been made available, whether or not the data that has been made available may be useful for their purposes (e.g. secondary data analysis, comparison to their own results, etc.), and even whether it may be useful to reach out to the study group of origin to request data that has not been provided or to set up a formal collaboration. This approach allows you to fulfill minimal data sharing requirements. Caveats As compared to the alternative (Wholistic Annotation), you don\u2019t get the full local annotation benefit that would come with fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), and how they relate to each other and to published results \u2013 these benefits include facilitating continuity and passed-down knowledge within study groups , and discovery, sharing, and re-use of the data and knowledge produced by the study outside of the original study group. As compared to the alternative (Wholistic Annotation), you may not get the full benefit of added study discoverability and transparencty for potential secondary data users and collaborators that the Resource Tracker can provide.","title":"Minimal annotation"},{"location":"terms/#wholistic-annotation","text":"When completing the Resource Tracker for your study : Wholistic annotation implies that you will list and annotate relevant study resources without regard for whether (or not) you will share those files in a public data repository When completing the Resource Tracker you will list and annotate relevant study files that will be shared AND those that will NOT be shared When listing/annotating a file that will NOT be shared, access level should be set to \"permanent-private\" to indicate that the file will not be shared See the alternative: Minimal Annotation Advantages Maximizes transparency and allows other researchers interested in the data to understand the full scope of the project and the data when accessing study documentation. allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by another researcher. You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Caveats Especially if you are late in your study, Wholistic Annotation may be more time consuming than the alternative (Minimal Annotation), because you are listing and annotating more files (i.e. files that will be shared AND files that will NOT be shared, versus ONLY files that will be shared) in the Resource Tracker for your study.","title":"Wholistic annotation"},{"location":"terms/#as-you-go-annotation","text":"As-you-go annotation implies that you will begin the data packaging and annotation process right away; you will audit and annotate all study resources already created, and keep up with annotation as you move through the remainder of your study timeline; you will generally audit and annotate all study resources regardless of whether these resources will ultimately be shared at a public data repository ( \"wholistic\" annotation ) See the alternative: Top-down Annotation As you move through the data packaging process steps , \"as-you-go\" annotation implies that you will : Audit relevant files start this right away audit all files/resources already collected or produced by or for your study determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) Organize and consistently name your files and folders start this right away consider using HEAL recommendations for organizing and naming study files/resources to establish naming and organization conventions that should be followed for all study files, including for files that already exist and for new study files/resources as they are collected or produced align file name/organization of existing study files to the conventions you established in the step above - this may require you to change file names and locations for already-existing files (NOTE: do not duplicate files in order to align with name/org conventions as this may lead to confusion down the road as to which copy is the \"source of truth\") Add Standard data package metadata - File-level Data Dictionary start this right away audit existing study files to identify any tabular data files that may have already been collected or produced by or for the study create a data dictionary for any existing tabular data files right away if/when any new tabular data files are collected or produced by or for the study, create a data dictionary for the new tabular data file(s) right away and save them using file name and organization conventions you have established for your study (see step 2 above) Results Tracker start this right away : determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) wait until your study is at the point of producing final result products (e.g. figures, tables, text statements) once your study has reached this point, and if your study has determined that a data sharing goal of the study is to provide results-support , start annotating final result products right away, even if you have not formally begun to add them to a publication or report as new final result products are produced by your study, annotate each final result product right away once your study has started to formally add final results to a publication, start adding result annotation files you've already produced (in the step just above) to a results tracker document for the publication - one results tracker per publication as new publications or reports are produced by your study, use the result annotation files you create for each final result product in the publication to create a results tracker document for the new publication Add Standard data package metadata - Study-level Experiment Tracker start this right away audit all component experiments or other activities that will be part of your study start annotating study component experiments or activities that have already been designed if/when any new study component experiments or activities are designed, annotate each new study component experiment or activity right away Resource Tracker start this right away audit all files/resources already collected or produced by or for your study (see step 1 above) establish file naming and organization conventions for your study files and make any necessary adjustments to current study file naming/organization to align with these conventions (see step 2 above) systematically annotate all files/resources already collected or produced by or for your study right away once you've annotated all files/resources already produced by or for your study, add these resource annotation files to your resource tracker as new files/resources are collected or produced by or for your study, save them using the file naming and organization conventions you established previously, and annotate each new resource right away Advantages Spreads out annotation and data packaging work across the course of the study so that burden at the end of the stusy is minimal. You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Maximizes transparency and allows other researchers interested in the data to understand the full scope of the project and the data when accessing study documentation. Allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by another researcher. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Caveats The \"as-you-go\" annotation approach, **when applied broadly as outlined above** is strongly recommended for study groups that are early on in their study as the burden of starting up is relatively light when few study files/resources have so far been collected or produced by or for the study. However, the start up burden of this approach may be quite substantial for studies groups that are late or even well into their study and have already accumulated many study files/resources, and we generally recommend these groups consider the alternative, more goal-focused and narrow annotation approach: [Top-down annotation](#top-down-annotation). **The \"as-you-go\" annotation approach may also be applied in a narrower sense**, especially by study groups that are later in their study and who will not apply the \"as-you-go\" annotation approach in the broadest sense. This implies that studies will consider the whole packaging overview process and complete items as they can, as opposed to waiting until the very end (for example, when they are about to submit a study manuscript for peer review) to start the process. Some examples include, 1) auditing study files for tabular data files, creating data dictionaries for existing tabular data files right away, and creating data dictionaries right away for new tabular data files as the study collects or produces them, 2) annotating final result products the study group knows will or likely will be included in a final manuscript as they are produced, and creating a results tracker for final manuscript documents as drafts begin to be forumulated by the study group, 3) annotating component experiments and other activities that are part of the study right away if already designed, or as soon as they are designed (especially if it is clear that the experiment or activity will or likely will produce data that will be used to support/produce final result products that will be included in a final manuscript).","title":"As-you-go annotation"},{"location":"terms/#top-down-annotation","text":"Top-down annotation implies that you will generally implement the data packaging and annotation process in a somewhat narrower, more goal-oriented manner as compared to how you would implement if using the alternative As-you-go annotation approach; You will generally determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation), then wait until your study has produced the goal sharing product (i.e. respectively, the final result product(s) or manuscripts containing a set of final result product(s) for which your data sharing will provide support, or the final dataset your study is interested in sharing/disseminating); you will then audit and annotate the subset of study resources required to interpret, use, and/or reproduce the results or dataset already created, and annotate this subset of study resources right away; when you take the \"top down\" annotation approach, you will generally audit the full subset of study resources required to support your result(s) and/or dataset regardless of whether these resources will ultimately be shared at a public data repository, however you may choose to 1) annotate all resources in this subset regardless of whether they will be shared in a public repository ( \"wholistic\" annotation ), or 2) annotate only the resources in this subset that will be shared in a public repository ( \"minimal\" annotation ) See the alternative: As-you-go Annotation As you move through the data packaging process steps , \"top-down\" annotation implies that you will : Audit relevant files start this right away : determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) wait until your study has produced the goal sharing product (i.e. respectively, the final result product(s) or manuscripts containing a set of final result product(s) for which your data sharing will provide support, or the final dataset your study is interested in sharing/disseminating) audit the full subset of study resources required to interpret, use, and/or reproduce the goal sharing product(s) (i.e. final study result product(s)/manuscipt(s) or dataset) Organize and consistently name your files and folders start this right away : determine if you are early in your study if you are early in your study: consider using HEAL recommendations for organizing and naming study files/resources to establish naming and organization conventions that should be followed for all study files, including for files that already exist and for new study files/resources as they are collected or produced align file name/organization of existing study files to the conventions you established in the step above - this may require you to change file names and locations for already-existing files (NOTE: do not duplicate files in order to align with name/org conventions as this may lead to confusion down the road as to which copy is the \"source of truth\") if you are late in your study: we generally recommend that you leave file names and organization as is you may use the approach laid out above for those early in their study, however if you are late in your study and have accumulated many study files already, establishing file naming and organization conventions now and back-applying them may be quite burdensome and potentially prone to error if you have sets of \"like\" files (e.g. a similarly formatted tabular data file or brain imaging file per study subject per study timepoint), it may be well worth establishing file naming and organization conventions based on HEAL recommendations for organizing and naming study files/resources now and back-applying them just for these file sets - doing so will make it possible/easier to annotate these file sets in one go instead of annotating them singly one at a time, and so may substantially reduce annotation/data-sharing burden for the study group Add Standard data package metadata - File-level Data Dictionary start this right away : determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) wait until your study has produced the goal sharing product (i.e. respectively, the final result product(s) or manuscripts containing a set of final result product(s) for which your data sharing will provide support, or the final dataset your study is interested in sharing/disseminating) audit the full subset of study resources required to interpret, use, and/or reproduce the goal sharing product(s) (i.e. final study result product(s)/manuscipt(s) or dataset) among this subset of files, identify any tabular data files create a data dictionary for these tabular data files right away Results Tracker start this right away : determine the data-sharing orientation(s) or goal(s) of your study right away (results-support orientation and/or dataset-sharing orientation) wait until your study is at the point of producing final result products (e.g. figures, tables, text statements) once your study has reached this point, and if your study has determined that a data sharing goal of the study is to provide results-support , start annotating final result products right away that are likely to be included in a final manuscript or report you will share and for which you would like to provide results-support, even if you have not formally begun to add them to a publication or report as new final result products are produced by your study, annotate each final result product right away once your study has started to formally add final results to a publication, start adding result annotation files you've already produced (in the step just above) to a results tracker document for the publication - one results tracker per publication as new publications are produced by your study, use the result annotation files you create for each final result product in the publication or report to create a results tracker document for the new publication or report alternatively, you may wait until your study is at the point of a final manuscript or report for which you would like to provide results-support; then start annotating only the final result products that are positively contained in this publication Add Standard data package metadata - Study-level Experiment Tracker start this right away OR wait until your study has produced the goal sharing product audit all component experiments or other activities that are or will be part of your study identify the subset of component experiments or other activities that positively produced or are likely to produce data or other products that are required to interpret, use, or reproduce your goal data sharing product (i.e. final result product(s), manuscripts, or a dataset you want to share/disseminate) start annotating this subset of study component experiments or activities if they have already been designed if/when any new study component experiments or activities are designed and it is determined that the new experiment/activity produced or is likely to produce data or other products that are required to interpret, use, or reproduce your goal data sharing product, annotate this new study component experiment(s) or activity(s) right away Resource Tracker wait until your study has produced the goal sharing product (i.e. final result product(s), manuscripts, or a dataset you want to share/disseminate) audit the full subset of study resources required to interpret, use, and/or reproduce the goal sharing product(s) (i.e. final study result product(s)/manuscipt(s) or dataset) (see step 1 above) starting with your goal data sharing product (i.e. manuscript, or a dataset you want to share/disseminate), systematically annotate either : all files/resources in this subset of study resources regardless of whether they will be shared in a public data repository ( \"wholistic\" annotation ) only the files/resources in this subset of study resources that will be shared in a public data repository ( \"minimal\" annotation ) once you've annotated all files/resources, add these resource annotation files to your resource tracker Advantages \"Top-down\" annotation allows for a narrower and more goal-focused approach to annotation and data-sharing as compared to the alternative \"As-you-go\" annotation; This approach may be very appealing to study groups that are well into their study as it reduces annotation and data sharing burden by focusing on producing the essential annotation needed to ensure that the results or datasets they share are discoverable, interpretable, reusable, and replicable by researchers and other potential collaborators. Concentrates annotation and data packaging work at the end of the study which allows singular focus on the task and may speed the process. If you choose to use \"wholistic\" annotation with the \"top-down\" annotation approach, you get the benefit of full local annotation of the files underlying your study's goal data sharing product (manuscript results or shared dataset), which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Even if you choose to use \"minimal\" annotation with the \"top down\" approach you still get the benefit of some local annotation to support preserving institutional knowledge within your study group. If you choose to use \"wholistic\" annotation with the \"top-down\" annotation approach, you maximize transparency and allow other researchers interested in the data to understand the full scope of the project and the data that explicitly underlies your study's goal data sharing product (manuscript results or shared dataset) when accessing study documentation. Even if you choose to use \"minimal\" annotation with the \"top down\" approach you still provide a lot of transparency about what data and non-data supporting files you are sharing to support your study's goal data sharing product, how these files relate to each other and how to use and interpret these files; this presents a huge value to potential secondary data users or collaborators. If you choose to use \"wholistic\" annotation with the \"top-down\" annotation approach, this still allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility, interpretability and use of the study's goal data sharing product (manuscript results or shared dataset) and can perhaps be requested directly from the study team by another researcher. If you choose to use \"wholistic\" annotation with the \"top-down\" annotation approach, this allows for documenting and sharing all metadata associated with the study's goal data sharing product (manuscript results or shared dataset) and this can increase the discoverability of your study. Caveats Placeholder Placeholder","title":"Top-down annotation"},{"location":"terms/#data-package","text":"Placeholder","title":"Data Package"},{"location":"terms/#shareable-data-package","text":"Placeholder","title":"Shareable Data Package"},{"location":"terms/#data-sharing-orientation","text":"A study group's goal(s) for data-sharing. People will have different goals in sharing data and non-data supporting files/resources from their study. These goals, or 'orientations' may be dictated by a number of factors such as the nature of the data, study staff resources and time constraints, investigator preference, and requirements imposed by a funder, journal, or other entity somehow governing the study. Determining what your goals are for data sharing will help guide you in determining what you will include in your data package and how and when you will annotate the data and non-data supporting files/resources in your data package. There are two main data-sharing \"orientations.\" These data sharing goals or \"orientations\" are not mutually exclusive. You may choose both Results-support and Dataset-sharing as goals for your study's data sharing goal.","title":"Data-sharing Orientation"},{"location":"terms/#results-support","text":"The study group wants to share data and non-data supporting files specifically to support results shared in a published manuscript or other venue (e.g. presentation, poster, report, etc.) They will share items required to interpret, replicate, or use these results","title":"Results-support"},{"location":"terms/#dataset-sharing","text":"The study group wants to share a specific dataset(s); perhaps the group has collected/created a very rich dataset and used this dataset to ask and publish results related to specific scientific questions; they believe other study groups may be able to leverage this dataset to ask and publish results related to other scientific questions that may be related or unrelated to the questions the dataset was originally collected to help investigate The study group wants to share this specific dataset(s), as well as the data and non-data supporting files specifically to support use of this dataset(s) They will share items required to interpret, replicate, or use this dataset(s)","title":"Dataset-sharing"},{"location":"terms/addtop/","text":"Add-as-you-go annotation of study artefacts \u00b6 This form of annotation involves tracking and annotating study data and non-data/supporting files as you move through the study. This type of annotation works best when paired with wholistic annotation . Add-as-you-go annotation is built into the study process and so does not require dedicating a significant time at study end to retrospectively gather files and annotate. If you are fairly early in your study and have collected minimal or no data, \u2018add-as-you-go\u2019 annotation is recommended. Top-down annotation of study artefacts \u00b6 This form of annotation involves tracking and annotating study data and non-data/supporting files at the end of your study, retrospectively. This type of annotation works best when paired with minimal annotation , however top-down annotation can also be wholistic . The decision of wholistic vs. minimal annotation may be determined by the amount of time and resources your study has set aside for data sharing, as wholistic annotation would be more time consuming in a top-down orientation. If you are later in your study and have collected almost all or all of your data, \u2018top-down\u2019 annotation is recommended.","title":"Addtop"},{"location":"terms/addtop/#add-as-you-go-annotation-of-study-artefacts","text":"This form of annotation involves tracking and annotating study data and non-data/supporting files as you move through the study. This type of annotation works best when paired with wholistic annotation . Add-as-you-go annotation is built into the study process and so does not require dedicating a significant time at study end to retrospectively gather files and annotate. If you are fairly early in your study and have collected minimal or no data, \u2018add-as-you-go\u2019 annotation is recommended.","title":"Add-as-you-go annotation of study artefacts"},{"location":"terms/addtop/#top-down-annotation-of-study-artefacts","text":"This form of annotation involves tracking and annotating study data and non-data/supporting files at the end of your study, retrospectively. This type of annotation works best when paired with minimal annotation , however top-down annotation can also be wholistic . The decision of wholistic vs. minimal annotation may be determined by the amount of time and resources your study has set aside for data sharing, as wholistic annotation would be more time consuming in a top-down orientation. If you are later in your study and have collected almost all or all of your data, \u2018top-down\u2019 annotation is recommended.","title":"Top-down annotation of study artefacts"},{"location":"terms/depend/","text":"'One Layer Deep' Dependencies \u00b6 Annotation using 'one-layer-deep' dependencies means only documenting the immediate dependencies needed to produce/interpret/use the artefact. If you are using wholistic annotation, you will use the 'one-layer-deep' approach while documenting dependencies. Each row/entry of the resource tracker will only have one dependency (the immediate file/document needed to create the file). The next row will list that dependency and document its dependencies, and so on, until you are annotating files with not dependencies. Given that wholistic annotation inolves documenting all files, this will allow researchers to trace the order of dependencies in a cascade to the original source files. THis will ease the ability to understand and reproduce results. 'Liberal' Dependencies \u00b6 Annotation using \u2018liberal\u2019 dependencies means documenting all dependencies, immediate and distal, needed to produce/interpret/use each artefact. If you are using minimal annotation, you will use the \u2018liberal\u2019 approach when documenting dependencies. Each row/entry of the resource tracker will include immediate dependencies (as in the \u2018one-layer-deep\u2019 approach) as well as the dependencies of those dependencies. Given that minimal annotation means that you are not documenting all study artefacts, this will allow researchers to understand the cascade of dependencies involved in producing each artefact, even if not all dependencies are entries in the resource tracker.","title":"Depend"},{"location":"terms/depend/#one-layer-deep-dependencies","text":"Annotation using 'one-layer-deep' dependencies means only documenting the immediate dependencies needed to produce/interpret/use the artefact. If you are using wholistic annotation, you will use the 'one-layer-deep' approach while documenting dependencies. Each row/entry of the resource tracker will only have one dependency (the immediate file/document needed to create the file). The next row will list that dependency and document its dependencies, and so on, until you are annotating files with not dependencies. Given that wholistic annotation inolves documenting all files, this will allow researchers to trace the order of dependencies in a cascade to the original source files. THis will ease the ability to understand and reproduce results.","title":"'One Layer Deep' Dependencies"},{"location":"terms/depend/#liberal-dependencies","text":"Annotation using \u2018liberal\u2019 dependencies means documenting all dependencies, immediate and distal, needed to produce/interpret/use each artefact. If you are using minimal annotation, you will use the \u2018liberal\u2019 approach when documenting dependencies. Each row/entry of the resource tracker will include immediate dependencies (as in the \u2018one-layer-deep\u2019 approach) as well as the dependencies of those dependencies. Given that minimal annotation means that you are not documenting all study artefacts, this will allow researchers to understand the cascade of dependencies involved in producing each artefact, even if not all dependencies are entries in the resource tracker.","title":"'Liberal' Dependencies"},{"location":"terms/filelevel/","text":"Results Tracker \u00b6 The results tracker provider detailed information and annotation for a specific publication, report or presentation. Each publication or report will have its own results tracker. The purpose of this tracker is to track and annotate each single result (e.g., a figure or textual statement) within each publication as well as the data and supporting files upone which each single result depends. Within the result tracker, each row corresponds to one result from the publication. Information eabout each result includes type of result (figure or text), description, and supported claims. Each results tracker should additionally be listed and annotated as its own row in the resource tracker . For detailed information on the name and definition of each field in the results tracker, please refer to the [LINK: results tracker schema]. Associated Files/Dependencies \u00b6 Any file that the study artefact being annotated depends on to be produced, interpreted, or used (e.g. a processed tabular data file may depend on a raw data file(s), plus a code file that implements the statistical analysis plan to be produced, and require a data dictionary to interpret/use) How you annotate the dependencies of a file in the result tracker will depend on whether you are annotating wholistically or minimally. If you are annotating artefacts wholistically Use the 'one layer deep' approach : list only the immediate dependencies needed to produce/interpret/use the result. List each dependency in the result tracker in its own row/entry For each row, list dependencies of that file one layer deep Continue until you are annotating files with no dependencies If you are annotating artefacts minimally Use the 'liberal' approach : list all dependencies. For a result, list immediate dependencies needed to produce/interpret/use the artefact plus dependencies of those dependencies Continue until you are listing files with no dependencies mkdo Example \u00b6 A processed tabular data file may depend on a raw data file(s), plus a code file that implements the statistical analysis plan to be produced, and require a data dictionary to interpret/use. For more information on 'one layer deep' and 'liberal' dependency documentation, click here. 'One layer deep' Dependencies Liberal Dependencies Immediately underlying raw data files, plus a code file that implements the statistical analysis plan and 'transforms' the raw data files into the processed data file, plus the data dictionary for the processed data file All immediate/one layer deep files, AND The statistical analysis plan that was used to produce the code, the data dictionary for the raw underlying data file(s), a protocol for collection of the raw underlying data file(s), etc. If possible, list liberal dependencies in order from most immediate to least immediate. Data Dictionary \u00b6 Each tabular data file should have its own data dictionary. The data dictionary describes the content, format, and structure of the data file. Within the data dictionary, each row contains information about each variable such as the name, description, type, format, and encodings. The data dictionary is a critical piece in re-use of the data by future researchers, as it contains information about how to read, format, and interpret the data. For detailed information on the name and definition of each field in the data dictionary, please refer to the [LINK: data dictionary schema]. Helpful tip If you are creating data dictionaries, save the data dictionary with the same filename as the data file, with a \"dd-\" prefix. Example: dd-my-data-1.csv","title":"Filelevel"},{"location":"terms/filelevel/#results-tracker","text":"The results tracker provider detailed information and annotation for a specific publication, report or presentation. Each publication or report will have its own results tracker. The purpose of this tracker is to track and annotate each single result (e.g., a figure or textual statement) within each publication as well as the data and supporting files upone which each single result depends. Within the result tracker, each row corresponds to one result from the publication. Information eabout each result includes type of result (figure or text), description, and supported claims. Each results tracker should additionally be listed and annotated as its own row in the resource tracker . For detailed information on the name and definition of each field in the results tracker, please refer to the [LINK: results tracker schema].","title":"Results Tracker"},{"location":"terms/filelevel/#associated-filesdependencies","text":"Any file that the study artefact being annotated depends on to be produced, interpreted, or used (e.g. a processed tabular data file may depend on a raw data file(s), plus a code file that implements the statistical analysis plan to be produced, and require a data dictionary to interpret/use) How you annotate the dependencies of a file in the result tracker will depend on whether you are annotating wholistically or minimally. If you are annotating artefacts wholistically Use the 'one layer deep' approach : list only the immediate dependencies needed to produce/interpret/use the result. List each dependency in the result tracker in its own row/entry For each row, list dependencies of that file one layer deep Continue until you are annotating files with no dependencies If you are annotating artefacts minimally Use the 'liberal' approach : list all dependencies. For a result, list immediate dependencies needed to produce/interpret/use the artefact plus dependencies of those dependencies Continue until you are listing files with no dependencies mkdo","title":"Associated Files/Dependencies"},{"location":"terms/filelevel/#example","text":"A processed tabular data file may depend on a raw data file(s), plus a code file that implements the statistical analysis plan to be produced, and require a data dictionary to interpret/use. For more information on 'one layer deep' and 'liberal' dependency documentation, click here. 'One layer deep' Dependencies Liberal Dependencies Immediately underlying raw data files, plus a code file that implements the statistical analysis plan and 'transforms' the raw data files into the processed data file, plus the data dictionary for the processed data file All immediate/one layer deep files, AND The statistical analysis plan that was used to produce the code, the data dictionary for the raw underlying data file(s), a protocol for collection of the raw underlying data file(s), etc. If possible, list liberal dependencies in order from most immediate to least immediate.","title":"Example"},{"location":"terms/filelevel/#data-dictionary","text":"Each tabular data file should have its own data dictionary. The data dictionary describes the content, format, and structure of the data file. Within the data dictionary, each row contains information about each variable such as the name, description, type, format, and encodings. The data dictionary is a critical piece in re-use of the data by future researchers, as it contains information about how to read, format, and interpret the data. For detailed information on the name and definition of each field in the data dictionary, please refer to the [LINK: data dictionary schema]. Helpful tip If you are creating data dictionaries, save the data dictionary with the same filename as the data file, with a \"dd-\" prefix. Example: dd-my-data-1.csv","title":"Data Dictionary"},{"location":"terms/files/","text":"Tabular data file \u00b6 A data file that is organized in a table with rows and columns.","title":"Files"},{"location":"terms/files/#tabular-data-file","text":"A data file that is organized in a table with rows and columns.","title":"Tabular data file"},{"location":"terms/holmin/","text":"Wholistic annotation of study artefacts \u00b6 Wholistic annotation involves creating and maintaining documentation that catalogues and annotates all files and non-data/supporting documents generated by and associated with your study regardless of whether you are planning to share them. Files that will not be shared are documented as permanent-private. This type of annotation works best paired with ['add as you go' annotation], as it easier an less time consuming to annotate wholistically, retrospectively, as part of ['top-down' annotation]. Although this mode of annotation is more time consuming, it maximizes transparency and allows investigators interested in the data to understand the full scope of the project when accessing study documentation on the Platform. This structure also allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by an investigator. Who may choose to share this way Study groups that have not started collecting data or are very early in the data collection process. Study groups that are earlier in the process and are interested in understanding and implementing a file and folder structure that facilitates data sharing in the future. Study groups that want to maximise the amount of information that they share about their study. Pros to sharing this way You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Although more time consuming at the beginning, integration of this process into your workflows allows for documentation and annotation in parts as you move through the study, so that you do not need to compile all that information at the end of the study, retrospectively. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Cons to sharing this way More time consuming, because it requires you to set up the structures to fully catalogue all data and non-data/supporting files that are relevant to your study. Notes about wholistic annotation If you are early in your study and choose to pursue wholistic annotation, we recommend you consider reviewing and implementing [HEAL recommendations for organizing and naming of study artefacts]. This will ease the process of annotation as well as future data sharing. If you are annotating wholistically, you will also document dependencies 'one-layer-deep'. For more information, see [Resource Tracker: Associated Files/Dependencies] and ['One Layer Deep' Dependencies]. Minimal annotation of study artefacts \u00b6 Minimal annotation starts with the main file you are focused on sharing, whether that is a publication, report or a dataset, and works backwards. Rather than documenting all files, you document only the data and non-data file dependencies required to reproduce the final results or dataset. This is likely more ideal for studies that may be close to data collection completion and are not focused on meeting data sharing requirements but do not have adequate funding or time to devote the complete documentation of the data. This type of annotation pairs well with \u2018top-down\u2019 annotation, which retrospectively annotates study artefacts. Who may choose to share this way Study groups that may be finished or close to finished collecting data and have already produced results files (e.g., figures, draft publications/NIH reports, etc.) Study groups that want to meet minimal data sharing requirements of sharing data underlying published results Pros to sharing this way You only catalog the data and non-data/supporting files that you will share/submit to a repository. This is less work than fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), especially if the study is well underway or complete/nearing completion and/or does not have resources or time set aside for a complete file inventory. This approach allows you to fulfill the minimal data sharing requirements of sharing data underlying published results. Cons to sharing this way You don\u2019t get the full local annotation benefit that would come with fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), and how they relate to each other and to published results \u2013 these benefits include facilitating continuity and passed-down knowledge within study groups , and discovery, sharing, and re-use of the data and knowledge produced by the study outside of the original study group. You don\u2019t get the full benefit of the added discoverability that data-package level metadata can provide. Notes about minimal annotation Even if you are not early in your study and are pursuing minimal annotation, we do not recommend reorganizing all your files or folder structures. However, we do recommend you consider reviewing and implementing naming conventions for like files, detailed in [HEAL recommendations for organizing and naming study artefacts]. This will ease the process of annotation. If you are annotating minimally, you will document dependences 'liberally'. For more information, see [Resource Tracker: Associated Files/Dependencies] adn ['Liberal' Dependencies].","title":"Holmin"},{"location":"terms/holmin/#wholistic-annotation-of-study-artefacts","text":"Wholistic annotation involves creating and maintaining documentation that catalogues and annotates all files and non-data/supporting documents generated by and associated with your study regardless of whether you are planning to share them. Files that will not be shared are documented as permanent-private. This type of annotation works best paired with ['add as you go' annotation], as it easier an less time consuming to annotate wholistically, retrospectively, as part of ['top-down' annotation]. Although this mode of annotation is more time consuming, it maximizes transparency and allows investigators interested in the data to understand the full scope of the project when accessing study documentation on the Platform. This structure also allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by an investigator. Who may choose to share this way Study groups that have not started collecting data or are very early in the data collection process. Study groups that are earlier in the process and are interested in understanding and implementing a file and folder structure that facilitates data sharing in the future. Study groups that want to maximise the amount of information that they share about their study. Pros to sharing this way You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Although more time consuming at the beginning, integration of this process into your workflows allows for documentation and annotation in parts as you move through the study, so that you do not need to compile all that information at the end of the study, retrospectively. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Cons to sharing this way More time consuming, because it requires you to set up the structures to fully catalogue all data and non-data/supporting files that are relevant to your study. Notes about wholistic annotation If you are early in your study and choose to pursue wholistic annotation, we recommend you consider reviewing and implementing [HEAL recommendations for organizing and naming of study artefacts]. This will ease the process of annotation as well as future data sharing. If you are annotating wholistically, you will also document dependencies 'one-layer-deep'. For more information, see [Resource Tracker: Associated Files/Dependencies] and ['One Layer Deep' Dependencies].","title":"Wholistic annotation of study artefacts"},{"location":"terms/holmin/#minimal-annotation-of-study-artefacts","text":"Minimal annotation starts with the main file you are focused on sharing, whether that is a publication, report or a dataset, and works backwards. Rather than documenting all files, you document only the data and non-data file dependencies required to reproduce the final results or dataset. This is likely more ideal for studies that may be close to data collection completion and are not focused on meeting data sharing requirements but do not have adequate funding or time to devote the complete documentation of the data. This type of annotation pairs well with \u2018top-down\u2019 annotation, which retrospectively annotates study artefacts. Who may choose to share this way Study groups that may be finished or close to finished collecting data and have already produced results files (e.g., figures, draft publications/NIH reports, etc.) Study groups that want to meet minimal data sharing requirements of sharing data underlying published results Pros to sharing this way You only catalog the data and non-data/supporting files that you will share/submit to a repository. This is less work than fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), especially if the study is well underway or complete/nearing completion and/or does not have resources or time set aside for a complete file inventory. This approach allows you to fulfill the minimal data sharing requirements of sharing data underlying published results. Cons to sharing this way You don\u2019t get the full local annotation benefit that would come with fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), and how they relate to each other and to published results \u2013 these benefits include facilitating continuity and passed-down knowledge within study groups , and discovery, sharing, and re-use of the data and knowledge produced by the study outside of the original study group. You don\u2019t get the full benefit of the added discoverability that data-package level metadata can provide. Notes about minimal annotation Even if you are not early in your study and are pursuing minimal annotation, we do not recommend reorganizing all your files or folder structures. However, we do recommend you consider reviewing and implementing naming conventions for like files, detailed in [HEAL recommendations for organizing and naming study artefacts]. This will ease the process of annotation. If you are annotating minimally, you will document dependences 'liberally'. For more information, see [Resource Tracker: Associated Files/Dependencies] adn ['Liberal' Dependencies].","title":"Minimal annotation of study artefacts"},{"location":"terms/index-archive/","text":"Tabular data file \u00b6 A data file that is organized in a table with rows and columns. Study resources \u00b6 All data and non-data supporting files/documents/resources generated by and associated with your study, regardless of whether you are planning to share them. Standard data package metadata files \u00b6 Standard metadata file types that, altogether, provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. These metadata files should be included in all data packages. NOTE : All standard data package metadata files have a standardized csv format in which they should be completed and provided. See here for [csv templates] and [field definitions] to aid you in completing the templates. Standard data package metadata - Study-level \u00b6 Experiment Tracker \u00b6 one per study; an inventory of experiments or activities included in the study; for a clinical trial, this may be simply one experiment equal to the registered clinical trial activity; for a basic biology study, this may be a listing of several orthogonal experiments used altogether to address and advance the study aims, Resource Tracker \u00b6 one per study; an inventory of all data and non-data supporting files produced during the course of the study (or, in some cases, only those which will be shared in a public data repository), including a description of what is in the file or what the file represents, file relationships and dependencies, and whether/how each file is shareable in a public repository or not Standard data package metadata - File-level \u00b6 Data Dictionary \u00b6 one per tabular data file; an inventory of variables included in a tabular data file Results Tracker \u00b6 one per publication or report; an inventory of figure, table, and text results included in a publication or report Experiment Tracker \u00b6 The experiment tracker provides contextual information on the experiments that are involved in the study. Complete one per study. The tracker is a csv file, where each row of the experiment tracker corresponds to one component experiment or activity included in the study. Information about the experiment includes research questions(s), approach, and hypotheses. For detailed information on the name and definition of each field in the experiment tracker, please refer to the [experiment tracker schema]. NOTE : For a clinical trial, this may be simply a listing of one experiment equal to the registered clinical trial activity; For a basic biology study, this may be a listing of several orthogonal experiments used altogether to address and advance the study aims. Study-level Metadata: Resource Tracker \u00b6 The resource tracker is an inventory and annotate list of all data and supporting/non-date files for the study. Each row of the resource tracker corresponds to one data or non-data resource. Informaiton in the tracker about each resource includes file path, description, access restrictions, format, corresponding software, and dependencies. The resource tracker is where you will also list and annotate any associated any associated results trackers docummenting publications or reports. See [Resource Tracker: Associated Files/Dependencies] for more information on how you should document your dependencies, depending on the type and amount of data you are sharing. For detailed information on the name and definition of each field in the resource tracker, please refer to the [resource tracker schema]. File-level Metadata: Data Dictionary \u00b6 File-level Metadata: Results Tracker \u00b6 The results tracker provides detailed information and annotation for publications, reports or presentations. Each publication will have its own results tracker. The purpose of this tracker is to track and annotate each single result (e.g., a figure or textual statement) within each multiresult file as well as all the data and supporting files upon which each single result depends. Within the results tracker, each row corresponds to one result from the publication. Information about each result includes type of result (figure or text), description, and supported claims. Note Each results tracker should be additionally listed and annotated as its own row in the resource tracker. For detailed information on the name and defiition of each field in the results tracker, please refer to the [results tracker schema]. Wholistic annotation of study artefacts \u00b6 Wholistic annotation involves creating and maintaining documentation that catalogues and annotates all files and non-data/supporting documents generated by and associated with your study regardless of whether you are planning to share them. Files that will not be shared are documented as permanent-private. This type of annotation works best paired with ['add as you go' annotation], as it easier an less time consuming to annotate wholistically, retrospectively, as part of ['top-down' annotation]. Although this mode of annotation is more time consuming, it maximizes transparency and allows investigators interested in the data to understand the full scope of the project when accessing study documentation on the Platform. This structure also allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by an investigator. Who may choose to share this way Study groups that have not started collecting data or are very early in the data collection process. Study groups that are earlier in the process and are interested in understanding and implementing a file and folder structure that facilitates data sharing in the future. Study groups that want to maximise the amount of information that they share about their study. Pros to sharing this way You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Although more time consuming at the beginning, integration of this process into your workflows allows for documentation and annotation in parts as you move through the study, so that you do not need to compile all that information at the end of the study, retrospectively. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Cons to sharing this way More time consuming, because it requires you to set up the structures to fully catalogue all data and non-data/supporting files that are relevant to your study. Notes about wholistic annotation If you are early in your study and choose to pursue wholistic annotation, we recommend you consider reviewing and implementing [HEAL recommendations for organizing and naming of study artefacts]. This will ease the process of annotation as well as future data sharing. If you are annotating wholistically, you will also document dependencies 'one-layer-deep'. For more information, see [Resource Tracker: Associated Files/Dependencies] and ['One Layer Deep' Dependencies]. Minimal annotation of study artefacts \u00b6 Minimal annotation starts with the main file you are focused on sharing, whether that is a publication, report, or a dataset, and works backwards. Rather than documenting all files, you document only the data and non-data file dependencies required to reproduce the final results or dataset. This is likely more ideal for studies that may be close to data collection completion and are not focused on meeting data sharing requirements but do not have adequate funding or time to devote the complete documentation of the data. This type of annotation pairs well with \u2018top-down\u2019 annotation, which retrospectively annotates study artefacts. Who may choose to share this way Study groups that may be finished or close to finished collecting data and have already produced results files (e.g., figures, draft publications/NIH reports, etc.) Study groups that want to meet minimal data sharing requirements of sharing data underlying published results Pros to sharing this way You only catalog the data and non-data/supporting files that you will share/submit to a repository. This is less work than fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), especially if the study is well underway or complete/nearing completion and/or does not have resources or time set aside for a complete file inventory. This approach allows you to fulfill the minimal data sharing requirements of sharing data underlying published results. Cons to sharing this way You don\u2019t get the full local annotation benefit that would come with fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), and how they relate to each other and to published results \u2013 these benefits include facilitating continuity and passed-down knowledge within study groups , and discovery, sharing, and re-use of the data and knowledge produced by the study outside of the original study group. You don\u2019t get the full benefit of the added discoverability that data-package level metadata can provide. Notes about minimal annotation Even if you are not early in your study and are pursuing minimal annotation, we do not recommend reorganizing all your files or folder structures. However, we do recommend you consider reviewing and implementing naming conventions for like files, detailed in [HEAL recommendations for organizing and naming study artefacts]. This will ease the process of annotation. If you are annotating minimally, you will document dependences 'liberally'. For more information, see [Resource Tracker: Associated Files/Dependencies] adn ['Liberal' Dependencies].","title":"Index archive"},{"location":"terms/index-archive/#tabular-data-file","text":"A data file that is organized in a table with rows and columns.","title":"Tabular data file"},{"location":"terms/index-archive/#study-resources","text":"All data and non-data supporting files/documents/resources generated by and associated with your study, regardless of whether you are planning to share them.","title":"Study resources"},{"location":"terms/index-archive/#standard-data-package-metadata-files","text":"Standard metadata file types that, altogether, provide essential usability and context information about the study as a whole and about the data files your study has produced/collected. These metadata files should be included in all data packages. NOTE : All standard data package metadata files have a standardized csv format in which they should be completed and provided. See here for [csv templates] and [field definitions] to aid you in completing the templates.","title":"Standard data package metadata files"},{"location":"terms/index-archive/#standard-data-package-metadata-study-level","text":"","title":"Standard data package metadata - Study-level"},{"location":"terms/index-archive/#experiment-tracker","text":"one per study; an inventory of experiments or activities included in the study; for a clinical trial, this may be simply one experiment equal to the registered clinical trial activity; for a basic biology study, this may be a listing of several orthogonal experiments used altogether to address and advance the study aims,","title":"Experiment Tracker"},{"location":"terms/index-archive/#resource-tracker","text":"one per study; an inventory of all data and non-data supporting files produced during the course of the study (or, in some cases, only those which will be shared in a public data repository), including a description of what is in the file or what the file represents, file relationships and dependencies, and whether/how each file is shareable in a public repository or not","title":"Resource Tracker"},{"location":"terms/index-archive/#standard-data-package-metadata-file-level","text":"","title":"Standard data package metadata - File-level"},{"location":"terms/index-archive/#data-dictionary","text":"one per tabular data file; an inventory of variables included in a tabular data file","title":"Data Dictionary"},{"location":"terms/index-archive/#results-tracker","text":"one per publication or report; an inventory of figure, table, and text results included in a publication or report","title":"Results Tracker"},{"location":"terms/index-archive/#experiment-tracker_1","text":"The experiment tracker provides contextual information on the experiments that are involved in the study. Complete one per study. The tracker is a csv file, where each row of the experiment tracker corresponds to one component experiment or activity included in the study. Information about the experiment includes research questions(s), approach, and hypotheses. For detailed information on the name and definition of each field in the experiment tracker, please refer to the [experiment tracker schema]. NOTE : For a clinical trial, this may be simply a listing of one experiment equal to the registered clinical trial activity; For a basic biology study, this may be a listing of several orthogonal experiments used altogether to address and advance the study aims.","title":"Experiment Tracker"},{"location":"terms/index-archive/#study-level-metadata-resource-tracker","text":"The resource tracker is an inventory and annotate list of all data and supporting/non-date files for the study. Each row of the resource tracker corresponds to one data or non-data resource. Informaiton in the tracker about each resource includes file path, description, access restrictions, format, corresponding software, and dependencies. The resource tracker is where you will also list and annotate any associated any associated results trackers docummenting publications or reports. See [Resource Tracker: Associated Files/Dependencies] for more information on how you should document your dependencies, depending on the type and amount of data you are sharing. For detailed information on the name and definition of each field in the resource tracker, please refer to the [resource tracker schema].","title":"Study-level Metadata: Resource Tracker"},{"location":"terms/index-archive/#file-level-metadata-data-dictionary","text":"","title":"File-level Metadata: Data Dictionary"},{"location":"terms/index-archive/#file-level-metadata-results-tracker","text":"The results tracker provides detailed information and annotation for publications, reports or presentations. Each publication will have its own results tracker. The purpose of this tracker is to track and annotate each single result (e.g., a figure or textual statement) within each multiresult file as well as all the data and supporting files upon which each single result depends. Within the results tracker, each row corresponds to one result from the publication. Information about each result includes type of result (figure or text), description, and supported claims. Note Each results tracker should be additionally listed and annotated as its own row in the resource tracker. For detailed information on the name and defiition of each field in the results tracker, please refer to the [results tracker schema].","title":"File-level Metadata: Results Tracker"},{"location":"terms/index-archive/#wholistic-annotation-of-study-artefacts","text":"Wholistic annotation involves creating and maintaining documentation that catalogues and annotates all files and non-data/supporting documents generated by and associated with your study regardless of whether you are planning to share them. Files that will not be shared are documented as permanent-private. This type of annotation works best paired with ['add as you go' annotation], as it easier an less time consuming to annotate wholistically, retrospectively, as part of ['top-down' annotation]. Although this mode of annotation is more time consuming, it maximizes transparency and allows investigators interested in the data to understand the full scope of the project when accessing study documentation on the Platform. This structure also allows for documentation of the existence and disposition of files that are too sensitive to share but are important for reproducibility and can perhaps be requested directly from the study team by an investigator. Who may choose to share this way Study groups that have not started collecting data or are very early in the data collection process. Study groups that are earlier in the process and are interested in understanding and implementing a file and folder structure that facilitates data sharing in the future. Study groups that want to maximise the amount of information that they share about their study. Pros to sharing this way You get the benefit of full local annotation, which not only maximizes the usefulness of your data for other investigators but also can be helpful internally, especially in preserving knowledge about the data even as team members may change over the course of the study. Although more time consuming at the beginning, integration of this process into your workflows allows for documentation and annotation in parts as you move through the study, so that you do not need to compile all that information at the end of the study, retrospectively. Documenting and sharing all metadata associated with your study can increase the discoverability of your study. Cons to sharing this way More time consuming, because it requires you to set up the structures to fully catalogue all data and non-data/supporting files that are relevant to your study. Notes about wholistic annotation If you are early in your study and choose to pursue wholistic annotation, we recommend you consider reviewing and implementing [HEAL recommendations for organizing and naming of study artefacts]. This will ease the process of annotation as well as future data sharing. If you are annotating wholistically, you will also document dependencies 'one-layer-deep'. For more information, see [Resource Tracker: Associated Files/Dependencies] and ['One Layer Deep' Dependencies].","title":"Wholistic annotation of study artefacts"},{"location":"terms/index-archive/#minimal-annotation-of-study-artefacts","text":"Minimal annotation starts with the main file you are focused on sharing, whether that is a publication, report, or a dataset, and works backwards. Rather than documenting all files, you document only the data and non-data file dependencies required to reproduce the final results or dataset. This is likely more ideal for studies that may be close to data collection completion and are not focused on meeting data sharing requirements but do not have adequate funding or time to devote the complete documentation of the data. This type of annotation pairs well with \u2018top-down\u2019 annotation, which retrospectively annotates study artefacts. Who may choose to share this way Study groups that may be finished or close to finished collecting data and have already produced results files (e.g., figures, draft publications/NIH reports, etc.) Study groups that want to meet minimal data sharing requirements of sharing data underlying published results Pros to sharing this way You only catalog the data and non-data/supporting files that you will share/submit to a repository. This is less work than fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), especially if the study is well underway or complete/nearing completion and/or does not have resources or time set aside for a complete file inventory. This approach allows you to fulfill the minimal data sharing requirements of sharing data underlying published results. Cons to sharing this way You don\u2019t get the full local annotation benefit that would come with fully cataloguing all data and non-data/supporting files relevant to a study (including files you will not share/submit to a repository), and how they relate to each other and to published results \u2013 these benefits include facilitating continuity and passed-down knowledge within study groups , and discovery, sharing, and re-use of the data and knowledge produced by the study outside of the original study group. You don\u2019t get the full benefit of the added discoverability that data-package level metadata can provide. Notes about minimal annotation Even if you are not early in your study and are pursuing minimal annotation, we do not recommend reorganizing all your files or folder structures. However, we do recommend you consider reviewing and implementing naming conventions for like files, detailed in [HEAL recommendations for organizing and naming study artefacts]. This will ease the process of annotation. If you are annotating minimally, you will document dependences 'liberally'. For more information, see [Resource Tracker: Associated Files/Dependencies] adn ['Liberal' Dependencies].","title":"Minimal annotation of study artefacts"},{"location":"terms/name/","text":"Organize study-related files into folders with consistent naming conventions \u00b6 Organize all your files (whether you will share them or not), into folders. In general, minimize the number of folders at each level, and the number of 'layers' of your whole directory. File naming conventions can often be used quite effectively to 'organize' files and to provide clues as to what the files are and how they relate to each other without resorting to using separation into different directories to serve that purpose. However, there is a limit to naming conventions, and you may want to separate your files into directors based on some type of logical groupings and/or based on th elevel or timing of access to the files you expect to provide. See below for further details. Reminder Before creating more directories or directory layers, consider whether file-naming conventions may be used more efficiently to create this organization. Create logical groupings \u00b6 There are a number of ways to group files logically. Below are some examples of how you can logically group your files in folder structures: Grouping strategy Examples By file category Raw data grouped together; processed data grouped together; protocols grouped together; data dictionaries grouped together By experimental subject All raw data files from a particular cell (across all protocols grouped together, etc.) By type of experiment All current clamp experiment files grouped together; all voltage clamp experiment files grouped together By experiment All raw data files mkdocsfrom a particular protocol, across all cells to which the protocol applies, grouped together By file type All CorelDraw figure files grouped together By expected use All files you expect a primary or secondary user of the data/metadata would want to use and/or cite together (as a package) - for example, grouping files that you expect you will want to provide different levels of access to/access restrictions for and/or that you want to provide access to with different timing. See below for more information on access level and timing. Consistently name your files and folders \u00b6 Universal conventions \u00b6 File and folder names should generally be informative/descriptive Wherever possible name your files and folders in a way that will let a human user reasonably infer quite a lot about what the file holds without any further information It is very helpful if file and folder names are all lower case Don't use spaces or special characters Preferred separator is a dash ('-'), e.g., my-data-file-1 Special conventions \u00b6 If you have multiple files or folders of the same type , choose a naming convention that's descriptive, and not unmanageably long, and be consistent. For example: One file per day, per cell, per protocol: Potential convention: \"day\"-YYYY-MM-DD-\"cell\"-[0-9]-\"protocol\"-{short-protocol-description} Example filename: day-2023-03-17-cell-3-protocol-200ms-pulse.asc One folder per experimental objective: Potential convention: {cell-type}-{short-experimental-approach-description}-{short-experimental-objective-description} Example foldrer name: drg-current-clamp-effect-of-K1395R-on-drg-neuronal-excitability; drg-voltage-clamp-biophysical-properties-of-K1395R If you have files that are related , choose a naming convention that's descriptive, and not unmanageably long, and be consistent. For example: A tabular/csv data file that has a data dictionary Potential convention: The data dictionary for a data file has the same filename as the data file, with a \"dd-\" prefix Example file name(s): Data file name : my-data-1.csv Corresponding data dictionary file name : dd-my-data-1.csv Several experiments that each have an experimental protocol Potential convention: The protocol of the experiment has the id of that experiment (from the experiment tracker) in the filename, with a \"protocol-\" prefix Example file name(s): Experiment id (from experiment tracker) : exp-1 Corresponding protocol file name : protocol-exp-1.txt","title":"Name"},{"location":"terms/name/#organize-study-related-files-into-folders-with-consistent-naming-conventions","text":"Organize all your files (whether you will share them or not), into folders. In general, minimize the number of folders at each level, and the number of 'layers' of your whole directory. File naming conventions can often be used quite effectively to 'organize' files and to provide clues as to what the files are and how they relate to each other without resorting to using separation into different directories to serve that purpose. However, there is a limit to naming conventions, and you may want to separate your files into directors based on some type of logical groupings and/or based on th elevel or timing of access to the files you expect to provide. See below for further details. Reminder Before creating more directories or directory layers, consider whether file-naming conventions may be used more efficiently to create this organization.","title":"Organize study-related files into folders with consistent naming conventions"},{"location":"terms/name/#create-logical-groupings","text":"There are a number of ways to group files logically. Below are some examples of how you can logically group your files in folder structures: Grouping strategy Examples By file category Raw data grouped together; processed data grouped together; protocols grouped together; data dictionaries grouped together By experimental subject All raw data files from a particular cell (across all protocols grouped together, etc.) By type of experiment All current clamp experiment files grouped together; all voltage clamp experiment files grouped together By experiment All raw data files mkdocsfrom a particular protocol, across all cells to which the protocol applies, grouped together By file type All CorelDraw figure files grouped together By expected use All files you expect a primary or secondary user of the data/metadata would want to use and/or cite together (as a package) - for example, grouping files that you expect you will want to provide different levels of access to/access restrictions for and/or that you want to provide access to with different timing. See below for more information on access level and timing.","title":"Create logical groupings"},{"location":"terms/name/#consistently-name-your-files-and-folders","text":"","title":"Consistently name your files and folders"},{"location":"terms/name/#universal-conventions","text":"File and folder names should generally be informative/descriptive Wherever possible name your files and folders in a way that will let a human user reasonably infer quite a lot about what the file holds without any further information It is very helpful if file and folder names are all lower case Don't use spaces or special characters Preferred separator is a dash ('-'), e.g., my-data-file-1","title":"Universal conventions"},{"location":"terms/name/#special-conventions","text":"If you have multiple files or folders of the same type , choose a naming convention that's descriptive, and not unmanageably long, and be consistent. For example: One file per day, per cell, per protocol: Potential convention: \"day\"-YYYY-MM-DD-\"cell\"-[0-9]-\"protocol\"-{short-protocol-description} Example filename: day-2023-03-17-cell-3-protocol-200ms-pulse.asc One folder per experimental objective: Potential convention: {cell-type}-{short-experimental-approach-description}-{short-experimental-objective-description} Example foldrer name: drg-current-clamp-effect-of-K1395R-on-drg-neuronal-excitability; drg-voltage-clamp-biophysical-properties-of-K1395R If you have files that are related , choose a naming convention that's descriptive, and not unmanageably long, and be consistent. For example: A tabular/csv data file that has a data dictionary Potential convention: The data dictionary for a data file has the same filename as the data file, with a \"dd-\" prefix Example file name(s): Data file name : my-data-1.csv Corresponding data dictionary file name : dd-my-data-1.csv Several experiments that each have an experimental protocol Potential convention: The protocol of the experiment has the id of that experiment (from the experiment tracker) in the filename, with a \"protocol-\" prefix Example file name(s): Experiment id (from experiment tracker) : exp-1 Corresponding protocol file name : protocol-exp-1.txt","title":"Special conventions"},{"location":"terms/sharing/","text":"Data sharing orientation: Dataset \u00b6 Investigators with this data sharing orientation are interested in sharing their data so that other researchers can get additional value from them and can conduct additional research and analyses beyond what their study investigated. Investigators may want to share as much data as possible from their study, or they may be interested in sharing a specific dataset that they know will have value to others. For example, an investigator may want to share a RNA sequencing data, which is a rich dataset that other investigators could analyze for insights unrelated to the study at hand. Investigators can use any type of annotation with this data sharing orientation. Data sharing orientation: results-support \u00b6 Investigators with this data sharing orientation are interested only in sharing what is necessary to reproduce a specific set of results, generally published in a manuscript. This data sharing orientation may appeal to investigators who have limited or no resources or funding allocated for fulfilling data sharing requirements, as this is the most minimal way to fulfill the HEAL requirements. Investigators may prefer to use minimal annotation with this data sharing orientation.","title":"Sharing"},{"location":"terms/sharing/#data-sharing-orientation-dataset","text":"Investigators with this data sharing orientation are interested in sharing their data so that other researchers can get additional value from them and can conduct additional research and analyses beyond what their study investigated. Investigators may want to share as much data as possible from their study, or they may be interested in sharing a specific dataset that they know will have value to others. For example, an investigator may want to share a RNA sequencing data, which is a rich dataset that other investigators could analyze for insights unrelated to the study at hand. Investigators can use any type of annotation with this data sharing orientation.","title":"Data sharing orientation: Dataset"},{"location":"terms/sharing/#data-sharing-orientation-results-support","text":"Investigators with this data sharing orientation are interested only in sharing what is necessary to reproduce a specific set of results, generally published in a manuscript. This data sharing orientation may appeal to investigators who have limited or no resources or funding allocated for fulfilling data sharing requirements, as this is the most minimal way to fulfill the HEAL requirements. Investigators may prefer to use minimal annotation with this data sharing orientation.","title":"Data sharing orientation: results-support"},{"location":"terms/studylevel/exp/","text":"About the Experiment Tracker \u00b6 The experiment tracker is an inventory and annotated list of all experiments included in the study. It provides contextual information for each experiment, including research questions, approach, and hypotheses.","title":"Exp"},{"location":"terms/studylevel/exp/#about-the-experiment-tracker","text":"The experiment tracker is an inventory and annotated list of all experiments included in the study. It provides contextual information for each experiment, including research questions, approach, and hypotheses.","title":"About the Experiment Tracker"},{"location":"terms/studylevel/resource/","text":"About the Resource Tracker \u00b6 The resource tracker is an inventory and annotated list of all data and supporting/non-data files for the study. Study-Level Metadata \u00b6 Each row of the resource tracker corresponds to one data or non-data resource. Information in the tracker about each resource includes file path, description, access restrictions, format and corresponding software, and dependencies. The resource tracker is where you will also list and annotate any associated results trackers documenting publications. See [LINK: Resource Tracker: Associated Files/Dependencies] for more information on how you should document your dependencies, depending on the type and amount of data you are sharing. For detailed information on the name and definition of each field in the resource tracker, please refer to the [LINK: resource tracker schema]. Associated Data Dictionary \u00b6 This is a field within the resource tracker. Wherever possible, you should document data dictionary files for your study using the associated data dictionary field (assoc.file.dd) rather than in associated files/dependencies. Associated Protocol \u00b6 This is a field within the resource tracker. Wherever possible, you should document protocols for your study using the associated protocol field (assoc.file.protocol) rather than in associated files/dependencies. Associated Results Tracker \u00b6 This is a field within the resource tracker. You should document your results tracker(s) using the associated results tracker field (assoc.file.results.tracker) rather than in associated files/dependencies. Associated Files/Dependencies \u00b6 Any file that the study artefact being annotated depends on to be produced, interpreted, or used (e.g. a processed tabular data file may depend on a raw data file(s), plus a code file that implements the statistical analysis plan to be produced, and require a data dictionary to interpret/use) Note Data dictionaries and protocols should be shared in the more specific Associated data dictionary, Associated protocol fields in the resource tracker wherever possible How you annotate the dependencies of a file in the resource tracker will depend on whether you are annotating wholistically or minimally. If you are annotating artefacts wholistically Use the 'one layer deep' approach : list only the immediate dependencies needed to produce/interpret/use the artefact. List each dependency in the resource tracker in its own row/entry For each row, list dependencies of that file one layer deep Continue until you are annotating files with no dependencies If you are annotating artefacts minimally Use the 'liberal' approach : list all dependencies. For a resource, list immediate dependencies needed to produce/interpret/use the artefact plus dependencies of those dependencies Continue until you are listing files with no dependencies mkdo Example \u00b6 A processed tabular data file may depend on a raw data file(s), plus a code file that implements the statistical analysis plan to be produced, and require a data dictionary to interpret/use. For more information on 'one layer deep' and 'liberal' dependency documentation, click here. 'One layer deep' Dependencies Liberal Dependencies Immediately underlying raw data files, plus a code file that implements the statistical analysis plan and 'transforms' the raw data files into the processed data file, plus the data dictionary for the processed data file All immediate/one layer deep files, AND The statistical analysis plan that was used to produce the code, the data dictionary for the raw underlying data file(s), a protocol for collection of the raw underlying data file(s), etc. If possible, list liberal dependencies in order from most immediate to least immediate.","title":"Resource"},{"location":"terms/studylevel/resource/#about-the-resource-tracker","text":"The resource tracker is an inventory and annotated list of all data and supporting/non-data files for the study.","title":"About the Resource Tracker"},{"location":"terms/studylevel/resource/#study-level-metadata","text":"Each row of the resource tracker corresponds to one data or non-data resource. Information in the tracker about each resource includes file path, description, access restrictions, format and corresponding software, and dependencies. The resource tracker is where you will also list and annotate any associated results trackers documenting publications. See [LINK: Resource Tracker: Associated Files/Dependencies] for more information on how you should document your dependencies, depending on the type and amount of data you are sharing. For detailed information on the name and definition of each field in the resource tracker, please refer to the [LINK: resource tracker schema].","title":"Study-Level Metadata"},{"location":"terms/studylevel/resource/#associated-data-dictionary","text":"This is a field within the resource tracker. Wherever possible, you should document data dictionary files for your study using the associated data dictionary field (assoc.file.dd) rather than in associated files/dependencies.","title":"Associated Data Dictionary"},{"location":"terms/studylevel/resource/#associated-protocol","text":"This is a field within the resource tracker. Wherever possible, you should document protocols for your study using the associated protocol field (assoc.file.protocol) rather than in associated files/dependencies.","title":"Associated Protocol"},{"location":"terms/studylevel/resource/#associated-results-tracker","text":"This is a field within the resource tracker. You should document your results tracker(s) using the associated results tracker field (assoc.file.results.tracker) rather than in associated files/dependencies.","title":"Associated Results Tracker"},{"location":"terms/studylevel/resource/#associated-filesdependencies","text":"Any file that the study artefact being annotated depends on to be produced, interpreted, or used (e.g. a processed tabular data file may depend on a raw data file(s), plus a code file that implements the statistical analysis plan to be produced, and require a data dictionary to interpret/use) Note Data dictionaries and protocols should be shared in the more specific Associated data dictionary, Associated protocol fields in the resource tracker wherever possible How you annotate the dependencies of a file in the resource tracker will depend on whether you are annotating wholistically or minimally. If you are annotating artefacts wholistically Use the 'one layer deep' approach : list only the immediate dependencies needed to produce/interpret/use the artefact. List each dependency in the resource tracker in its own row/entry For each row, list dependencies of that file one layer deep Continue until you are annotating files with no dependencies If you are annotating artefacts minimally Use the 'liberal' approach : list all dependencies. For a resource, list immediate dependencies needed to produce/interpret/use the artefact plus dependencies of those dependencies Continue until you are listing files with no dependencies mkdo","title":"Associated Files/Dependencies"},{"location":"terms/studylevel/resource/#example","text":"A processed tabular data file may depend on a raw data file(s), plus a code file that implements the statistical analysis plan to be produced, and require a data dictionary to interpret/use. For more information on 'one layer deep' and 'liberal' dependency documentation, click here. 'One layer deep' Dependencies Liberal Dependencies Immediately underlying raw data files, plus a code file that implements the statistical analysis plan and 'transforms' the raw data files into the processed data file, plus the data dictionary for the processed data file All immediate/one layer deep files, AND The statistical analysis plan that was used to produce the code, the data dictionary for the raw underlying data file(s), a protocol for collection of the raw underlying data file(s), etc. If possible, list liberal dependencies in order from most immediate to least immediate.","title":"Example"}]}